{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to train models and eval dataset.\n",
    "\n",
    "This notebook provides instruction to train models and evaluate dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Install requirements\n",
    "Make sure to download the repo via `git clone https://github.com/fanminshi/acts-lstm`\n",
    "and `cd acts-lstm`. This notebook **assume** that you are in the ` acts-lstm` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/fanmin/atcs-lstm\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pytorch-lightning (from atcs-lstm==0.0.0)\n",
      "  Using cached pytorch_lightning-2.2.2-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from pytorch-lightning->atcs-lstm==0.0.0) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from pytorch-lightning->atcs-lstm==0.0.0) (2.2.2)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from pytorch-lightning->atcs-lstm==0.0.0) (4.66.2)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from pytorch-lightning->atcs-lstm==0.0.0) (6.0.1)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning->atcs-lstm==0.0.0) (2023.10.0)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from pytorch-lightning->atcs-lstm==0.0.0) (1.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from pytorch-lightning->atcs-lstm==0.0.0) (22.0)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from pytorch-lightning->atcs-lstm==0.0.0) (4.11.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from pytorch-lightning->atcs-lstm==0.0.0) (0.11.2)\n",
      "Requirement already satisfied: requests in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning->atcs-lstm==0.0.0) (2.31.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning->atcs-lstm==0.0.0) (3.9.3)\n",
      "Requirement already satisfied: setuptools in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from lightning-utilities>=0.8.0->pytorch-lightning->atcs-lstm==0.0.0) (69.2.0)\n",
      "Requirement already satisfied: filelock in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from torch>=1.13.0->pytorch-lightning->atcs-lstm==0.0.0) (3.13.4)\n",
      "Requirement already satisfied: sympy in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from torch>=1.13.0->pytorch-lightning->atcs-lstm==0.0.0) (1.12)\n",
      "Requirement already satisfied: networkx in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from torch>=1.13.0->pytorch-lightning->atcs-lstm==0.0.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from torch>=1.13.0->pytorch-lightning->atcs-lstm==0.0.0) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from torch>=1.13.0->pytorch-lightning->atcs-lstm==0.0.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from torch>=1.13.0->pytorch-lightning->atcs-lstm==0.0.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from torch>=1.13.0->pytorch-lightning->atcs-lstm==0.0.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from torch>=1.13.0->pytorch-lightning->atcs-lstm==0.0.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from torch>=1.13.0->pytorch-lightning->atcs-lstm==0.0.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from torch>=1.13.0->pytorch-lightning->atcs-lstm==0.0.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from torch>=1.13.0->pytorch-lightning->atcs-lstm==0.0.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from torch>=1.13.0->pytorch-lightning->atcs-lstm==0.0.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from torch>=1.13.0->pytorch-lightning->atcs-lstm==0.0.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from torch>=1.13.0->pytorch-lightning->atcs-lstm==0.0.0) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from torch>=1.13.0->pytorch-lightning->atcs-lstm==0.0.0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from torch>=1.13.0->pytorch-lightning->atcs-lstm==0.0.0) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->pytorch-lightning->atcs-lstm==0.0.0) (12.4.127)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->atcs-lstm==0.0.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->atcs-lstm==0.0.0) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->atcs-lstm==0.0.0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->atcs-lstm==0.0.0) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->atcs-lstm==0.0.0) (1.9.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->pytorch-lightning->atcs-lstm==0.0.0) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning->atcs-lstm==0.0.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning->atcs-lstm==0.0.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning->atcs-lstm==0.0.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning->atcs-lstm==0.0.0) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from sympy->torch>=1.13.0->pytorch-lightning->atcs-lstm==0.0.0) (1.3.0)\n",
      "Using cached pytorch_lightning-2.2.2-py3-none-any.whl (801 kB)\n",
      "Installing collected packages: pytorch-lightning, atcs-lstm\n",
      "  Running setup.py develop for atcs-lstm\n",
      "Successfully installed atcs-lstm-0.0.0 pytorch-lightning-2.2.2\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (2.2.2)\n",
      "Requirement already satisfied: torchvision>=0.15.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (0.17.2)\n",
      "Requirement already satisfied: lightning==1.9.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (1.9.0)\n",
      "Requirement already satisfied: nltk in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (3.8.1)\n",
      "Requirement already satisfied: torchtext in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (0.17.2)\n",
      "Requirement already satisfied: torchmetrics in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (1.3.2)\n",
      "Requirement already satisfied: datasets==2.16.1 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (2.16.1)\n",
      "Requirement already satisfied: Jinja2<5.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from lightning==1.9.0->-r requirements.txt (line 4)) (3.1.3)\n",
      "Requirement already satisfied: PyYAML<8.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from lightning==1.9.0->-r requirements.txt (line 4)) (6.0.1)\n",
      "Requirement already satisfied: arrow<3.0,>=1.2.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from lightning==1.9.0->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: beautifulsoup4<6.0,>=4.8.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from lightning==1.9.0->-r requirements.txt (line 4)) (4.12.3)\n",
      "Requirement already satisfied: click<10.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from lightning==1.9.0->-r requirements.txt (line 4)) (8.1.7)\n",
      "Requirement already satisfied: croniter<1.4.0,>=1.3.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from lightning==1.9.0->-r requirements.txt (line 4)) (1.3.15)\n",
      "Requirement already satisfied: dateutils<2.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from lightning==1.9.0->-r requirements.txt (line 4)) (0.6.12)\n",
      "Requirement already satisfied: deepdiff<8.0,>=5.7.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from lightning==1.9.0->-r requirements.txt (line 4)) (7.0.1)\n",
      "Requirement already satisfied: fastapi<0.89.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from lightning==1.9.0->-r requirements.txt (line 4)) (0.88.0)\n",
      "Requirement already satisfied: fsspec<2024.0,>=2022.5.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from lightning==1.9.0->-r requirements.txt (line 4)) (2023.10.0)\n",
      "Requirement already satisfied: inquirer<5.0,>=2.10.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from lightning==1.9.0->-r requirements.txt (line 4)) (3.2.4)\n",
      "Requirement already satisfied: lightning-cloud<2.0,>=0.5.12 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from lightning==1.9.0->-r requirements.txt (line 4)) (0.5.65)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.4.2 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from lightning==1.9.0->-r requirements.txt (line 4)) (0.11.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.17.2 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from lightning==1.9.0->-r requirements.txt (line 4)) (1.26.4)\n",
      "Requirement already satisfied: packaging in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from lightning==1.9.0->-r requirements.txt (line 4)) (22.0)\n",
      "Requirement already satisfied: psutil<7.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from lightning==1.9.0->-r requirements.txt (line 4)) (5.9.8)\n",
      "Requirement already satisfied: pydantic<3.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from lightning==1.9.0->-r requirements.txt (line 4)) (1.10.15)\n",
      "Requirement already satisfied: requests<4.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from lightning==1.9.0->-r requirements.txt (line 4)) (2.31.0)\n",
      "Requirement already satisfied: rich<15.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from lightning==1.9.0->-r requirements.txt (line 4)) (13.7.1)\n",
      "Requirement already satisfied: starlette<2.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from lightning==1.9.0->-r requirements.txt (line 4)) (0.22.0)\n",
      "Requirement already satisfied: starsessions<2.0,>=1.2.1 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from lightning==1.9.0->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from lightning==1.9.0->-r requirements.txt (line 4)) (4.66.2)\n",
      "Requirement already satisfied: traitlets<7.0,>=5.3.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from lightning==1.9.0->-r requirements.txt (line 4)) (5.14.2)\n",
      "Requirement already satisfied: typing-extensions<6.0,>=4.0.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from lightning==1.9.0->-r requirements.txt (line 4)) (4.11.0)\n",
      "Requirement already satisfied: urllib3<3.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from lightning==1.9.0->-r requirements.txt (line 4)) (2.2.1)\n",
      "Requirement already satisfied: uvicorn<2.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from lightning==1.9.0->-r requirements.txt (line 4)) (0.29.0)\n",
      "Requirement already satisfied: websocket-client<3.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from lightning==1.9.0->-r requirements.txt (line 4)) (1.7.0)\n",
      "Requirement already satisfied: websockets<12.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from lightning==1.9.0->-r requirements.txt (line 4)) (11.0.3)\n",
      "Requirement already satisfied: filelock in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from datasets==2.16.1->-r requirements.txt (line 8)) (3.13.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from datasets==2.16.1->-r requirements.txt (line 8)) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from datasets==2.16.1->-r requirements.txt (line 8)) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from datasets==2.16.1->-r requirements.txt (line 8)) (0.3.7)\n",
      "Requirement already satisfied: pandas in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from datasets==2.16.1->-r requirements.txt (line 8)) (2.2.1)\n",
      "Requirement already satisfied: xxhash in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from datasets==2.16.1->-r requirements.txt (line 8)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from datasets==2.16.1->-r requirements.txt (line 8)) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from datasets==2.16.1->-r requirements.txt (line 8)) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from datasets==2.16.1->-r requirements.txt (line 8)) (0.22.2)\n",
      "Requirement already satisfied: sympy in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (1.12)\n",
      "Requirement already satisfied: networkx in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (3.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->-r requirements.txt (line 2)) (12.4.127)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from torchvision>=0.15.0->-r requirements.txt (line 3)) (10.3.0)\n",
      "Requirement already satisfied: joblib in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from nltk->-r requirements.txt (line 5)) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from nltk->-r requirements.txt (line 5)) (2023.12.25)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from arrow<3.0,>=1.2.0->lightning==1.9.0->-r requirements.txt (line 4)) (2.9.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from arrow<3.0,>=1.2.0->lightning==1.9.0->-r requirements.txt (line 4)) (2.9.0.20240316)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from beautifulsoup4<6.0,>=4.8.0->lightning==1.9.0->-r requirements.txt (line 4)) (2.5)\n",
      "Requirement already satisfied: pytz in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from dateutils<2.0->lightning==1.9.0->-r requirements.txt (line 4)) (2024.1)\n",
      "Requirement already satisfied: ordered-set<4.2.0,>=4.1.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from deepdiff<8.0,>=5.7.0->lightning==1.9.0->-r requirements.txt (line 4)) (4.1.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from starlette<2.0->lightning==1.9.0->-r requirements.txt (line 4)) (4.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from aiohttp->datasets==2.16.1->-r requirements.txt (line 8)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from aiohttp->datasets==2.16.1->-r requirements.txt (line 8)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from aiohttp->datasets==2.16.1->-r requirements.txt (line 8)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from aiohttp->datasets==2.16.1->-r requirements.txt (line 8)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from aiohttp->datasets==2.16.1->-r requirements.txt (line 8)) (1.9.4)\n",
      "Requirement already satisfied: blessed>=1.19.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from inquirer<5.0,>=2.10.0->lightning==1.9.0->-r requirements.txt (line 4)) (1.20.0)\n",
      "Requirement already satisfied: editor>=1.6.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from inquirer<5.0,>=2.10.0->lightning==1.9.0->-r requirements.txt (line 4)) (1.6.6)\n",
      "Requirement already satisfied: readchar>=3.0.6 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from inquirer<5.0,>=2.10.0->lightning==1.9.0->-r requirements.txt (line 4)) (4.0.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from Jinja2<5.0->lightning==1.9.0->-r requirements.txt (line 4)) (2.1.5)\n",
      "Requirement already satisfied: pyjwt in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from lightning-cloud<2.0,>=0.5.12->lightning==1.9.0->-r requirements.txt (line 4)) (2.8.0)\n",
      "Requirement already satisfied: python-multipart in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from lightning-cloud<2.0,>=0.5.12->lightning==1.9.0->-r requirements.txt (line 4)) (0.0.9)\n",
      "Requirement already satisfied: six in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from lightning-cloud<2.0,>=0.5.12->lightning==1.9.0->-r requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: boto3 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from lightning-cloud<2.0,>=0.5.12->lightning==1.9.0->-r requirements.txt (line 4)) (1.34.84)\n",
      "Requirement already satisfied: setuptools in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from lightning-utilities<2.0,>=0.4.2->lightning==1.9.0->-r requirements.txt (line 4)) (69.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from requests<4.0->lightning==1.9.0->-r requirements.txt (line 4)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from requests<4.0->lightning==1.9.0->-r requirements.txt (line 4)) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from requests<4.0->lightning==1.9.0->-r requirements.txt (line 4)) (2024.2.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from rich<15.0->lightning==1.9.0->-r requirements.txt (line 4)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from rich<15.0->lightning==1.9.0->-r requirements.txt (line 4)) (2.17.2)\n",
      "Requirement already satisfied: itsdangerous<3.0.0,>=2.0.1 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from starsessions<2.0,>=1.2.1->lightning==1.9.0->-r requirements.txt (line 4)) (2.1.2)\n",
      "Requirement already satisfied: h11>=0.8 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from uvicorn<2.0->lightning==1.9.0->-r requirements.txt (line 4)) (0.14.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from pandas->datasets==2.16.1->-r requirements.txt (line 8)) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from sympy->torch>=2.0.0->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from anyio<5,>=3.4.0->starlette<2.0->lightning==1.9.0->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from blessed>=1.19.0->inquirer<5.0,>=2.10.0->lightning==1.9.0->-r requirements.txt (line 4)) (0.2.13)\n",
      "Requirement already satisfied: runs in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from editor>=1.6.0->inquirer<5.0,>=2.10.0->lightning==1.9.0->-r requirements.txt (line 4)) (1.2.2)\n",
      "Requirement already satisfied: xmod in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from editor>=1.6.0->inquirer<5.0,>=2.10.0->lightning==1.9.0->-r requirements.txt (line 4)) (1.8.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<15.0->lightning==1.9.0->-r requirements.txt (line 4)) (0.1.2)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.84 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from boto3->lightning-cloud<2.0,>=0.5.12->lightning==1.9.0->-r requirements.txt (line 4)) (1.34.84)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from boto3->lightning-cloud<2.0,>=0.5.12->lightning==1.9.0->-r requirements.txt (line 4)) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages (from boto3->lightning-cloud<2.0,>=0.5.12->lightning==1.9.0->-r requirements.txt (line 4)) (0.10.1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# install project dependency\n",
    "! pip install -e .   \n",
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 1234\n",
      "unique token len 37211\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory ./models exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type               | Params\n",
      "------------------------------------------------\n",
      "0 | encoder  | BaselineEncoder    | 11.2 M\n",
      "1 | mlp      | Sequential         | 879 K \n",
      "2 | accuracy | MulticlassAccuracy | 0     \n",
      "------------------------------------------------\n",
      "879 K     Trainable params\n",
      "11.2 M    Non-trainable params\n",
      "12.0 M    Total params\n",
      "48.171    Total estimated model params size (MB)\n",
      "Epoch 0:  98%|███████▊| 1075/1095 [00:05<00:00, 189.75it/s, loss=0.943, v_num=1]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  98%|███████▊| 1076/1095 [00:06<00:00, 172.60it/s, loss=0.943, v_num=1]\u001b[A\n",
      "Epoch 0:  98%|███████▊| 1077/1095 [00:06<00:00, 172.50it/s, loss=0.943, v_num=1]\u001b[A\n",
      "Epoch 0:  98%|███████▉| 1078/1095 [00:06<00:00, 172.41it/s, loss=0.943, v_num=1]\u001b[A\n",
      "Epoch 0:  99%|███████▉| 1079/1095 [00:06<00:00, 172.31it/s, loss=0.943, v_num=1]\u001b[A\n",
      "Epoch 0:  99%|███████▉| 1080/1095 [00:06<00:00, 172.16it/s, loss=0.943, v_num=1]\u001b[A\n",
      "Epoch 0:  99%|███████▉| 1081/1095 [00:06<00:00, 172.06it/s, loss=0.943, v_num=1]\u001b[A\n",
      "Epoch 0:  99%|███████▉| 1082/1095 [00:06<00:00, 171.97it/s, loss=0.943, v_num=1]\u001b[A\n",
      "Epoch 0:  99%|███████▉| 1083/1095 [00:06<00:00, 171.87it/s, loss=0.943, v_num=1]\u001b[A\n",
      "Epoch 0:  99%|███████▉| 1084/1095 [00:06<00:00, 171.80it/s, loss=0.943, v_num=1]\u001b[A\n",
      "Epoch 0:  99%|███████▉| 1085/1095 [00:06<00:00, 171.93it/s, loss=0.943, v_num=1]\u001b[A\n",
      "Epoch 0:  99%|███████▉| 1086/1095 [00:06<00:00, 171.92it/s, loss=0.943, v_num=1]\u001b[A\n",
      "Epoch 0:  99%|███████▉| 1087/1095 [00:06<00:00, 171.83it/s, loss=0.943, v_num=1]\u001b[A\n",
      "Epoch 0:  99%|███████▉| 1088/1095 [00:06<00:00, 171.73it/s, loss=0.943, v_num=1]\u001b[A\n",
      "Epoch 0:  99%|███████▉| 1089/1095 [00:06<00:00, 171.80it/s, loss=0.943, v_num=1]\u001b[A\n",
      "Epoch 0: 100%|███████▉| 1090/1095 [00:06<00:00, 171.71it/s, loss=0.943, v_num=1]\u001b[A\n",
      "Epoch 0: 100%|███████▉| 1091/1095 [00:06<00:00, 171.62it/s, loss=0.943, v_num=1]\u001b[A\n",
      "Epoch 0: 100%|███████▉| 1092/1095 [00:06<00:00, 171.52it/s, loss=0.943, v_num=1]\u001b[A\n",
      "Epoch 0: 100%|███████▉| 1093/1095 [00:06<00:00, 171.43it/s, loss=0.943, v_num=1]\u001b[A\n",
      "Epoch 0: 100%|███████▉| 1094/1095 [00:06<00:00, 171.33it/s, loss=0.943, v_num=1]\u001b[A\n",
      "Epoch 0: 100%|█| 1095/1095 [00:06<00:00, 171.19it/s, loss=0.943, v_num=1, val_lo\u001b[A\n",
      "Epoch 1:  98%|▉| 1075/1095 [00:05<00:00, 188.22it/s, loss=0.901, v_num=1, val_lo\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  98%|▉| 1076/1095 [00:06<00:00, 171.37it/s, loss=0.901, v_num=1, val_lo\u001b[A\n",
      "Epoch 1:  98%|▉| 1077/1095 [00:06<00:00, 171.24it/s, loss=0.901, v_num=1, val_lo\u001b[A\n",
      "Epoch 1:  98%|▉| 1078/1095 [00:06<00:00, 171.16it/s, loss=0.901, v_num=1, val_lo\u001b[A\n",
      "Epoch 1:  99%|▉| 1079/1095 [00:06<00:00, 171.13it/s, loss=0.901, v_num=1, val_lo\u001b[A\n",
      "Epoch 1:  99%|▉| 1080/1095 [00:06<00:00, 171.09it/s, loss=0.901, v_num=1, val_lo\u001b[A\n",
      "Epoch 1:  99%|▉| 1081/1095 [00:06<00:00, 171.00it/s, loss=0.901, v_num=1, val_lo\u001b[A\n",
      "Epoch 1:  99%|▉| 1082/1095 [00:06<00:00, 170.90it/s, loss=0.901, v_num=1, val_lo\u001b[A\n",
      "Epoch 1:  99%|▉| 1083/1095 [00:06<00:00, 170.81it/s, loss=0.901, v_num=1, val_lo\u001b[A\n",
      "Epoch 1:  99%|▉| 1084/1095 [00:06<00:00, 170.72it/s, loss=0.901, v_num=1, val_lo\u001b[A\n",
      "Epoch 1:  99%|▉| 1085/1095 [00:06<00:00, 170.62it/s, loss=0.901, v_num=1, val_lo\u001b[A\n",
      "Epoch 1:  99%|▉| 1086/1095 [00:06<00:00, 170.53it/s, loss=0.901, v_num=1, val_lo\u001b[A\n",
      "Epoch 1:  99%|▉| 1087/1095 [00:06<00:00, 170.43it/s, loss=0.901, v_num=1, val_lo\u001b[A\n",
      "Epoch 1:  99%|▉| 1088/1095 [00:06<00:00, 170.34it/s, loss=0.901, v_num=1, val_lo\u001b[A\n",
      "Epoch 1:  99%|▉| 1089/1095 [00:06<00:00, 170.25it/s, loss=0.901, v_num=1, val_lo\u001b[A\n",
      "Epoch 1: 100%|▉| 1090/1095 [00:06<00:00, 170.16it/s, loss=0.901, v_num=1, val_lo\u001b[A\n",
      "Epoch 1: 100%|▉| 1091/1095 [00:06<00:00, 170.07it/s, loss=0.901, v_num=1, val_lo\u001b[A\n",
      "Epoch 1: 100%|▉| 1092/1095 [00:06<00:00, 169.98it/s, loss=0.901, v_num=1, val_lo\u001b[A\n",
      "Epoch 1: 100%|▉| 1093/1095 [00:06<00:00, 169.89it/s, loss=0.901, v_num=1, val_lo\u001b[A\n",
      "Epoch 1: 100%|▉| 1094/1095 [00:06<00:00, 169.92it/s, loss=0.901, v_num=1, val_lo\u001b[A\n",
      "Epoch 1: 100%|█| 1095/1095 [00:06<00:00, 169.98it/s, loss=0.901, v_num=1, val_lo\u001b[A\n",
      "Epoch 2:  98%|▉| 1075/1095 [00:05<00:00, 188.15it/s, loss=0.876, v_num=1, val_lo\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  98%|▉| 1076/1095 [00:06<00:00, 170.50it/s, loss=0.876, v_num=1, val_lo\u001b[A\n",
      "Epoch 2:  98%|▉| 1077/1095 [00:06<00:00, 170.41it/s, loss=0.876, v_num=1, val_lo\u001b[A\n",
      "Epoch 2:  98%|▉| 1078/1095 [00:06<00:00, 170.54it/s, loss=0.876, v_num=1, val_lo\u001b[A\n",
      "Epoch 2:  99%|▉| 1079/1095 [00:06<00:00, 170.67it/s, loss=0.876, v_num=1, val_lo\u001b[A\n",
      "Epoch 2:  99%|▉| 1080/1095 [00:06<00:00, 170.74it/s, loss=0.876, v_num=1, val_lo\u001b[A\n",
      "Epoch 2:  99%|▉| 1081/1095 [00:06<00:00, 170.65it/s, loss=0.876, v_num=1, val_lo\u001b[A\n",
      "Epoch 2:  99%|▉| 1082/1095 [00:06<00:00, 170.56it/s, loss=0.876, v_num=1, val_lo\u001b[A\n",
      "Epoch 2:  99%|▉| 1083/1095 [00:06<00:00, 170.49it/s, loss=0.876, v_num=1, val_lo\u001b[A\n",
      "Epoch 2:  99%|▉| 1084/1095 [00:06<00:00, 170.45it/s, loss=0.876, v_num=1, val_lo\u001b[A\n",
      "Epoch 2:  99%|▉| 1085/1095 [00:06<00:00, 170.36it/s, loss=0.876, v_num=1, val_lo\u001b[A\n",
      "Epoch 2:  99%|▉| 1086/1095 [00:06<00:00, 170.27it/s, loss=0.876, v_num=1, val_lo\u001b[A\n",
      "Epoch 2:  99%|▉| 1087/1095 [00:06<00:00, 170.18it/s, loss=0.876, v_num=1, val_lo\u001b[A\n",
      "Epoch 2:  99%|▉| 1088/1095 [00:06<00:00, 170.09it/s, loss=0.876, v_num=1, val_lo\u001b[A\n",
      "Epoch 2:  99%|▉| 1089/1095 [00:06<00:00, 169.99it/s, loss=0.876, v_num=1, val_lo\u001b[A\n",
      "Epoch 2: 100%|▉| 1090/1095 [00:06<00:00, 169.90it/s, loss=0.876, v_num=1, val_lo\u001b[A\n",
      "Epoch 2: 100%|▉| 1091/1095 [00:06<00:00, 169.81it/s, loss=0.876, v_num=1, val_lo\u001b[A\n",
      "Epoch 2: 100%|▉| 1092/1095 [00:06<00:00, 169.72it/s, loss=0.876, v_num=1, val_lo\u001b[A\n",
      "Epoch 2: 100%|▉| 1093/1095 [00:06<00:00, 169.63it/s, loss=0.876, v_num=1, val_lo\u001b[A\n",
      "Epoch 2: 100%|▉| 1094/1095 [00:06<00:00, 169.54it/s, loss=0.876, v_num=1, val_lo\u001b[A\n",
      "Epoch 2: 100%|█| 1095/1095 [00:06<00:00, 169.40it/s, loss=0.876, v_num=1, val_lo\u001b[A\n",
      "Epoch 3:  98%|▉| 1075/1095 [00:05<00:00, 186.79it/s, loss=0.847, v_num=1, val_lo\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  98%|▉| 1076/1095 [00:06<00:00, 170.47it/s, loss=0.847, v_num=1, val_lo\u001b[A\n",
      "Epoch 3:  98%|▉| 1077/1095 [00:06<00:00, 170.39it/s, loss=0.847, v_num=1, val_lo\u001b[A\n",
      "Epoch 3:  98%|▉| 1078/1095 [00:06<00:00, 170.30it/s, loss=0.847, v_num=1, val_lo\u001b[A\n",
      "Epoch 3:  99%|▉| 1079/1095 [00:06<00:00, 170.21it/s, loss=0.847, v_num=1, val_lo\u001b[A\n",
      "Epoch 3:  99%|▉| 1080/1095 [00:06<00:00, 170.12it/s, loss=0.847, v_num=1, val_lo\u001b[A\n",
      "Epoch 3:  99%|▉| 1081/1095 [00:06<00:00, 170.03it/s, loss=0.847, v_num=1, val_lo\u001b[A\n",
      "Epoch 3:  99%|▉| 1082/1095 [00:06<00:00, 169.93it/s, loss=0.847, v_num=1, val_lo\u001b[A\n",
      "Epoch 3:  99%|▉| 1083/1095 [00:06<00:00, 169.84it/s, loss=0.847, v_num=1, val_lo\u001b[A\n",
      "Epoch 3:  99%|▉| 1084/1095 [00:06<00:00, 169.75it/s, loss=0.847, v_num=1, val_lo\u001b[A\n",
      "Epoch 3:  99%|▉| 1085/1095 [00:06<00:00, 169.77it/s, loss=0.847, v_num=1, val_lo\u001b[A\n",
      "Epoch 3:  99%|▉| 1086/1095 [00:06<00:00, 169.88it/s, loss=0.847, v_num=1, val_lo\u001b[A\n",
      "Epoch 3:  99%|▉| 1087/1095 [00:06<00:00, 169.84it/s, loss=0.847, v_num=1, val_lo\u001b[A\n",
      "Epoch 3:  99%|▉| 1088/1095 [00:06<00:00, 169.75it/s, loss=0.847, v_num=1, val_lo\u001b[A\n",
      "Epoch 3:  99%|▉| 1089/1095 [00:06<00:00, 169.66it/s, loss=0.847, v_num=1, val_lo\u001b[A\n",
      "Epoch 3: 100%|▉| 1090/1095 [00:06<00:00, 169.57it/s, loss=0.847, v_num=1, val_lo\u001b[A\n",
      "Epoch 3: 100%|▉| 1091/1095 [00:06<00:00, 169.61it/s, loss=0.847, v_num=1, val_lo\u001b[A\n",
      "Epoch 3: 100%|▉| 1092/1095 [00:06<00:00, 169.52it/s, loss=0.847, v_num=1, val_lo\u001b[A\n",
      "Epoch 3: 100%|▉| 1093/1095 [00:06<00:00, 169.43it/s, loss=0.847, v_num=1, val_lo\u001b[A\n",
      "Epoch 3: 100%|▉| 1094/1095 [00:06<00:00, 169.35it/s, loss=0.847, v_num=1, val_lo\u001b[A\n",
      "Epoch 3: 100%|█| 1095/1095 [00:06<00:00, 169.20it/s, loss=0.847, v_num=1, val_lo\u001b[A\n",
      "Epoch 4:  98%|▉| 1075/1095 [00:05<00:00, 186.54it/s, loss=0.827, v_num=1, val_lo\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  98%|▉| 1076/1095 [00:06<00:00, 168.81it/s, loss=0.827, v_num=1, val_lo\u001b[A\n",
      "Epoch 4:  98%|▉| 1077/1095 [00:06<00:00, 168.73it/s, loss=0.827, v_num=1, val_lo\u001b[A\n",
      "Epoch 4:  98%|▉| 1078/1095 [00:06<00:00, 168.65it/s, loss=0.827, v_num=1, val_lo\u001b[A\n",
      "Epoch 4:  99%|▉| 1079/1095 [00:06<00:00, 168.56it/s, loss=0.827, v_num=1, val_lo\u001b[A\n",
      "Epoch 4:  99%|▉| 1080/1095 [00:06<00:00, 168.41it/s, loss=0.827, v_num=1, val_lo\u001b[A\n",
      "Epoch 4:  99%|▉| 1081/1095 [00:06<00:00, 168.32it/s, loss=0.827, v_num=1, val_lo\u001b[A\n",
      "Epoch 4:  99%|▉| 1082/1095 [00:06<00:00, 168.24it/s, loss=0.827, v_num=1, val_lo\u001b[A\n",
      "Epoch 4:  99%|▉| 1083/1095 [00:06<00:00, 168.17it/s, loss=0.827, v_num=1, val_lo\u001b[A\n",
      "Epoch 4:  99%|▉| 1084/1095 [00:06<00:00, 168.28it/s, loss=0.827, v_num=1, val_lo\u001b[A\n",
      "Epoch 4:  99%|▉| 1085/1095 [00:06<00:00, 168.24it/s, loss=0.827, v_num=1, val_lo\u001b[A\n",
      "Epoch 4:  99%|▉| 1086/1095 [00:06<00:00, 168.15it/s, loss=0.827, v_num=1, val_lo\u001b[A\n",
      "Epoch 4:  99%|▉| 1087/1095 [00:06<00:00, 168.07it/s, loss=0.827, v_num=1, val_lo\u001b[A\n",
      "Epoch 4:  99%|▉| 1088/1095 [00:06<00:00, 168.09it/s, loss=0.827, v_num=1, val_lo\u001b[A\n",
      "Epoch 4:  99%|▉| 1089/1095 [00:06<00:00, 168.01it/s, loss=0.827, v_num=1, val_lo\u001b[A\n",
      "Epoch 4: 100%|▉| 1090/1095 [00:06<00:00, 167.92it/s, loss=0.827, v_num=1, val_lo\u001b[A\n",
      "Epoch 4: 100%|▉| 1091/1095 [00:06<00:00, 167.84it/s, loss=0.827, v_num=1, val_lo\u001b[A\n",
      "Epoch 4: 100%|▉| 1092/1095 [00:06<00:00, 167.75it/s, loss=0.827, v_num=1, val_lo\u001b[A\n",
      "Epoch 4: 100%|▉| 1093/1095 [00:06<00:00, 167.66it/s, loss=0.827, v_num=1, val_lo\u001b[A\n",
      "Epoch 4: 100%|▉| 1094/1095 [00:06<00:00, 167.58it/s, loss=0.827, v_num=1, val_lo\u001b[A\n",
      "Epoch 4: 100%|█| 1095/1095 [00:06<00:00, 167.44it/s, loss=0.827, v_num=1, val_lo\u001b[A\n",
      "Epoch 5:  98%|▉| 1075/1095 [00:05<00:00, 186.69it/s, loss=0.813, v_num=1, val_lo\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  98%|▉| 1076/1095 [00:06<00:00, 170.12it/s, loss=0.813, v_num=1, val_lo\u001b[A\n",
      "Epoch 5:  98%|▉| 1077/1095 [00:06<00:00, 170.04it/s, loss=0.813, v_num=1, val_lo\u001b[A\n",
      "Epoch 5:  98%|▉| 1078/1095 [00:06<00:00, 169.95it/s, loss=0.813, v_num=1, val_lo\u001b[A\n",
      "Epoch 5:  99%|▉| 1079/1095 [00:06<00:00, 169.87it/s, loss=0.813, v_num=1, val_lo\u001b[A\n",
      "Epoch 5:  99%|▉| 1080/1095 [00:06<00:00, 169.81it/s, loss=0.813, v_num=1, val_lo\u001b[A\n",
      "Epoch 5:  99%|▉| 1081/1095 [00:06<00:00, 169.72it/s, loss=0.813, v_num=1, val_lo\u001b[A\n",
      "Epoch 5:  99%|▉| 1082/1095 [00:06<00:00, 169.63it/s, loss=0.813, v_num=1, val_lo\u001b[A\n",
      "Epoch 5:  99%|▉| 1083/1095 [00:06<00:00, 169.54it/s, loss=0.813, v_num=1, val_lo\u001b[A\n",
      "Epoch 5:  99%|▉| 1084/1095 [00:06<00:00, 169.45it/s, loss=0.813, v_num=1, val_lo\u001b[A\n",
      "Epoch 5:  99%|▉| 1085/1095 [00:06<00:00, 169.36it/s, loss=0.813, v_num=1, val_lo\u001b[A\n",
      "Epoch 5:  99%|▉| 1086/1095 [00:06<00:00, 169.26it/s, loss=0.813, v_num=1, val_lo\u001b[A\n",
      "Epoch 5:  99%|▉| 1087/1095 [00:06<00:00, 169.18it/s, loss=0.813, v_num=1, val_lo\u001b[A\n",
      "Epoch 5:  99%|▉| 1088/1095 [00:06<00:00, 169.09it/s, loss=0.813, v_num=1, val_lo\u001b[A\n",
      "Epoch 5:  99%|▉| 1089/1095 [00:06<00:00, 169.00it/s, loss=0.813, v_num=1, val_lo\u001b[A\n",
      "Epoch 5: 100%|▉| 1090/1095 [00:06<00:00, 168.91it/s, loss=0.813, v_num=1, val_lo\u001b[A\n",
      "Epoch 5: 100%|▉| 1091/1095 [00:06<00:00, 168.82it/s, loss=0.813, v_num=1, val_lo\u001b[A\n",
      "Epoch 5: 100%|▉| 1092/1095 [00:06<00:00, 168.73it/s, loss=0.813, v_num=1, val_lo\u001b[A\n",
      "Epoch 5: 100%|▉| 1093/1095 [00:06<00:00, 168.75it/s, loss=0.813, v_num=1, val_lo\u001b[A\n",
      "Epoch 5: 100%|▉| 1094/1095 [00:06<00:00, 168.88it/s, loss=0.813, v_num=1, val_lo\u001b[A\n",
      "Epoch 5: 100%|█| 1095/1095 [00:06<00:00, 168.78it/s, loss=0.813, v_num=1, val_lo\u001b[A\n",
      "Epoch 6:  98%|▉| 1075/1095 [00:05<00:00, 188.00it/s, loss=0.798, v_num=1, val_lo\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  98%|▉| 1076/1095 [00:06<00:00, 171.24it/s, loss=0.798, v_num=1, val_lo\u001b[A\n",
      "Epoch 6:  98%|▉| 1077/1095 [00:06<00:00, 171.14it/s, loss=0.798, v_num=1, val_lo\u001b[A\n",
      "Epoch 6:  98%|▉| 1078/1095 [00:06<00:00, 171.06it/s, loss=0.798, v_num=1, val_lo\u001b[A\n",
      "Epoch 6:  99%|▉| 1079/1095 [00:06<00:00, 170.96it/s, loss=0.798, v_num=1, val_lo\u001b[A\n",
      "Epoch 6:  99%|▉| 1080/1095 [00:06<00:00, 170.87it/s, loss=0.798, v_num=1, val_lo\u001b[A\n",
      "Epoch 6:  99%|▉| 1081/1095 [00:06<00:00, 170.78it/s, loss=0.798, v_num=1, val_lo\u001b[A\n",
      "Epoch 6:  99%|▉| 1082/1095 [00:06<00:00, 170.68it/s, loss=0.798, v_num=1, val_lo\u001b[A\n",
      "Epoch 6:  99%|▉| 1083/1095 [00:06<00:00, 170.59it/s, loss=0.798, v_num=1, val_lo\u001b[A\n",
      "Epoch 6:  99%|▉| 1084/1095 [00:06<00:00, 170.50it/s, loss=0.798, v_num=1, val_lo\u001b[A\n",
      "Epoch 6:  99%|▉| 1085/1095 [00:06<00:00, 170.40it/s, loss=0.798, v_num=1, val_lo\u001b[A\n",
      "Epoch 6:  99%|▉| 1086/1095 [00:06<00:00, 170.31it/s, loss=0.798, v_num=1, val_lo\u001b[A\n",
      "Epoch 6:  99%|▉| 1087/1095 [00:06<00:00, 170.42it/s, loss=0.798, v_num=1, val_lo\u001b[A\n",
      "Epoch 6:  99%|▉| 1088/1095 [00:06<00:00, 170.37it/s, loss=0.798, v_num=1, val_lo\u001b[A\n",
      "Epoch 6:  99%|▉| 1089/1095 [00:06<00:00, 170.28it/s, loss=0.798, v_num=1, val_lo\u001b[A\n",
      "Epoch 6: 100%|▉| 1090/1095 [00:06<00:00, 170.26it/s, loss=0.798, v_num=1, val_lo\u001b[A\n",
      "Epoch 6: 100%|▉| 1091/1095 [00:06<00:00, 170.17it/s, loss=0.798, v_num=1, val_lo\u001b[A\n",
      "Epoch 6: 100%|▉| 1092/1095 [00:06<00:00, 170.08it/s, loss=0.798, v_num=1, val_lo\u001b[A\n",
      "Epoch 6: 100%|▉| 1093/1095 [00:06<00:00, 169.99it/s, loss=0.798, v_num=1, val_lo\u001b[A\n",
      "Epoch 6: 100%|▉| 1094/1095 [00:06<00:00, 169.90it/s, loss=0.798, v_num=1, val_lo\u001b[A\n",
      "Epoch 6: 100%|█| 1095/1095 [00:06<00:00, 169.76it/s, loss=0.798, v_num=1, val_lo\u001b[A\n",
      "Epoch 7:  98%|▉| 1075/1095 [00:05<00:00, 188.66it/s, loss=0.777, v_num=1, val_lo\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  98%|▉| 1076/1095 [00:06<00:00, 171.42it/s, loss=0.777, v_num=1, val_lo\u001b[A\n",
      "Epoch 7:  98%|▉| 1077/1095 [00:06<00:00, 171.33it/s, loss=0.777, v_num=1, val_lo\u001b[A\n",
      "Epoch 7:  98%|▉| 1078/1095 [00:06<00:00, 171.25it/s, loss=0.777, v_num=1, val_lo\u001b[A\n",
      "Epoch 7:  99%|▉| 1079/1095 [00:06<00:00, 171.16it/s, loss=0.777, v_num=1, val_lo\u001b[A\n",
      "Epoch 7:  99%|▉| 1080/1095 [00:06<00:00, 171.06it/s, loss=0.777, v_num=1, val_lo\u001b[A\n",
      "Epoch 7:  99%|▉| 1081/1095 [00:06<00:00, 170.97it/s, loss=0.777, v_num=1, val_lo\u001b[A\n",
      "Epoch 7:  99%|▉| 1082/1095 [00:06<00:00, 170.89it/s, loss=0.777, v_num=1, val_lo\u001b[A\n",
      "Epoch 7:  99%|▉| 1083/1095 [00:06<00:00, 171.02it/s, loss=0.777, v_num=1, val_lo\u001b[A\n",
      "Epoch 7:  99%|▉| 1084/1095 [00:06<00:00, 171.07it/s, loss=0.777, v_num=1, val_lo\u001b[A\n",
      "Epoch 7:  99%|▉| 1085/1095 [00:06<00:00, 170.98it/s, loss=0.777, v_num=1, val_lo\u001b[A\n",
      "Epoch 7:  99%|▉| 1086/1095 [00:06<00:00, 170.89it/s, loss=0.777, v_num=1, val_lo\u001b[A\n",
      "Epoch 7:  99%|▉| 1087/1095 [00:06<00:00, 170.80it/s, loss=0.777, v_num=1, val_lo\u001b[A\n",
      "Epoch 7:  99%|▉| 1088/1095 [00:06<00:00, 170.72it/s, loss=0.777, v_num=1, val_lo\u001b[A\n",
      "Epoch 7:  99%|▉| 1089/1095 [00:06<00:00, 170.63it/s, loss=0.777, v_num=1, val_lo\u001b[A\n",
      "Epoch 7: 100%|▉| 1090/1095 [00:06<00:00, 170.54it/s, loss=0.777, v_num=1, val_lo\u001b[A\n",
      "Epoch 7: 100%|▉| 1091/1095 [00:06<00:00, 170.45it/s, loss=0.777, v_num=1, val_lo\u001b[A\n",
      "Epoch 7: 100%|▉| 1092/1095 [00:06<00:00, 170.35it/s, loss=0.777, v_num=1, val_lo\u001b[A\n",
      "Epoch 7: 100%|▉| 1093/1095 [00:06<00:00, 170.26it/s, loss=0.777, v_num=1, val_lo\u001b[A\n",
      "Epoch 7: 100%|▉| 1094/1095 [00:06<00:00, 170.17it/s, loss=0.777, v_num=1, val_lo\u001b[A\n",
      "Epoch 7: 100%|█| 1095/1095 [00:06<00:00, 170.03it/s, loss=0.777, v_num=1, val_lo\u001b[A\n",
      "Epoch 8:  98%|▉| 1075/1095 [00:05<00:00, 187.11it/s, loss=0.769, v_num=1, val_lo\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8:  98%|▉| 1076/1095 [00:06<00:00, 169.98it/s, loss=0.769, v_num=1, val_lo\u001b[A\n",
      "Epoch 8:  98%|▉| 1077/1095 [00:06<00:00, 170.08it/s, loss=0.769, v_num=1, val_lo\u001b[A\n",
      "Epoch 8:  98%|▉| 1078/1095 [00:06<00:00, 170.09it/s, loss=0.769, v_num=1, val_lo\u001b[A\n",
      "Epoch 8:  99%|▉| 1079/1095 [00:06<00:00, 170.00it/s, loss=0.769, v_num=1, val_lo\u001b[A\n",
      "Epoch 8:  99%|▉| 1080/1095 [00:06<00:00, 169.91it/s, loss=0.769, v_num=1, val_lo\u001b[A\n",
      "Epoch 8:  99%|▉| 1081/1095 [00:06<00:00, 169.82it/s, loss=0.769, v_num=1, val_lo\u001b[A\n",
      "Epoch 8:  99%|▉| 1082/1095 [00:06<00:00, 169.93it/s, loss=0.769, v_num=1, val_lo\u001b[A\n",
      "Epoch 8:  99%|▉| 1083/1095 [00:06<00:00, 169.89it/s, loss=0.769, v_num=1, val_lo\u001b[A\n",
      "Epoch 8:  99%|▉| 1084/1095 [00:06<00:00, 169.80it/s, loss=0.769, v_num=1, val_lo\u001b[A\n",
      "Epoch 8:  99%|▉| 1085/1095 [00:06<00:00, 169.72it/s, loss=0.769, v_num=1, val_lo\u001b[A\n",
      "Epoch 8:  99%|▉| 1086/1095 [00:06<00:00, 169.62it/s, loss=0.769, v_num=1, val_lo\u001b[A\n",
      "Epoch 8:  99%|▉| 1087/1095 [00:06<00:00, 169.53it/s, loss=0.769, v_num=1, val_lo\u001b[A\n",
      "Epoch 8:  99%|▉| 1088/1095 [00:06<00:00, 169.44it/s, loss=0.769, v_num=1, val_lo\u001b[A\n",
      "Epoch 8:  99%|▉| 1089/1095 [00:06<00:00, 169.35it/s, loss=0.769, v_num=1, val_lo\u001b[A\n",
      "Epoch 8: 100%|▉| 1090/1095 [00:06<00:00, 169.26it/s, loss=0.769, v_num=1, val_lo\u001b[A\n",
      "Epoch 8: 100%|▉| 1091/1095 [00:06<00:00, 169.17it/s, loss=0.769, v_num=1, val_lo\u001b[A\n",
      "Epoch 8: 100%|▉| 1092/1095 [00:06<00:00, 169.08it/s, loss=0.769, v_num=1, val_lo\u001b[A\n",
      "Epoch 8: 100%|▉| 1093/1095 [00:06<00:00, 169.00it/s, loss=0.769, v_num=1, val_lo\u001b[A\n",
      "Epoch 8: 100%|▉| 1094/1095 [00:06<00:00, 168.91it/s, loss=0.769, v_num=1, val_lo\u001b[A\n",
      "Epoch 8: 100%|█| 1095/1095 [00:06<00:00, 168.76it/s, loss=0.769, v_num=1, val_lo\u001b[A\n",
      "Epoch 9:  98%|▉| 1075/1095 [00:05<00:00, 187.95it/s, loss=0.755, v_num=1, val_lo\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9:  98%|▉| 1076/1095 [00:06<00:00, 170.40it/s, loss=0.755, v_num=1, val_lo\u001b[A\n",
      "Epoch 9:  98%|▉| 1077/1095 [00:06<00:00, 170.32it/s, loss=0.755, v_num=1, val_lo\u001b[A\n",
      "Epoch 9:  98%|▉| 1078/1095 [00:06<00:00, 170.23it/s, loss=0.755, v_num=1, val_lo\u001b[A\n",
      "Epoch 9:  99%|▉| 1079/1095 [00:06<00:00, 170.14it/s, loss=0.755, v_num=1, val_lo\u001b[A\n",
      "Epoch 9:  99%|▉| 1080/1095 [00:06<00:00, 170.04it/s, loss=0.755, v_num=1, val_lo\u001b[A\n",
      "Epoch 9:  99%|▉| 1081/1095 [00:06<00:00, 169.89it/s, loss=0.755, v_num=1, val_lo\u001b[A\n",
      "Epoch 9:  99%|▉| 1082/1095 [00:06<00:00, 169.80it/s, loss=0.755, v_num=1, val_lo\u001b[A\n",
      "Epoch 9:  99%|▉| 1083/1095 [00:06<00:00, 169.71it/s, loss=0.755, v_num=1, val_lo\u001b[A\n",
      "Epoch 9:  99%|▉| 1084/1095 [00:06<00:00, 169.62it/s, loss=0.755, v_num=1, val_lo\u001b[A\n",
      "Epoch 9:  99%|▉| 1085/1095 [00:06<00:00, 169.53it/s, loss=0.755, v_num=1, val_lo\u001b[A\n",
      "Epoch 9:  99%|▉| 1086/1095 [00:06<00:00, 169.44it/s, loss=0.755, v_num=1, val_lo\u001b[A\n",
      "Epoch 9:  99%|▉| 1087/1095 [00:06<00:00, 169.38it/s, loss=0.755, v_num=1, val_lo\u001b[A\n",
      "Epoch 9:  99%|▉| 1088/1095 [00:06<00:00, 169.51it/s, loss=0.755, v_num=1, val_lo\u001b[A\n",
      "Epoch 9:  99%|▉| 1089/1095 [00:06<00:00, 169.64it/s, loss=0.755, v_num=1, val_lo\u001b[A\n",
      "Epoch 9: 100%|▉| 1090/1095 [00:06<00:00, 169.60it/s, loss=0.755, v_num=1, val_lo\u001b[A\n",
      "Epoch 9: 100%|▉| 1091/1095 [00:06<00:00, 169.51it/s, loss=0.755, v_num=1, val_lo\u001b[A\n",
      "Epoch 9: 100%|▉| 1092/1095 [00:06<00:00, 169.42it/s, loss=0.755, v_num=1, val_lo\u001b[A\n",
      "Epoch 9: 100%|▉| 1093/1095 [00:06<00:00, 169.45it/s, loss=0.755, v_num=1, val_lo\u001b[A\n",
      "Epoch 9: 100%|▉| 1094/1095 [00:06<00:00, 169.36it/s, loss=0.755, v_num=1, val_lo\u001b[A\n",
      "Epoch 9: 100%|█| 1095/1095 [00:06<00:00, 169.22it/s, loss=0.755, v_num=1, val_lo\u001b[A\n",
      "Epoch 10:  98%|▉| 1075/1095 [00:05<00:00, 187.40it/s, loss=0.754, v_num=1, val_l\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10:  98%|▉| 1076/1095 [00:06<00:00, 170.22it/s, loss=0.754, v_num=1, val_l\u001b[A\n",
      "Epoch 10:  98%|▉| 1077/1095 [00:06<00:00, 170.14it/s, loss=0.754, v_num=1, val_l\u001b[A\n",
      "Epoch 10:  98%|▉| 1078/1095 [00:06<00:00, 170.05it/s, loss=0.754, v_num=1, val_l\u001b[A\n",
      "Epoch 10:  99%|▉| 1079/1095 [00:06<00:00, 169.96it/s, loss=0.754, v_num=1, val_l\u001b[A\n",
      "Epoch 10:  99%|▉| 1080/1095 [00:06<00:00, 169.87it/s, loss=0.754, v_num=1, val_l\u001b[A\n",
      "Epoch 10:  99%|▉| 1081/1095 [00:06<00:00, 169.77it/s, loss=0.754, v_num=1, val_l\u001b[A\n",
      "Epoch 10:  99%|▉| 1082/1095 [00:06<00:00, 169.68it/s, loss=0.754, v_num=1, val_l\u001b[A\n",
      "Epoch 10:  99%|▉| 1083/1095 [00:06<00:00, 169.59it/s, loss=0.754, v_num=1, val_l\u001b[A\n",
      "Epoch 10:  99%|▉| 1084/1095 [00:06<00:00, 169.50it/s, loss=0.754, v_num=1, val_l\u001b[A\n",
      "Epoch 10:  99%|▉| 1085/1095 [00:06<00:00, 169.41it/s, loss=0.754, v_num=1, val_l\u001b[A\n",
      "Epoch 10:  99%|▉| 1086/1095 [00:06<00:00, 169.32it/s, loss=0.754, v_num=1, val_l\u001b[A\n",
      "Epoch 10:  99%|▉| 1087/1095 [00:06<00:00, 169.23it/s, loss=0.754, v_num=1, val_l\u001b[A\n",
      "Epoch 10:  99%|▉| 1088/1095 [00:06<00:00, 169.24it/s, loss=0.754, v_num=1, val_l\u001b[A\n",
      "Epoch 10:  99%|▉| 1089/1095 [00:06<00:00, 169.35it/s, loss=0.754, v_num=1, val_l\u001b[A\n",
      "Epoch 10: 100%|▉| 1090/1095 [00:06<00:00, 169.31it/s, loss=0.754, v_num=1, val_l\u001b[A\n",
      "Epoch 10: 100%|▉| 1091/1095 [00:06<00:00, 169.23it/s, loss=0.754, v_num=1, val_l\u001b[A\n",
      "Epoch 10: 100%|▉| 1092/1095 [00:06<00:00, 169.14it/s, loss=0.754, v_num=1, val_l\u001b[A\n",
      "Epoch 10: 100%|▉| 1093/1095 [00:06<00:00, 169.14it/s, loss=0.754, v_num=1, val_l\u001b[A\n",
      "Epoch 10: 100%|▉| 1094/1095 [00:06<00:00, 169.05it/s, loss=0.754, v_num=1, val_l\u001b[A\n",
      "Epoch 10: 100%|█| 1095/1095 [00:06<00:00, 168.91it/s, loss=0.754, v_num=1, val_l\u001b[A\n",
      "Epoch 11:  98%|▉| 1075/1095 [00:05<00:00, 186.89it/s, loss=0.736, v_num=1, val_l\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11:  98%|▉| 1076/1095 [00:06<00:00, 170.35it/s, loss=0.736, v_num=1, val_l\u001b[A\n",
      "Epoch 11:  98%|▉| 1077/1095 [00:06<00:00, 170.27it/s, loss=0.736, v_num=1, val_l\u001b[A\n",
      "Epoch 11:  98%|▉| 1078/1095 [00:06<00:00, 170.18it/s, loss=0.736, v_num=1, val_l\u001b[A\n",
      "Epoch 11:  99%|▉| 1079/1095 [00:06<00:00, 170.09it/s, loss=0.736, v_num=1, val_l\u001b[A\n",
      "Epoch 11:  99%|▉| 1080/1095 [00:06<00:00, 170.20it/s, loss=0.736, v_num=1, val_l\u001b[A\n",
      "Epoch 11:  99%|▉| 1081/1095 [00:06<00:00, 170.08it/s, loss=0.736, v_num=1, val_l\u001b[A\n",
      "Epoch 11:  99%|▉| 1082/1095 [00:06<00:00, 169.99it/s, loss=0.736, v_num=1, val_l\u001b[A\n",
      "Epoch 11:  99%|▉| 1083/1095 [00:06<00:00, 169.90it/s, loss=0.736, v_num=1, val_l\u001b[A\n",
      "Epoch 11:  99%|▉| 1084/1095 [00:06<00:00, 169.94it/s, loss=0.736, v_num=1, val_l\u001b[A\n",
      "Epoch 11:  99%|▉| 1085/1095 [00:06<00:00, 169.85it/s, loss=0.736, v_num=1, val_l\u001b[A\n",
      "Epoch 11:  99%|▉| 1086/1095 [00:06<00:00, 169.76it/s, loss=0.736, v_num=1, val_l\u001b[A\n",
      "Epoch 11:  99%|▉| 1087/1095 [00:06<00:00, 169.67it/s, loss=0.736, v_num=1, val_l\u001b[A\n",
      "Epoch 11:  99%|▉| 1088/1095 [00:06<00:00, 169.57it/s, loss=0.736, v_num=1, val_l\u001b[A\n",
      "Epoch 11:  99%|▉| 1089/1095 [00:06<00:00, 169.48it/s, loss=0.736, v_num=1, val_l\u001b[A\n",
      "Epoch 11: 100%|▉| 1090/1095 [00:06<00:00, 169.39it/s, loss=0.736, v_num=1, val_l\u001b[A\n",
      "Epoch 11: 100%|▉| 1091/1095 [00:06<00:00, 169.30it/s, loss=0.736, v_num=1, val_l\u001b[A\n",
      "Epoch 11: 100%|▉| 1092/1095 [00:06<00:00, 169.21it/s, loss=0.736, v_num=1, val_l\u001b[A\n",
      "Epoch 11: 100%|▉| 1093/1095 [00:06<00:00, 169.13it/s, loss=0.736, v_num=1, val_l\u001b[A\n",
      "Epoch 11: 100%|▉| 1094/1095 [00:06<00:00, 169.04it/s, loss=0.736, v_num=1, val_l\u001b[A\n",
      "Epoch 11: 100%|█| 1095/1095 [00:06<00:00, 168.90it/s, loss=0.736, v_num=1, val_l\u001b[A\n",
      "Epoch 12:  98%|▉| 1075/1095 [00:05<00:00, 188.24it/s, loss=0.725, v_num=1, val_l\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12:  98%|▉| 1076/1095 [00:06<00:00, 171.16it/s, loss=0.725, v_num=1, val_l\u001b[A\n",
      "Epoch 12:  98%|▉| 1077/1095 [00:06<00:00, 171.08it/s, loss=0.725, v_num=1, val_l\u001b[A\n",
      "Epoch 12:  98%|▉| 1078/1095 [00:06<00:00, 170.99it/s, loss=0.725, v_num=1, val_l\u001b[A\n",
      "Epoch 12:  99%|▉| 1079/1095 [00:06<00:00, 170.89it/s, loss=0.725, v_num=1, val_l\u001b[A\n",
      "Epoch 12:  99%|▉| 1080/1095 [00:06<00:00, 170.80it/s, loss=0.725, v_num=1, val_l\u001b[A\n",
      "Epoch 12:  99%|▉| 1081/1095 [00:06<00:00, 170.71it/s, loss=0.725, v_num=1, val_l\u001b[A\n",
      "Epoch 12:  99%|▉| 1082/1095 [00:06<00:00, 170.72it/s, loss=0.725, v_num=1, val_l\u001b[A\n",
      "Epoch 12:  99%|▉| 1083/1095 [00:06<00:00, 170.85it/s, loss=0.725, v_num=1, val_l\u001b[A\n",
      "Epoch 12:  99%|▉| 1084/1095 [00:06<00:00, 170.79it/s, loss=0.725, v_num=1, val_l\u001b[A\n",
      "Epoch 12:  99%|▉| 1085/1095 [00:06<00:00, 170.70it/s, loss=0.725, v_num=1, val_l\u001b[A\n",
      "Epoch 12:  99%|▉| 1086/1095 [00:06<00:00, 170.60it/s, loss=0.725, v_num=1, val_l\u001b[A\n",
      "Epoch 12:  99%|▉| 1087/1095 [00:06<00:00, 170.61it/s, loss=0.725, v_num=1, val_l\u001b[A\n",
      "Epoch 12:  99%|▉| 1088/1095 [00:06<00:00, 170.58it/s, loss=0.725, v_num=1, val_l\u001b[A\n",
      "Epoch 12:  99%|▉| 1089/1095 [00:06<00:00, 170.48it/s, loss=0.725, v_num=1, val_l\u001b[A\n",
      "Epoch 12: 100%|▉| 1090/1095 [00:06<00:00, 170.39it/s, loss=0.725, v_num=1, val_l\u001b[A\n",
      "Epoch 12: 100%|▉| 1091/1095 [00:06<00:00, 170.30it/s, loss=0.725, v_num=1, val_l\u001b[A\n",
      "Epoch 12: 100%|▉| 1092/1095 [00:06<00:00, 170.21it/s, loss=0.725, v_num=1, val_l\u001b[A\n",
      "Epoch 12: 100%|▉| 1093/1095 [00:06<00:00, 170.12it/s, loss=0.725, v_num=1, val_l\u001b[A\n",
      "Epoch 12: 100%|▉| 1094/1095 [00:06<00:00, 170.03it/s, loss=0.725, v_num=1, val_l\u001b[A\n",
      "Epoch 12: 100%|█| 1095/1095 [00:06<00:00, 169.88it/s, loss=0.725, v_num=1, val_l\u001b[A\n",
      "Epoch 13:  98%|▉| 1075/1095 [00:05<00:00, 187.59it/s, loss=0.708, v_num=1, val_l\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13:  98%|▉| 1076/1095 [00:06<00:00, 170.24it/s, loss=0.708, v_num=1, val_l\u001b[A\n",
      "Epoch 13:  98%|▉| 1077/1095 [00:06<00:00, 170.17it/s, loss=0.708, v_num=1, val_l\u001b[A\n",
      "Epoch 13:  98%|▉| 1078/1095 [00:06<00:00, 170.08it/s, loss=0.708, v_num=1, val_l\u001b[A\n",
      "Epoch 13:  99%|▉| 1079/1095 [00:06<00:00, 169.99it/s, loss=0.708, v_num=1, val_l\u001b[A\n",
      "Epoch 13:  99%|▉| 1080/1095 [00:06<00:00, 169.89it/s, loss=0.708, v_num=1, val_l\u001b[A\n",
      "Epoch 13:  99%|▉| 1081/1095 [00:06<00:00, 169.80it/s, loss=0.708, v_num=1, val_l\u001b[A\n",
      "Epoch 13:  99%|▉| 1082/1095 [00:06<00:00, 169.71it/s, loss=0.708, v_num=1, val_l\u001b[A\n",
      "Epoch 13:  99%|▉| 1083/1095 [00:06<00:00, 169.62it/s, loss=0.708, v_num=1, val_l\u001b[A\n",
      "Epoch 13:  99%|▉| 1084/1095 [00:06<00:00, 169.53it/s, loss=0.708, v_num=1, val_l\u001b[A\n",
      "Epoch 13:  99%|▉| 1085/1095 [00:06<00:00, 169.44it/s, loss=0.708, v_num=1, val_l\u001b[A\n",
      "Epoch 13:  99%|▉| 1086/1095 [00:06<00:00, 169.35it/s, loss=0.708, v_num=1, val_l\u001b[A\n",
      "Epoch 13:  99%|▉| 1087/1095 [00:06<00:00, 169.39it/s, loss=0.708, v_num=1, val_l\u001b[A\n",
      "Epoch 13:  99%|▉| 1088/1095 [00:06<00:00, 169.50it/s, loss=0.708, v_num=1, val_l\u001b[A\n",
      "Epoch 13:  99%|▉| 1089/1095 [00:06<00:00, 169.51it/s, loss=0.708, v_num=1, val_l\u001b[A\n",
      "Epoch 13: 100%|▉| 1090/1095 [00:06<00:00, 169.42it/s, loss=0.708, v_num=1, val_l\u001b[A\n",
      "Epoch 13: 100%|▉| 1091/1095 [00:06<00:00, 169.33it/s, loss=0.708, v_num=1, val_l\u001b[A\n",
      "Epoch 13: 100%|▉| 1092/1095 [00:06<00:00, 169.33it/s, loss=0.708, v_num=1, val_l\u001b[A\n",
      "Epoch 13: 100%|▉| 1093/1095 [00:06<00:00, 169.29it/s, loss=0.708, v_num=1, val_l\u001b[A\n",
      "Epoch 13: 100%|▉| 1094/1095 [00:06<00:00, 169.20it/s, loss=0.708, v_num=1, val_l\u001b[A\n",
      "Epoch 13: 100%|█| 1095/1095 [00:06<00:00, 169.06it/s, loss=0.708, v_num=1, val_l\u001b[A\n",
      "Epoch 14:  98%|▉| 1075/1095 [00:05<00:00, 188.27it/s, loss=0.712, v_num=1, val_l\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14:  98%|▉| 1076/1095 [00:06<00:00, 170.49it/s, loss=0.712, v_num=1, val_l\u001b[A\n",
      "Epoch 14:  98%|▉| 1077/1095 [00:06<00:00, 170.42it/s, loss=0.712, v_num=1, val_l\u001b[A\n",
      "Epoch 14:  98%|▉| 1078/1095 [00:06<00:00, 170.43it/s, loss=0.712, v_num=1, val_l\u001b[A\n",
      "Epoch 14:  99%|▉| 1079/1095 [00:06<00:00, 170.39it/s, loss=0.712, v_num=1, val_l\u001b[A\n",
      "Epoch 14:  99%|▉| 1080/1095 [00:06<00:00, 170.30it/s, loss=0.712, v_num=1, val_l\u001b[A\n",
      "Epoch 14:  99%|▉| 1081/1095 [00:06<00:00, 170.21it/s, loss=0.712, v_num=1, val_l\u001b[A\n",
      "Epoch 14:  99%|▉| 1082/1095 [00:06<00:00, 170.12it/s, loss=0.712, v_num=1, val_l\u001b[A\n",
      "Epoch 14:  99%|▉| 1083/1095 [00:06<00:00, 170.03it/s, loss=0.712, v_num=1, val_l\u001b[A\n",
      "Epoch 14:  99%|▉| 1084/1095 [00:06<00:00, 169.93it/s, loss=0.712, v_num=1, val_l\u001b[A\n",
      "Epoch 14:  99%|▉| 1085/1095 [00:06<00:00, 169.78it/s, loss=0.712, v_num=1, val_l\u001b[A\n",
      "Epoch 14:  99%|▉| 1086/1095 [00:06<00:00, 169.69it/s, loss=0.712, v_num=1, val_l\u001b[A\n",
      "Epoch 14:  99%|▉| 1087/1095 [00:06<00:00, 169.60it/s, loss=0.712, v_num=1, val_l\u001b[A\n",
      "Epoch 14:  99%|▉| 1088/1095 [00:06<00:00, 169.51it/s, loss=0.712, v_num=1, val_l\u001b[A\n",
      "Epoch 14:  99%|▉| 1089/1095 [00:06<00:00, 169.42it/s, loss=0.712, v_num=1, val_l\u001b[A\n",
      "Epoch 14: 100%|▉| 1090/1095 [00:06<00:00, 169.33it/s, loss=0.712, v_num=1, val_l\u001b[A\n",
      "Epoch 14: 100%|▉| 1091/1095 [00:06<00:00, 169.24it/s, loss=0.712, v_num=1, val_l\u001b[A\n",
      "Epoch 14: 100%|▉| 1092/1095 [00:06<00:00, 169.27it/s, loss=0.712, v_num=1, val_l\u001b[A\n",
      "Epoch 14: 100%|▉| 1093/1095 [00:06<00:00, 169.33it/s, loss=0.712, v_num=1, val_l\u001b[A\n",
      "Epoch 14: 100%|▉| 1094/1095 [00:06<00:00, 169.24it/s, loss=0.712, v_num=1, val_l\u001b[A\n",
      "Epoch 14: 100%|█| 1095/1095 [00:06<00:00, 169.10it/s, loss=0.712, v_num=1, val_l\u001b[A\n",
      "Epoch 15:  98%|▉| 1075/1095 [00:05<00:00, 188.03it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15:  98%|▉| 1076/1095 [00:06<00:00, 170.60it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 15:  98%|▉| 1077/1095 [00:06<00:00, 170.52it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 15:  98%|▉| 1078/1095 [00:06<00:00, 170.43it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 15:  99%|▉| 1079/1095 [00:06<00:00, 170.33it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 15:  99%|▉| 1080/1095 [00:06<00:00, 170.24it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 15:  99%|▉| 1081/1095 [00:06<00:00, 170.15it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 15:  99%|▉| 1082/1095 [00:06<00:00, 170.06it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 15:  99%|▉| 1083/1095 [00:06<00:00, 169.97it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 15:  99%|▉| 1084/1095 [00:06<00:00, 169.87it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 15:  99%|▉| 1085/1095 [00:06<00:00, 169.92it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 15:  99%|▉| 1086/1095 [00:06<00:00, 170.02it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 15:  99%|▉| 1087/1095 [00:06<00:00, 169.98it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 15:  99%|▉| 1088/1095 [00:06<00:00, 169.89it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 15:  99%|▉| 1089/1095 [00:06<00:00, 169.80it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 15: 100%|▉| 1090/1095 [00:06<00:00, 169.83it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 15: 100%|▉| 1091/1095 [00:06<00:00, 169.74it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 15: 100%|▉| 1092/1095 [00:06<00:00, 169.65it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 15: 100%|▉| 1093/1095 [00:06<00:00, 169.56it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 15: 100%|▉| 1094/1095 [00:06<00:00, 169.47it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 15: 100%|█| 1095/1095 [00:06<00:00, 169.33it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 16:  98%|▉| 1075/1095 [00:05<00:00, 187.02it/s, loss=0.704, v_num=1, val_l\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16:  98%|▉| 1076/1095 [00:06<00:00, 169.60it/s, loss=0.704, v_num=1, val_l\u001b[A\n",
      "Epoch 16:  98%|▉| 1077/1095 [00:06<00:00, 169.52it/s, loss=0.704, v_num=1, val_l\u001b[A\n",
      "Epoch 16:  98%|▉| 1078/1095 [00:06<00:00, 169.43it/s, loss=0.704, v_num=1, val_l\u001b[A\n",
      "Epoch 16:  99%|▉| 1079/1095 [00:06<00:00, 169.34it/s, loss=0.704, v_num=1, val_l\u001b[A\n",
      "Epoch 16:  99%|▉| 1080/1095 [00:06<00:00, 169.25it/s, loss=0.704, v_num=1, val_l\u001b[A\n",
      "Epoch 16:  99%|▉| 1081/1095 [00:06<00:00, 169.16it/s, loss=0.704, v_num=1, val_l\u001b[A\n",
      "Epoch 16:  99%|▉| 1082/1095 [00:06<00:00, 169.07it/s, loss=0.704, v_num=1, val_l\u001b[A\n",
      "Epoch 16:  99%|▉| 1083/1095 [00:06<00:00, 168.98it/s, loss=0.704, v_num=1, val_l\u001b[A\n",
      "Epoch 16:  99%|▉| 1084/1095 [00:06<00:00, 168.89it/s, loss=0.704, v_num=1, val_l\u001b[A\n",
      "Epoch 16:  99%|▉| 1085/1095 [00:06<00:00, 168.80it/s, loss=0.704, v_num=1, val_l\u001b[A\n",
      "Epoch 16:  99%|▉| 1086/1095 [00:06<00:00, 168.71it/s, loss=0.704, v_num=1, val_l\u001b[A\n",
      "Epoch 16:  99%|▉| 1087/1095 [00:06<00:00, 168.62it/s, loss=0.704, v_num=1, val_l\u001b[A\n",
      "Epoch 16:  99%|▉| 1088/1095 [00:06<00:00, 168.73it/s, loss=0.704, v_num=1, val_l\u001b[A\n",
      "Epoch 16:  99%|▉| 1089/1095 [00:06<00:00, 168.65it/s, loss=0.704, v_num=1, val_l\u001b[A\n",
      "Epoch 16: 100%|▉| 1090/1095 [00:06<00:00, 168.56it/s, loss=0.704, v_num=1, val_l\u001b[A\n",
      "Epoch 16: 100%|▉| 1091/1095 [00:06<00:00, 168.48it/s, loss=0.704, v_num=1, val_l\u001b[A\n",
      "Epoch 16: 100%|▉| 1092/1095 [00:06<00:00, 168.53it/s, loss=0.704, v_num=1, val_l\u001b[A\n",
      "Epoch 16: 100%|▉| 1093/1095 [00:06<00:00, 168.44it/s, loss=0.704, v_num=1, val_l\u001b[A\n",
      "Epoch 16: 100%|▉| 1094/1095 [00:06<00:00, 168.36it/s, loss=0.704, v_num=1, val_l\u001b[A\n",
      "Epoch 16: 100%|█| 1095/1095 [00:06<00:00, 168.22it/s, loss=0.704, v_num=1, val_l\u001b[A\n",
      "Epoch 17:  98%|▉| 1075/1095 [00:05<00:00, 186.72it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17:  98%|▉| 1076/1095 [00:06<00:00, 169.74it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 17:  98%|▉| 1077/1095 [00:06<00:00, 169.66it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 17:  98%|▉| 1078/1095 [00:06<00:00, 169.57it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 17:  99%|▉| 1079/1095 [00:06<00:00, 169.48it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 17:  99%|▉| 1080/1095 [00:06<00:00, 169.39it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 17:  99%|▉| 1081/1095 [00:06<00:00, 169.30it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 17:  99%|▉| 1082/1095 [00:06<00:00, 169.30it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 17:  99%|▉| 1083/1095 [00:06<00:00, 169.23it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 17:  99%|▉| 1084/1095 [00:06<00:00, 169.14it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 17:  99%|▉| 1085/1095 [00:06<00:00, 169.05it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 17:  99%|▉| 1086/1095 [00:06<00:00, 169.04it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 17:  99%|▉| 1087/1095 [00:06<00:00, 168.95it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 17:  99%|▉| 1088/1095 [00:06<00:00, 168.86it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 17:  99%|▉| 1089/1095 [00:06<00:00, 168.78it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 17: 100%|▉| 1090/1095 [00:06<00:00, 168.69it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 17: 100%|▉| 1091/1095 [00:06<00:00, 168.60it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 17: 100%|▉| 1092/1095 [00:06<00:00, 168.51it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 17: 100%|▉| 1093/1095 [00:06<00:00, 168.42it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 17: 100%|▉| 1094/1095 [00:06<00:00, 168.33it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 17: 100%|█| 1095/1095 [00:06<00:00, 168.20it/s, loss=0.693, v_num=1, val_l\u001b[A\n",
      "Epoch 18:  98%|▉| 1075/1095 [00:03<00:00, 291.78it/s, loss=0.678, v_num=1, val_l\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 18:  98%|▉| 1076/1095 [00:04<00:00, 249.74it/s, loss=0.678, v_num=1, val_l\u001b[A\n",
      "Epoch 18:  98%|▉| 1077/1095 [00:04<00:00, 249.45it/s, loss=0.678, v_num=1, val_l\u001b[A\n",
      "Epoch 18:  98%|▉| 1078/1095 [00:04<00:00, 249.22it/s, loss=0.678, v_num=1, val_l\u001b[A\n",
      "Epoch 18:  99%|▉| 1079/1095 [00:04<00:00, 249.38it/s, loss=0.678, v_num=1, val_l\u001b[A\n",
      "Epoch 18:  99%|▉| 1080/1095 [00:04<00:00, 249.18it/s, loss=0.678, v_num=1, val_l\u001b[A\n",
      "Epoch 18:  99%|▉| 1081/1095 [00:04<00:00, 248.88it/s, loss=0.678, v_num=1, val_l\u001b[A\n",
      "Epoch 18:  99%|▉| 1082/1095 [00:04<00:00, 248.58it/s, loss=0.678, v_num=1, val_l\u001b[A\n",
      "Epoch 18:  99%|▉| 1083/1095 [00:04<00:00, 248.41it/s, loss=0.678, v_num=1, val_l\u001b[A\n",
      "Epoch 18:  99%|▉| 1084/1095 [00:04<00:00, 248.12it/s, loss=0.678, v_num=1, val_l\u001b[A\n",
      "Epoch 18:  99%|▉| 1085/1095 [00:04<00:00, 247.82it/s, loss=0.678, v_num=1, val_l\u001b[A\n",
      "Epoch 18:  99%|▉| 1086/1095 [00:04<00:00, 247.52it/s, loss=0.678, v_num=1, val_l\u001b[A\n",
      "Epoch 18:  99%|▉| 1087/1095 [00:04<00:00, 247.23it/s, loss=0.678, v_num=1, val_l\u001b[A\n",
      "Epoch 18:  99%|▉| 1088/1095 [00:04<00:00, 246.92it/s, loss=0.678, v_num=1, val_l\u001b[A\n",
      "Epoch 18:  99%|▉| 1089/1095 [00:04<00:00, 246.63it/s, loss=0.678, v_num=1, val_l\u001b[A\n",
      "Epoch 18: 100%|▉| 1090/1095 [00:04<00:00, 246.34it/s, loss=0.678, v_num=1, val_l\u001b[A\n",
      "Epoch 18: 100%|▉| 1091/1095 [00:04<00:00, 246.05it/s, loss=0.678, v_num=1, val_l\u001b[A\n",
      "Epoch 18: 100%|▉| 1092/1095 [00:04<00:00, 245.76it/s, loss=0.678, v_num=1, val_l\u001b[A\n",
      "Epoch 18: 100%|▉| 1093/1095 [00:04<00:00, 245.47it/s, loss=0.678, v_num=1, val_l\u001b[A\n",
      "Epoch 18: 100%|▉| 1094/1095 [00:04<00:00, 245.18it/s, loss=0.678, v_num=1, val_l\u001b[A\n",
      "Epoch 18: 100%|█| 1095/1095 [00:04<00:00, 244.78it/s, loss=0.678, v_num=1, val_l\u001b[A\n",
      "Epoch 19:  98%|▉| 1075/1095 [00:05<00:00, 187.19it/s, loss=0.673, v_num=1, val_l\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19:  98%|▉| 1076/1095 [00:06<00:00, 169.76it/s, loss=0.673, v_num=1, val_l\u001b[A\n",
      "Epoch 19:  98%|▉| 1077/1095 [00:06<00:00, 169.67it/s, loss=0.673, v_num=1, val_l\u001b[A\n",
      "Epoch 19:  98%|▉| 1078/1095 [00:06<00:00, 169.79it/s, loss=0.673, v_num=1, val_l\u001b[A\n",
      "Epoch 19:  99%|▉| 1079/1095 [00:06<00:00, 169.74it/s, loss=0.673, v_num=1, val_l\u001b[A\n",
      "Epoch 19:  99%|▉| 1080/1095 [00:06<00:00, 169.65it/s, loss=0.673, v_num=1, val_l\u001b[A\n",
      "Epoch 19:  99%|▉| 1081/1095 [00:06<00:00, 169.56it/s, loss=0.673, v_num=1, val_l\u001b[A\n",
      "Epoch 19:  99%|▉| 1082/1095 [00:06<00:00, 169.55it/s, loss=0.673, v_num=1, val_l\u001b[A\n",
      "Epoch 19:  99%|▉| 1083/1095 [00:06<00:00, 169.46it/s, loss=0.673, v_num=1, val_l\u001b[A\n",
      "Epoch 19:  99%|▉| 1084/1095 [00:06<00:00, 169.37it/s, loss=0.673, v_num=1, val_l\u001b[A\n",
      "Epoch 19:  99%|▉| 1085/1095 [00:06<00:00, 169.28it/s, loss=0.673, v_num=1, val_l\u001b[A\n",
      "Epoch 19:  99%|▉| 1086/1095 [00:06<00:00, 169.19it/s, loss=0.673, v_num=1, val_l\u001b[A\n",
      "Epoch 19:  99%|▉| 1087/1095 [00:06<00:00, 169.10it/s, loss=0.673, v_num=1, val_l\u001b[A\n",
      "Epoch 19:  99%|▉| 1088/1095 [00:06<00:00, 169.01it/s, loss=0.673, v_num=1, val_l\u001b[A\n",
      "Epoch 19:  99%|▉| 1089/1095 [00:06<00:00, 168.92it/s, loss=0.673, v_num=1, val_l\u001b[A\n",
      "Epoch 19: 100%|▉| 1090/1095 [00:06<00:00, 168.83it/s, loss=0.673, v_num=1, val_l\u001b[A\n",
      "Epoch 19: 100%|▉| 1091/1095 [00:06<00:00, 168.75it/s, loss=0.673, v_num=1, val_l\u001b[A\n",
      "Epoch 19: 100%|▉| 1092/1095 [00:06<00:00, 168.66it/s, loss=0.673, v_num=1, val_l\u001b[A\n",
      "Epoch 19: 100%|▉| 1093/1095 [00:06<00:00, 168.57it/s, loss=0.673, v_num=1, val_l\u001b[A\n",
      "Epoch 19: 100%|▉| 1094/1095 [00:06<00:00, 168.48it/s, loss=0.673, v_num=1, val_l\u001b[A\n",
      "Epoch 19: 100%|█| 1095/1095 [00:06<00:00, 168.34it/s, loss=0.673, v_num=1, val_l\u001b[A\n",
      "Epoch 19: 100%|█| 1095/1095 [00:06<00:00, 168.26it/s, loss=0.673, v_num=1, val_l\u001b[A`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "Epoch 19: 100%|█| 1095/1095 [00:06<00:00, 166.75it/s, loss=0.673, v_num=1, val_l\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing DataLoader 0: 100%|█████████████████████| 20/20 [00:00<00:00, 70.45it/s]\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m  Runningstage.testing   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m                           \u001b[0m┃\n",
      "┃\u001b[1m \u001b[0m\u001b[1m         metric          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7205822467803955    \u001b[0m\u001b[35m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6591294407844543    \u001b[0m\u001b[35m \u001b[0m│\n",
      "└───────────────────────────┴───────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# This saves model check point and logging under ./models folder inside root dir atcs-lstm. \n",
    "# Train avg embedding model\n",
    "! python train.py --encoder=avg --save_dir='./models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 1234\n",
      "unique token len 37211\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Missing logger folder: ./models/lstm\n",
      "/home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory ./models exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type               | Params\n",
      "------------------------------------------------\n",
      "0 | encoder  | LSTMEncoder        | 30.4 M\n",
      "1 | mlp      | Sequential         | 4.5 M \n",
      "2 | accuracy | MulticlassAccuracy | 0     \n",
      "------------------------------------------------\n",
      "23.7 M    Trainable params\n",
      "11.2 M    Non-trainable params\n",
      "34.9 M    Total params\n",
      "139.495   Total estimated model params size (MB)\n",
      "Epoch 0:  98%|████████▊| 1075/1095 [01:13<00:01, 14.59it/s, loss=0.983, v_num=0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1076/1095 [01:14<00:01, 14.47it/s, loss=0.983, v_num=0]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1077/1095 [01:14<00:01, 14.48it/s, loss=0.983, v_num=0]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1078/1095 [01:14<00:01, 14.49it/s, loss=0.983, v_num=0]\u001b[A\n",
      "Epoch 0:  99%|████████▊| 1079/1095 [01:14<00:01, 14.50it/s, loss=0.983, v_num=0]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1080/1095 [01:14<00:01, 14.51it/s, loss=0.983, v_num=0]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1081/1095 [01:14<00:00, 14.52it/s, loss=0.983, v_num=0]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1082/1095 [01:14<00:00, 14.53it/s, loss=0.983, v_num=0]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1083/1095 [01:14<00:00, 14.53it/s, loss=0.983, v_num=0]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1084/1095 [01:14<00:00, 14.54it/s, loss=0.983, v_num=0]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1085/1095 [01:14<00:00, 14.55it/s, loss=0.983, v_num=0]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1086/1095 [01:14<00:00, 14.56it/s, loss=0.983, v_num=0]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1087/1095 [01:14<00:00, 14.57it/s, loss=0.983, v_num=0]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1088/1095 [01:14<00:00, 14.58it/s, loss=0.983, v_num=0]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1089/1095 [01:14<00:00, 14.59it/s, loss=0.983, v_num=0]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 1090/1095 [01:14<00:00, 14.60it/s, loss=0.983, v_num=0]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 1091/1095 [01:14<00:00, 14.60it/s, loss=0.983, v_num=0]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 1092/1095 [01:14<00:00, 14.61it/s, loss=0.983, v_num=0]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 1093/1095 [01:14<00:00, 14.62it/s, loss=0.983, v_num=0]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 1094/1095 [01:14<00:00, 14.63it/s, loss=0.983, v_num=0]\u001b[A\n",
      "Epoch 0: 100%|█| 1095/1095 [01:14<00:00, 14.64it/s, loss=0.983, v_num=0, val_los\u001b[A\n",
      "Epoch 1:  98%|▉| 1075/1095 [01:11<00:01, 15.05it/s, loss=0.82, v_num=0, val_loss\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  98%|▉| 1076/1095 [01:12<00:01, 14.92it/s, loss=0.82, v_num=0, val_loss\u001b[A\n",
      "Epoch 1:  98%|▉| 1077/1095 [01:12<00:01, 14.93it/s, loss=0.82, v_num=0, val_loss\u001b[A\n",
      "Epoch 1:  98%|▉| 1078/1095 [01:12<00:01, 14.94it/s, loss=0.82, v_num=0, val_loss\u001b[A\n",
      "Epoch 1:  99%|▉| 1079/1095 [01:12<00:01, 14.95it/s, loss=0.82, v_num=0, val_loss\u001b[A\n",
      "Epoch 1:  99%|▉| 1080/1095 [01:12<00:01, 14.95it/s, loss=0.82, v_num=0, val_loss\u001b[A\n",
      "Epoch 1:  99%|▉| 1081/1095 [01:12<00:00, 14.96it/s, loss=0.82, v_num=0, val_loss\u001b[A\n",
      "Epoch 1:  99%|▉| 1082/1095 [01:12<00:00, 14.97it/s, loss=0.82, v_num=0, val_loss\u001b[A\n",
      "Epoch 1:  99%|▉| 1083/1095 [01:12<00:00, 14.98it/s, loss=0.82, v_num=0, val_loss\u001b[A\n",
      "Epoch 1:  99%|▉| 1084/1095 [01:12<00:00, 14.99it/s, loss=0.82, v_num=0, val_loss\u001b[A\n",
      "Epoch 1:  99%|▉| 1085/1095 [01:12<00:00, 15.00it/s, loss=0.82, v_num=0, val_loss\u001b[A\n",
      "Epoch 1:  99%|▉| 1086/1095 [01:12<00:00, 15.01it/s, loss=0.82, v_num=0, val_loss\u001b[A\n",
      "Epoch 1:  99%|▉| 1087/1095 [01:12<00:00, 15.02it/s, loss=0.82, v_num=0, val_loss\u001b[A\n",
      "Epoch 1:  99%|▉| 1088/1095 [01:12<00:00, 15.02it/s, loss=0.82, v_num=0, val_loss\u001b[A\n",
      "Epoch 1:  99%|▉| 1089/1095 [01:12<00:00, 15.03it/s, loss=0.82, v_num=0, val_loss\u001b[A\n",
      "Epoch 1: 100%|▉| 1090/1095 [01:12<00:00, 15.04it/s, loss=0.82, v_num=0, val_loss\u001b[A\n",
      "Epoch 1: 100%|▉| 1091/1095 [01:12<00:00, 15.05it/s, loss=0.82, v_num=0, val_loss\u001b[A\n",
      "Epoch 1: 100%|▉| 1092/1095 [01:12<00:00, 15.06it/s, loss=0.82, v_num=0, val_loss\u001b[A\n",
      "Epoch 1: 100%|▉| 1093/1095 [01:12<00:00, 15.07it/s, loss=0.82, v_num=0, val_loss\u001b[A\n",
      "Epoch 1: 100%|▉| 1094/1095 [01:12<00:00, 15.08it/s, loss=0.82, v_num=0, val_loss\u001b[A\n",
      "Epoch 1: 100%|█| 1095/1095 [01:12<00:00, 15.09it/s, loss=0.82, v_num=0, val_loss\u001b[A\n",
      "Epoch 2:  98%|▉| 1075/1095 [01:11<00:01, 15.03it/s, loss=0.759, v_num=0, val_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  98%|▉| 1076/1095 [01:12<00:01, 14.90it/s, loss=0.759, v_num=0, val_los\u001b[A\n",
      "Epoch 2:  98%|▉| 1077/1095 [01:12<00:01, 14.91it/s, loss=0.759, v_num=0, val_los\u001b[A\n",
      "Epoch 2:  98%|▉| 1078/1095 [01:12<00:01, 14.92it/s, loss=0.759, v_num=0, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 1079/1095 [01:12<00:01, 14.93it/s, loss=0.759, v_num=0, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 1080/1095 [01:12<00:01, 14.94it/s, loss=0.759, v_num=0, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 1081/1095 [01:12<00:00, 14.95it/s, loss=0.759, v_num=0, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 1082/1095 [01:12<00:00, 14.96it/s, loss=0.759, v_num=0, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 1083/1095 [01:12<00:00, 14.96it/s, loss=0.759, v_num=0, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 1084/1095 [01:12<00:00, 14.97it/s, loss=0.759, v_num=0, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 1085/1095 [01:12<00:00, 14.98it/s, loss=0.759, v_num=0, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 1086/1095 [01:12<00:00, 14.99it/s, loss=0.759, v_num=0, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 1087/1095 [01:12<00:00, 15.00it/s, loss=0.759, v_num=0, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 1088/1095 [01:12<00:00, 15.01it/s, loss=0.759, v_num=0, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 1089/1095 [01:12<00:00, 15.02it/s, loss=0.759, v_num=0, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 1090/1095 [01:12<00:00, 15.02it/s, loss=0.759, v_num=0, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 1091/1095 [01:12<00:00, 15.03it/s, loss=0.759, v_num=0, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 1092/1095 [01:12<00:00, 15.04it/s, loss=0.759, v_num=0, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 1093/1095 [01:12<00:00, 15.05it/s, loss=0.759, v_num=0, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 1094/1095 [01:12<00:00, 15.06it/s, loss=0.759, v_num=0, val_los\u001b[A\n",
      "Epoch 2: 100%|█| 1095/1095 [01:12<00:00, 15.07it/s, loss=0.759, v_num=0, val_los\u001b[A\n",
      "Epoch 3:  98%|▉| 1075/1095 [01:11<00:01, 15.03it/s, loss=0.706, v_num=0, val_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  98%|▉| 1076/1095 [01:12<00:01, 14.91it/s, loss=0.706, v_num=0, val_los\u001b[A\n",
      "Epoch 3:  98%|▉| 1077/1095 [01:12<00:01, 14.92it/s, loss=0.706, v_num=0, val_los\u001b[A\n",
      "Epoch 3:  98%|▉| 1078/1095 [01:12<00:01, 14.92it/s, loss=0.706, v_num=0, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 1079/1095 [01:12<00:01, 14.93it/s, loss=0.706, v_num=0, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 1080/1095 [01:12<00:01, 14.94it/s, loss=0.706, v_num=0, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 1081/1095 [01:12<00:00, 14.95it/s, loss=0.706, v_num=0, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 1082/1095 [01:12<00:00, 14.96it/s, loss=0.706, v_num=0, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 1083/1095 [01:12<00:00, 14.97it/s, loss=0.706, v_num=0, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 1084/1095 [01:12<00:00, 14.98it/s, loss=0.706, v_num=0, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 1085/1095 [01:12<00:00, 14.99it/s, loss=0.706, v_num=0, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 1086/1095 [01:12<00:00, 14.99it/s, loss=0.706, v_num=0, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 1087/1095 [01:12<00:00, 15.00it/s, loss=0.706, v_num=0, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 1088/1095 [01:12<00:00, 15.01it/s, loss=0.706, v_num=0, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 1089/1095 [01:12<00:00, 15.02it/s, loss=0.706, v_num=0, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 1090/1095 [01:12<00:00, 15.03it/s, loss=0.706, v_num=0, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 1091/1095 [01:12<00:00, 15.04it/s, loss=0.706, v_num=0, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 1092/1095 [01:12<00:00, 15.05it/s, loss=0.706, v_num=0, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 1093/1095 [01:12<00:00, 15.06it/s, loss=0.706, v_num=0, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 1094/1095 [01:12<00:00, 15.07it/s, loss=0.706, v_num=0, val_los\u001b[A\n",
      "Epoch 4:  98%|▉| 1075/1095 [01:11<00:01, 15.03it/s, loss=0.654, v_num=0, val_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  98%|▉| 1076/1095 [01:12<00:01, 14.91it/s, loss=0.654, v_num=0, val_los\u001b[A\n",
      "Epoch 4:  98%|▉| 1077/1095 [01:12<00:01, 14.92it/s, loss=0.654, v_num=0, val_los\u001b[A\n",
      "Epoch 4:  98%|▉| 1078/1095 [01:12<00:01, 14.93it/s, loss=0.654, v_num=0, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 1079/1095 [01:12<00:01, 14.93it/s, loss=0.654, v_num=0, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 1080/1095 [01:12<00:01, 14.94it/s, loss=0.654, v_num=0, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 1081/1095 [01:12<00:00, 14.95it/s, loss=0.654, v_num=0, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 1082/1095 [01:12<00:00, 14.96it/s, loss=0.654, v_num=0, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 1083/1095 [01:12<00:00, 14.97it/s, loss=0.654, v_num=0, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 1084/1095 [01:12<00:00, 14.98it/s, loss=0.654, v_num=0, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 1085/1095 [01:12<00:00, 14.99it/s, loss=0.654, v_num=0, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 1086/1095 [01:12<00:00, 14.99it/s, loss=0.654, v_num=0, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 1087/1095 [01:12<00:00, 15.00it/s, loss=0.654, v_num=0, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 1088/1095 [01:12<00:00, 15.01it/s, loss=0.654, v_num=0, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 1089/1095 [01:12<00:00, 15.02it/s, loss=0.654, v_num=0, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 1090/1095 [01:12<00:00, 15.03it/s, loss=0.654, v_num=0, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 1091/1095 [01:12<00:00, 15.04it/s, loss=0.654, v_num=0, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 1092/1095 [01:12<00:00, 15.05it/s, loss=0.654, v_num=0, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 1093/1095 [01:12<00:00, 15.06it/s, loss=0.654, v_num=0, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 1094/1095 [01:12<00:00, 15.07it/s, loss=0.654, v_num=0, val_los\u001b[A\n",
      "Epoch 4: 100%|█| 1095/1095 [01:12<00:00, 15.08it/s, loss=0.654, v_num=0, val_los\u001b[A\n",
      "Epoch 5:  98%|▉| 1075/1095 [01:11<00:01, 15.03it/s, loss=0.619, v_num=0, val_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  98%|▉| 1076/1095 [01:12<00:01, 14.90it/s, loss=0.619, v_num=0, val_los\u001b[A\n",
      "Epoch 5:  98%|▉| 1077/1095 [01:12<00:01, 14.91it/s, loss=0.619, v_num=0, val_los\u001b[A\n",
      "Epoch 5:  98%|▉| 1078/1095 [01:12<00:01, 14.92it/s, loss=0.619, v_num=0, val_los\u001b[A\n",
      "Epoch 5:  99%|▉| 1079/1095 [01:12<00:01, 14.93it/s, loss=0.619, v_num=0, val_los\u001b[A\n",
      "Epoch 5:  99%|▉| 1080/1095 [01:12<00:01, 14.94it/s, loss=0.619, v_num=0, val_los\u001b[A\n",
      "Epoch 5:  99%|▉| 1081/1095 [01:12<00:00, 14.95it/s, loss=0.619, v_num=0, val_los\u001b[A\n",
      "Epoch 5:  99%|▉| 1082/1095 [01:12<00:00, 14.96it/s, loss=0.619, v_num=0, val_los\u001b[A\n",
      "Epoch 5:  99%|▉| 1083/1095 [01:12<00:00, 14.97it/s, loss=0.619, v_num=0, val_los\u001b[A\n",
      "Epoch 5:  99%|▉| 1084/1095 [01:12<00:00, 14.97it/s, loss=0.619, v_num=0, val_los\u001b[A\n",
      "Epoch 5:  99%|▉| 1085/1095 [01:12<00:00, 14.98it/s, loss=0.619, v_num=0, val_los\u001b[A\n",
      "Epoch 5:  99%|▉| 1086/1095 [01:12<00:00, 14.99it/s, loss=0.619, v_num=0, val_los\u001b[A\n",
      "Epoch 5:  99%|▉| 1087/1095 [01:12<00:00, 15.00it/s, loss=0.619, v_num=0, val_los\u001b[A\n",
      "Epoch 5:  99%|▉| 1088/1095 [01:12<00:00, 15.01it/s, loss=0.619, v_num=0, val_los\u001b[A\n",
      "Epoch 5:  99%|▉| 1089/1095 [01:12<00:00, 15.02it/s, loss=0.619, v_num=0, val_los\u001b[A\n",
      "Epoch 5: 100%|▉| 1090/1095 [01:12<00:00, 15.03it/s, loss=0.619, v_num=0, val_los\u001b[A\n",
      "Epoch 5: 100%|▉| 1091/1095 [01:12<00:00, 15.04it/s, loss=0.619, v_num=0, val_los\u001b[A\n",
      "Epoch 5: 100%|▉| 1092/1095 [01:12<00:00, 15.05it/s, loss=0.619, v_num=0, val_los\u001b[A\n",
      "Epoch 5: 100%|▉| 1093/1095 [01:12<00:00, 15.05it/s, loss=0.619, v_num=0, val_los\u001b[A\n",
      "Epoch 5: 100%|▉| 1094/1095 [01:12<00:00, 15.06it/s, loss=0.619, v_num=0, val_los\u001b[A\n",
      "Epoch 5: 100%|█| 1095/1095 [01:12<00:00, 15.07it/s, loss=0.619, v_num=0, val_los\u001b[A\n",
      "Epoch 6:  98%|▉| 1075/1095 [01:11<00:01, 15.04it/s, loss=0.587, v_num=0, val_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  98%|▉| 1076/1095 [01:12<00:01, 14.91it/s, loss=0.587, v_num=0, val_los\u001b[A\n",
      "Epoch 6:  98%|▉| 1077/1095 [01:12<00:01, 14.92it/s, loss=0.587, v_num=0, val_los\u001b[A\n",
      "Epoch 6:  98%|▉| 1078/1095 [01:12<00:01, 14.93it/s, loss=0.587, v_num=0, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 1079/1095 [01:12<00:01, 14.94it/s, loss=0.587, v_num=0, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 1080/1095 [01:12<00:01, 14.94it/s, loss=0.587, v_num=0, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 1081/1095 [01:12<00:00, 14.95it/s, loss=0.587, v_num=0, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 1082/1095 [01:12<00:00, 14.96it/s, loss=0.587, v_num=0, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 1083/1095 [01:12<00:00, 14.97it/s, loss=0.587, v_num=0, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 1084/1095 [01:12<00:00, 14.98it/s, loss=0.587, v_num=0, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 1085/1095 [01:12<00:00, 14.99it/s, loss=0.587, v_num=0, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 1086/1095 [01:12<00:00, 15.00it/s, loss=0.587, v_num=0, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 1087/1095 [01:12<00:00, 15.01it/s, loss=0.587, v_num=0, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 1088/1095 [01:12<00:00, 15.01it/s, loss=0.587, v_num=0, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 1089/1095 [01:12<00:00, 15.02it/s, loss=0.587, v_num=0, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 1090/1095 [01:12<00:00, 15.03it/s, loss=0.587, v_num=0, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 1091/1095 [01:12<00:00, 15.04it/s, loss=0.587, v_num=0, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 1092/1095 [01:12<00:00, 15.05it/s, loss=0.587, v_num=0, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 1093/1095 [01:12<00:00, 15.06it/s, loss=0.587, v_num=0, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 1094/1095 [01:12<00:00, 15.07it/s, loss=0.587, v_num=0, val_los\u001b[A\n",
      "Epoch 6: 100%|█| 1095/1095 [01:12<00:00, 15.08it/s, loss=0.587, v_num=0, val_los\u001b[A\n",
      "Epoch 7:  98%|▉| 1075/1095 [01:11<00:01, 15.03it/s, loss=0.563, v_num=0, val_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  98%|▉| 1076/1095 [01:12<00:01, 14.90it/s, loss=0.563, v_num=0, val_los\u001b[A\n",
      "Epoch 7:  98%|▉| 1077/1095 [01:12<00:01, 14.91it/s, loss=0.563, v_num=0, val_los\u001b[A\n",
      "Epoch 7:  98%|▉| 1078/1095 [01:12<00:01, 14.92it/s, loss=0.563, v_num=0, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 1079/1095 [01:12<00:01, 14.93it/s, loss=0.563, v_num=0, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 1080/1095 [01:12<00:01, 14.94it/s, loss=0.563, v_num=0, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 1081/1095 [01:12<00:00, 14.95it/s, loss=0.563, v_num=0, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 1082/1095 [01:12<00:00, 14.95it/s, loss=0.563, v_num=0, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 1083/1095 [01:12<00:00, 14.96it/s, loss=0.563, v_num=0, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 1084/1095 [01:12<00:00, 14.97it/s, loss=0.563, v_num=0, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 1085/1095 [01:12<00:00, 14.98it/s, loss=0.563, v_num=0, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 1086/1095 [01:12<00:00, 14.99it/s, loss=0.563, v_num=0, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 1087/1095 [01:12<00:00, 15.00it/s, loss=0.563, v_num=0, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 1088/1095 [01:12<00:00, 15.01it/s, loss=0.563, v_num=0, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 1089/1095 [01:12<00:00, 15.02it/s, loss=0.563, v_num=0, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 1090/1095 [01:12<00:00, 15.03it/s, loss=0.563, v_num=0, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 1091/1095 [01:12<00:00, 15.03it/s, loss=0.563, v_num=0, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 1092/1095 [01:12<00:00, 15.04it/s, loss=0.563, v_num=0, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 1093/1095 [01:12<00:00, 15.05it/s, loss=0.563, v_num=0, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 1094/1095 [01:12<00:00, 15.06it/s, loss=0.563, v_num=0, val_los\u001b[A\n",
      "Epoch 7: 100%|█| 1095/1095 [01:12<00:00, 15.07it/s, loss=0.563, v_num=0, val_los\u001b[A\n",
      "Epoch 8:  98%|▉| 1075/1095 [01:11<00:01, 15.03it/s, loss=0.555, v_num=0, val_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8:  98%|▉| 1076/1095 [01:12<00:01, 14.91it/s, loss=0.555, v_num=0, val_los\u001b[A\n",
      "Epoch 8:  98%|▉| 1077/1095 [01:12<00:01, 14.92it/s, loss=0.555, v_num=0, val_los\u001b[A\n",
      "Epoch 8:  98%|▉| 1078/1095 [01:12<00:01, 14.93it/s, loss=0.555, v_num=0, val_los\u001b[A\n",
      "Epoch 8:  99%|▉| 1079/1095 [01:12<00:01, 14.94it/s, loss=0.555, v_num=0, val_los\u001b[A\n",
      "Epoch 8:  99%|▉| 1080/1095 [01:12<00:01, 14.95it/s, loss=0.555, v_num=0, val_los\u001b[A\n",
      "Epoch 8:  99%|▉| 1081/1095 [01:12<00:00, 14.96it/s, loss=0.555, v_num=0, val_los\u001b[A\n",
      "Epoch 8:  99%|▉| 1082/1095 [01:12<00:00, 14.96it/s, loss=0.555, v_num=0, val_los\u001b[A\n",
      "Epoch 8:  99%|▉| 1083/1095 [01:12<00:00, 14.97it/s, loss=0.555, v_num=0, val_los\u001b[A\n",
      "Epoch 8:  99%|▉| 1084/1095 [01:12<00:00, 14.98it/s, loss=0.555, v_num=0, val_los\u001b[A\n",
      "Epoch 8:  99%|▉| 1085/1095 [01:12<00:00, 14.99it/s, loss=0.555, v_num=0, val_los\u001b[A\n",
      "Epoch 8:  99%|▉| 1086/1095 [01:12<00:00, 15.00it/s, loss=0.555, v_num=0, val_los\u001b[A\n",
      "Epoch 8:  99%|▉| 1087/1095 [01:12<00:00, 15.01it/s, loss=0.555, v_num=0, val_los\u001b[A\n",
      "Epoch 8:  99%|▉| 1088/1095 [01:12<00:00, 15.02it/s, loss=0.555, v_num=0, val_los\u001b[A\n",
      "Epoch 8:  99%|▉| 1089/1095 [01:12<00:00, 15.02it/s, loss=0.555, v_num=0, val_los\u001b[A\n",
      "Epoch 8: 100%|▉| 1090/1095 [01:12<00:00, 15.03it/s, loss=0.555, v_num=0, val_los\u001b[A\n",
      "Epoch 8: 100%|▉| 1091/1095 [01:12<00:00, 15.04it/s, loss=0.555, v_num=0, val_los\u001b[A\n",
      "Epoch 8: 100%|▉| 1092/1095 [01:12<00:00, 15.05it/s, loss=0.555, v_num=0, val_los\u001b[A\n",
      "Epoch 8: 100%|▉| 1093/1095 [01:12<00:00, 15.06it/s, loss=0.555, v_num=0, val_los\u001b[A\n",
      "Epoch 8: 100%|▉| 1094/1095 [01:12<00:00, 15.07it/s, loss=0.555, v_num=0, val_los\u001b[A\n",
      "Epoch 8: 100%|█| 1095/1095 [01:12<00:00, 15.08it/s, loss=0.555, v_num=0, val_los\u001b[A\n",
      "Epoch 9:  98%|▉| 1075/1095 [01:11<00:01, 15.04it/s, loss=0.533, v_num=0, val_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9:  98%|▉| 1076/1095 [01:12<00:01, 14.92it/s, loss=0.533, v_num=0, val_los\u001b[A\n",
      "Epoch 9:  98%|▉| 1077/1095 [01:12<00:01, 14.92it/s, loss=0.533, v_num=0, val_los\u001b[A\n",
      "Epoch 9:  98%|▉| 1078/1095 [01:12<00:01, 14.93it/s, loss=0.533, v_num=0, val_los\u001b[A\n",
      "Epoch 9:  99%|▉| 1079/1095 [01:12<00:01, 14.94it/s, loss=0.533, v_num=0, val_los\u001b[A\n",
      "Epoch 9:  99%|▉| 1080/1095 [01:12<00:01, 14.95it/s, loss=0.533, v_num=0, val_los\u001b[A\n",
      "Epoch 9:  99%|▉| 1081/1095 [01:12<00:00, 14.96it/s, loss=0.533, v_num=0, val_los\u001b[A\n",
      "Epoch 9:  99%|▉| 1082/1095 [01:12<00:00, 14.97it/s, loss=0.533, v_num=0, val_los\u001b[A\n",
      "Epoch 9:  99%|▉| 1083/1095 [01:12<00:00, 14.98it/s, loss=0.533, v_num=0, val_los\u001b[A\n",
      "Epoch 9:  99%|▉| 1084/1095 [01:12<00:00, 14.99it/s, loss=0.533, v_num=0, val_los\u001b[A\n",
      "Epoch 9:  99%|▉| 1085/1095 [01:12<00:00, 15.00it/s, loss=0.533, v_num=0, val_los\u001b[A\n",
      "Epoch 9:  99%|▉| 1086/1095 [01:12<00:00, 15.00it/s, loss=0.533, v_num=0, val_los\u001b[A\n",
      "Epoch 9:  99%|▉| 1087/1095 [01:12<00:00, 15.01it/s, loss=0.533, v_num=0, val_los\u001b[A\n",
      "Epoch 9:  99%|▉| 1088/1095 [01:12<00:00, 15.02it/s, loss=0.533, v_num=0, val_los\u001b[A\n",
      "Epoch 9:  99%|▉| 1089/1095 [01:12<00:00, 15.03it/s, loss=0.533, v_num=0, val_los\u001b[A\n",
      "Epoch 9: 100%|▉| 1090/1095 [01:12<00:00, 15.04it/s, loss=0.533, v_num=0, val_los\u001b[A\n",
      "Epoch 9: 100%|▉| 1091/1095 [01:12<00:00, 15.05it/s, loss=0.533, v_num=0, val_los\u001b[A\n",
      "Epoch 9: 100%|▉| 1092/1095 [01:12<00:00, 15.06it/s, loss=0.533, v_num=0, val_los\u001b[A\n",
      "Epoch 9: 100%|▉| 1093/1095 [01:12<00:00, 15.07it/s, loss=0.533, v_num=0, val_los\u001b[A\n",
      "Epoch 9: 100%|▉| 1094/1095 [01:12<00:00, 15.07it/s, loss=0.533, v_num=0, val_los\u001b[A\n",
      "Epoch 9: 100%|█| 1095/1095 [01:12<00:00, 15.08it/s, loss=0.533, v_num=0, val_los\u001b[A\n",
      "Epoch 10:  98%|▉| 1075/1095 [01:11<00:01, 15.04it/s, loss=0.502, v_num=0, val_lo\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10:  98%|▉| 1076/1095 [01:12<00:01, 14.91it/s, loss=0.502, v_num=0, val_lo\u001b[A\n",
      "Epoch 10:  98%|▉| 1077/1095 [01:12<00:01, 14.92it/s, loss=0.502, v_num=0, val_lo\u001b[A\n",
      "Epoch 10:  98%|▉| 1078/1095 [01:12<00:01, 14.93it/s, loss=0.502, v_num=0, val_lo\u001b[A\n",
      "Epoch 10:  99%|▉| 1079/1095 [01:12<00:01, 14.93it/s, loss=0.502, v_num=0, val_lo\u001b[A\n",
      "Epoch 10:  99%|▉| 1080/1095 [01:12<00:01, 14.94it/s, loss=0.502, v_num=0, val_lo\u001b[A\n",
      "Epoch 10:  99%|▉| 1081/1095 [01:12<00:00, 14.95it/s, loss=0.502, v_num=0, val_lo\u001b[A\n",
      "Epoch 10:  99%|▉| 1082/1095 [01:12<00:00, 14.96it/s, loss=0.502, v_num=0, val_lo\u001b[A\n",
      "Epoch 10:  99%|▉| 1083/1095 [01:12<00:00, 14.97it/s, loss=0.502, v_num=0, val_lo\u001b[A\n",
      "Epoch 10:  99%|▉| 1084/1095 [01:12<00:00, 14.98it/s, loss=0.502, v_num=0, val_lo\u001b[A\n",
      "Epoch 10:  99%|▉| 1085/1095 [01:12<00:00, 14.99it/s, loss=0.502, v_num=0, val_lo\u001b[A\n",
      "Epoch 10:  99%|▉| 1086/1095 [01:12<00:00, 14.99it/s, loss=0.502, v_num=0, val_lo\u001b[A\n",
      "Epoch 10:  99%|▉| 1087/1095 [01:12<00:00, 15.00it/s, loss=0.502, v_num=0, val_lo\u001b[A\n",
      "Epoch 10:  99%|▉| 1088/1095 [01:12<00:00, 15.01it/s, loss=0.502, v_num=0, val_lo\u001b[A\n",
      "Epoch 10:  99%|▉| 1089/1095 [01:12<00:00, 15.02it/s, loss=0.502, v_num=0, val_lo\u001b[A\n",
      "Epoch 10: 100%|▉| 1090/1095 [01:12<00:00, 15.03it/s, loss=0.502, v_num=0, val_lo\u001b[A\n",
      "Epoch 10: 100%|▉| 1091/1095 [01:12<00:00, 15.04it/s, loss=0.502, v_num=0, val_lo\u001b[A\n",
      "Epoch 10: 100%|▉| 1092/1095 [01:12<00:00, 15.05it/s, loss=0.502, v_num=0, val_lo\u001b[A\n",
      "Epoch 10: 100%|▉| 1093/1095 [01:12<00:00, 15.06it/s, loss=0.502, v_num=0, val_lo\u001b[A\n",
      "Epoch 10: 100%|▉| 1094/1095 [01:12<00:00, 15.07it/s, loss=0.502, v_num=0, val_lo\u001b[A\n",
      "Epoch 10: 100%|█| 1095/1095 [01:12<00:00, 15.08it/s, loss=0.502, v_num=0, val_lo\u001b[A\n",
      "Epoch 11:  98%|▉| 1075/1095 [01:11<00:01, 15.03it/s, loss=0.487, v_num=0, val_lo\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11:  98%|▉| 1076/1095 [01:12<00:01, 14.90it/s, loss=0.487, v_num=0, val_lo\u001b[A\n",
      "Epoch 11:  98%|▉| 1077/1095 [01:12<00:01, 14.91it/s, loss=0.487, v_num=0, val_lo\u001b[A\n",
      "Epoch 11:  98%|▉| 1078/1095 [01:12<00:01, 14.92it/s, loss=0.487, v_num=0, val_lo\u001b[A\n",
      "Epoch 11:  99%|▉| 1079/1095 [01:12<00:01, 14.93it/s, loss=0.487, v_num=0, val_lo\u001b[A\n",
      "Epoch 11:  99%|▉| 1080/1095 [01:12<00:01, 14.94it/s, loss=0.487, v_num=0, val_lo\u001b[A\n",
      "Epoch 11:  99%|▉| 1081/1095 [01:12<00:00, 14.95it/s, loss=0.487, v_num=0, val_lo\u001b[A\n",
      "Epoch 11:  99%|▉| 1082/1095 [01:12<00:00, 14.96it/s, loss=0.487, v_num=0, val_lo\u001b[A\n",
      "Epoch 11:  99%|▉| 1083/1095 [01:12<00:00, 14.97it/s, loss=0.487, v_num=0, val_lo\u001b[A\n",
      "Epoch 11:  99%|▉| 1084/1095 [01:12<00:00, 14.98it/s, loss=0.487, v_num=0, val_lo\u001b[A\n",
      "Epoch 11:  99%|▉| 1085/1095 [01:12<00:00, 14.98it/s, loss=0.487, v_num=0, val_lo\u001b[A\n",
      "Epoch 11:  99%|▉| 1086/1095 [01:12<00:00, 14.99it/s, loss=0.487, v_num=0, val_lo\u001b[A\n",
      "Epoch 11:  99%|▉| 1087/1095 [01:12<00:00, 15.00it/s, loss=0.487, v_num=0, val_lo\u001b[A\n",
      "Epoch 11:  99%|▉| 1088/1095 [01:12<00:00, 15.01it/s, loss=0.487, v_num=0, val_lo\u001b[A\n",
      "Epoch 11:  99%|▉| 1089/1095 [01:12<00:00, 15.02it/s, loss=0.487, v_num=0, val_lo\u001b[A\n",
      "Epoch 11: 100%|▉| 1090/1095 [01:12<00:00, 15.03it/s, loss=0.487, v_num=0, val_lo\u001b[A\n",
      "Epoch 11: 100%|▉| 1091/1095 [01:12<00:00, 15.04it/s, loss=0.487, v_num=0, val_lo\u001b[A\n",
      "Epoch 11: 100%|▉| 1092/1095 [01:12<00:00, 15.04it/s, loss=0.487, v_num=0, val_lo\u001b[A\n",
      "Epoch 11: 100%|▉| 1093/1095 [01:12<00:00, 15.05it/s, loss=0.487, v_num=0, val_lo\u001b[A\n",
      "Epoch 11: 100%|▉| 1094/1095 [01:12<00:00, 15.06it/s, loss=0.487, v_num=0, val_lo\u001b[A\n",
      "Epoch 11: 100%|█| 1095/1095 [01:12<00:00, 15.07it/s, loss=0.487, v_num=0, val_lo\u001b[A\n",
      "Epoch 11: 100%|█| 1095/1095 [01:12<00:00, 15.07it/s, loss=0.487, v_num=0, val_lo\u001b[A\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing DataLoader 0: 100%|█████████████████████| 20/20 [00:00<00:00, 32.43it/s]\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m  Runningstage.testing   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m                           \u001b[0m┃\n",
      "┃\u001b[1m \u001b[0m\u001b[1m         metric          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7750407457351685    \u001b[0m\u001b[35m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5761765241622925    \u001b[0m\u001b[35m \u001b[0m│\n",
      "└───────────────────────────┴───────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Train lstm model\n",
    "! python train.py --encoder=lstm --save_dir='./models'\n",
    "# ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
    "# ┃   Runningstage.testing    ┃                           ┃\n",
    "# ┃          metric           ┃       DataLoader 0        ┃\n",
    "# ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
    "# │       test_accuracy       │    0.8037459254264832     │\n",
    "# │         test_loss         │    0.5357282757759094     │\n",
    "# └───────────────────────────┴───────────────────────────┘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 1234\n",
      "unique token len 37211\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory ./models exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type               | Params\n",
      "------------------------------------------------\n",
      "0 | encoder  | LSTMEncoder        | 49.7 M\n",
      "1 | mlp      | Sequential         | 8.7 M \n",
      "2 | accuracy | MulticlassAccuracy | 0     \n",
      "------------------------------------------------\n",
      "47.2 M    Trainable params\n",
      "11.2 M    Non-trainable params\n",
      "58.3 M    Total params\n",
      "233.277   Total estimated model params size (MB)\n",
      "Epoch 0:  98%|████████▊| 8597/8754 [08:17<00:09, 17.28it/s, loss=0.624, v_num=2]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 8598/8754 [08:18<00:09, 17.25it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 8599/8754 [08:18<00:08, 17.25it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 8600/8754 [08:18<00:08, 17.25it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 8601/8754 [08:18<00:08, 17.25it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 8602/8754 [08:18<00:08, 17.25it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 8603/8754 [08:18<00:08, 17.25it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 8604/8754 [08:18<00:08, 17.26it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 8605/8754 [08:18<00:08, 17.26it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 8606/8754 [08:18<00:08, 17.26it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 8607/8754 [08:18<00:08, 17.26it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 8608/8754 [08:18<00:08, 17.26it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 8609/8754 [08:18<00:08, 17.26it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 8610/8754 [08:18<00:08, 17.26it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 8611/8754 [08:18<00:08, 17.27it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 8612/8754 [08:18<00:08, 17.27it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 8613/8754 [08:18<00:08, 17.27it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 8614/8754 [08:18<00:08, 17.27it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 8615/8754 [08:18<00:08, 17.27it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 8616/8754 [08:18<00:07, 17.27it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 8617/8754 [08:18<00:07, 17.27it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 8618/8754 [08:18<00:07, 17.28it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 8619/8754 [08:18<00:07, 17.28it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 8620/8754 [08:18<00:07, 17.28it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 8621/8754 [08:18<00:07, 17.28it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 8622/8754 [08:18<00:07, 17.28it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▊| 8623/8754 [08:18<00:07, 17.28it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▊| 8624/8754 [08:18<00:07, 17.28it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▊| 8625/8754 [08:18<00:07, 17.28it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▊| 8626/8754 [08:19<00:07, 17.29it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▊| 8627/8754 [08:19<00:07, 17.29it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▊| 8628/8754 [08:19<00:07, 17.29it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▊| 8629/8754 [08:19<00:07, 17.29it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▊| 8630/8754 [08:19<00:07, 17.29it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▊| 8631/8754 [08:19<00:07, 17.29it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▊| 8632/8754 [08:19<00:07, 17.29it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8633/8754 [08:19<00:06, 17.30it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8634/8754 [08:19<00:06, 17.30it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8635/8754 [08:19<00:06, 17.30it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8636/8754 [08:19<00:06, 17.30it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8637/8754 [08:19<00:06, 17.30it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8638/8754 [08:19<00:06, 17.30it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8639/8754 [08:19<00:06, 17.30it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8640/8754 [08:19<00:06, 17.31it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8641/8754 [08:19<00:06, 17.31it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8642/8754 [08:19<00:06, 17.31it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8643/8754 [08:19<00:06, 17.31it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8644/8754 [08:19<00:06, 17.31it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8645/8754 [08:19<00:06, 17.31it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8646/8754 [08:19<00:06, 17.31it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8647/8754 [08:19<00:06, 17.32it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8648/8754 [08:19<00:06, 17.32it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8649/8754 [08:19<00:06, 17.32it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8650/8754 [08:19<00:06, 17.32it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8651/8754 [08:19<00:05, 17.32it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8652/8754 [08:19<00:05, 17.32it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8653/8754 [08:19<00:05, 17.32it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8654/8754 [08:19<00:05, 17.33it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8655/8754 [08:19<00:05, 17.33it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8656/8754 [08:19<00:05, 17.33it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8657/8754 [08:19<00:05, 17.33it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8658/8754 [08:19<00:05, 17.33it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8659/8754 [08:19<00:05, 17.33it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8660/8754 [08:19<00:05, 17.33it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8661/8754 [08:19<00:05, 17.33it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8662/8754 [08:19<00:05, 17.34it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8663/8754 [08:19<00:05, 17.34it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8664/8754 [08:19<00:05, 17.34it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8665/8754 [08:19<00:05, 17.34it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8666/8754 [08:19<00:05, 17.34it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8667/8754 [08:19<00:05, 17.34it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8668/8754 [08:19<00:04, 17.34it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8669/8754 [08:19<00:04, 17.35it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8670/8754 [08:19<00:04, 17.35it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8671/8754 [08:19<00:04, 17.35it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8672/8754 [08:19<00:04, 17.35it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8673/8754 [08:19<00:04, 17.35it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8674/8754 [08:19<00:04, 17.35it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8675/8754 [08:19<00:04, 17.35it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8676/8754 [08:19<00:04, 17.36it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8677/8754 [08:19<00:04, 17.36it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8678/8754 [08:19<00:04, 17.36it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8679/8754 [08:19<00:04, 17.36it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8680/8754 [08:19<00:04, 17.36it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8681/8754 [08:20<00:04, 17.36it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8682/8754 [08:20<00:04, 17.36it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8683/8754 [08:20<00:04, 17.36it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8684/8754 [08:20<00:04, 17.36it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8685/8754 [08:20<00:03, 17.37it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8686/8754 [08:20<00:03, 17.37it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8687/8754 [08:20<00:03, 17.37it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8688/8754 [08:20<00:03, 17.37it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8689/8754 [08:20<00:03, 17.37it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8690/8754 [08:20<00:03, 17.37it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8691/8754 [08:20<00:03, 17.37it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8692/8754 [08:20<00:03, 17.38it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8693/8754 [08:20<00:03, 17.38it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8694/8754 [08:20<00:03, 17.38it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8695/8754 [08:20<00:03, 17.38it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8696/8754 [08:20<00:03, 17.38it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8697/8754 [08:20<00:03, 17.38it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8698/8754 [08:20<00:03, 17.38it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8699/8754 [08:20<00:03, 17.39it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8700/8754 [08:20<00:03, 17.39it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8701/8754 [08:20<00:03, 17.39it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8702/8754 [08:20<00:02, 17.39it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8703/8754 [08:20<00:02, 17.39it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8704/8754 [08:20<00:02, 17.39it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8705/8754 [08:20<00:02, 17.39it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8706/8754 [08:20<00:02, 17.39it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8707/8754 [08:20<00:02, 17.40it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8708/8754 [08:20<00:02, 17.40it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8709/8754 [08:20<00:02, 17.40it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 8710/8754 [08:20<00:02, 17.40it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8711/8754 [08:20<00:02, 17.40it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8712/8754 [08:20<00:02, 17.40it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8713/8754 [08:20<00:02, 17.40it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8714/8754 [08:20<00:02, 17.41it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8715/8754 [08:20<00:02, 17.41it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8716/8754 [08:20<00:02, 17.41it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8717/8754 [08:20<00:02, 17.41it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8718/8754 [08:20<00:02, 17.41it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8719/8754 [08:20<00:02, 17.41it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8720/8754 [08:20<00:01, 17.41it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8721/8754 [08:20<00:01, 17.42it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8722/8754 [08:20<00:01, 17.42it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8723/8754 [08:20<00:01, 17.42it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8724/8754 [08:20<00:01, 17.42it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8725/8754 [08:20<00:01, 17.42it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8726/8754 [08:20<00:01, 17.42it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8727/8754 [08:20<00:01, 17.42it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8728/8754 [08:20<00:01, 17.42it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8729/8754 [08:20<00:01, 17.43it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8730/8754 [08:20<00:01, 17.43it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8731/8754 [08:20<00:01, 17.43it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8732/8754 [08:20<00:01, 17.43it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8733/8754 [08:20<00:01, 17.43it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8734/8754 [08:21<00:01, 17.43it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8735/8754 [08:21<00:01, 17.43it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8736/8754 [08:21<00:01, 17.44it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8737/8754 [08:21<00:00, 17.44it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8738/8754 [08:21<00:00, 17.44it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8739/8754 [08:21<00:00, 17.44it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8740/8754 [08:21<00:00, 17.44it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8741/8754 [08:21<00:00, 17.44it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8742/8754 [08:21<00:00, 17.44it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8743/8754 [08:21<00:00, 17.45it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8744/8754 [08:21<00:00, 17.45it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8745/8754 [08:21<00:00, 17.45it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8746/8754 [08:21<00:00, 17.45it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8747/8754 [08:21<00:00, 17.45it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8748/8754 [08:21<00:00, 17.45it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8749/8754 [08:21<00:00, 17.45it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8750/8754 [08:21<00:00, 17.45it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8751/8754 [08:21<00:00, 17.46it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8752/8754 [08:21<00:00, 17.46it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 8753/8754 [08:21<00:00, 17.46it/s, loss=0.624, v_num=2]\u001b[A\n",
      "Epoch 0: 100%|█| 8754/8754 [08:21<00:00, 17.46it/s, loss=0.624, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  98%|▉| 8597/8754 [08:17<00:09, 17.27it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  98%|▉| 8598/8754 [08:18<00:09, 17.24it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  98%|▉| 8599/8754 [08:18<00:08, 17.24it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  98%|▉| 8600/8754 [08:18<00:08, 17.24it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  98%|▉| 8601/8754 [08:18<00:08, 17.24it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  98%|▉| 8602/8754 [08:18<00:08, 17.25it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  98%|▉| 8603/8754 [08:18<00:08, 17.25it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  98%|▉| 8604/8754 [08:18<00:08, 17.25it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  98%|▉| 8605/8754 [08:18<00:08, 17.25it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  98%|▉| 8606/8754 [08:18<00:08, 17.25it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  98%|▉| 8607/8754 [08:18<00:08, 17.25it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  98%|▉| 8608/8754 [08:18<00:08, 17.25it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  98%|▉| 8609/8754 [08:18<00:08, 17.26it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  98%|▉| 8610/8754 [08:18<00:08, 17.26it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  98%|▉| 8611/8754 [08:18<00:08, 17.26it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  98%|▉| 8612/8754 [08:18<00:08, 17.26it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  98%|▉| 8613/8754 [08:18<00:08, 17.26it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  98%|▉| 8614/8754 [08:19<00:08, 17.26it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  98%|▉| 8615/8754 [08:19<00:08, 17.26it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  98%|▉| 8616/8754 [08:19<00:07, 17.27it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  98%|▉| 8617/8754 [08:19<00:07, 17.27it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  98%|▉| 8618/8754 [08:19<00:07, 17.27it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  98%|▉| 8619/8754 [08:19<00:07, 17.27it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  98%|▉| 8620/8754 [08:19<00:07, 17.27it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  98%|▉| 8621/8754 [08:19<00:07, 17.27it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  98%|▉| 8622/8754 [08:19<00:07, 17.27it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8623/8754 [08:19<00:07, 17.27it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8624/8754 [08:19<00:07, 17.28it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8625/8754 [08:19<00:07, 17.28it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8626/8754 [08:19<00:07, 17.28it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8627/8754 [08:19<00:07, 17.28it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8628/8754 [08:19<00:07, 17.28it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8629/8754 [08:19<00:07, 17.28it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8630/8754 [08:19<00:07, 17.28it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8631/8754 [08:19<00:07, 17.29it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8632/8754 [08:19<00:07, 17.29it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8633/8754 [08:19<00:06, 17.29it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8634/8754 [08:19<00:06, 17.29it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8635/8754 [08:19<00:06, 17.29it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8636/8754 [08:19<00:06, 17.29it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8637/8754 [08:19<00:06, 17.29it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8638/8754 [08:19<00:06, 17.30it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8639/8754 [08:19<00:06, 17.30it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8640/8754 [08:19<00:06, 17.30it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8641/8754 [08:19<00:06, 17.30it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8642/8754 [08:19<00:06, 17.30it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8643/8754 [08:19<00:06, 17.30it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8644/8754 [08:19<00:06, 17.30it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8645/8754 [08:19<00:06, 17.31it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8646/8754 [08:19<00:06, 17.31it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8647/8754 [08:19<00:06, 17.31it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8648/8754 [08:19<00:06, 17.31it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8649/8754 [08:19<00:06, 17.31it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8650/8754 [08:19<00:06, 17.31it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8651/8754 [08:19<00:05, 17.31it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8652/8754 [08:19<00:05, 17.32it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8653/8754 [08:19<00:05, 17.32it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8654/8754 [08:19<00:05, 17.32it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8655/8754 [08:19<00:05, 17.32it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8656/8754 [08:19<00:05, 17.32it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8657/8754 [08:19<00:05, 17.32it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8658/8754 [08:19<00:05, 17.32it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8659/8754 [08:19<00:05, 17.32it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8660/8754 [08:19<00:05, 17.33it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8661/8754 [08:19<00:05, 17.33it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8662/8754 [08:19<00:05, 17.33it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8663/8754 [08:19<00:05, 17.33it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8664/8754 [08:19<00:05, 17.33it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8665/8754 [08:19<00:05, 17.33it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8666/8754 [08:19<00:05, 17.33it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8667/8754 [08:19<00:05, 17.34it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8668/8754 [08:19<00:04, 17.34it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8669/8754 [08:19<00:04, 17.34it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8670/8754 [08:20<00:04, 17.34it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8671/8754 [08:20<00:04, 17.34it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8672/8754 [08:20<00:04, 17.34it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8673/8754 [08:20<00:04, 17.34it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8674/8754 [08:20<00:04, 17.35it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8675/8754 [08:20<00:04, 17.35it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8676/8754 [08:20<00:04, 17.35it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8677/8754 [08:20<00:04, 17.35it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8678/8754 [08:20<00:04, 17.35it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8679/8754 [08:20<00:04, 17.35it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8680/8754 [08:20<00:04, 17.35it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8681/8754 [08:20<00:04, 17.35it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8682/8754 [08:20<00:04, 17.36it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8683/8754 [08:20<00:04, 17.36it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8684/8754 [08:20<00:04, 17.36it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8685/8754 [08:20<00:03, 17.36it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8686/8754 [08:20<00:03, 17.36it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8687/8754 [08:20<00:03, 17.36it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8688/8754 [08:20<00:03, 17.36it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8689/8754 [08:20<00:03, 17.37it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8690/8754 [08:20<00:03, 17.37it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8691/8754 [08:20<00:03, 17.37it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8692/8754 [08:20<00:03, 17.37it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8693/8754 [08:20<00:03, 17.37it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8694/8754 [08:20<00:03, 17.37it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8695/8754 [08:20<00:03, 17.37it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8696/8754 [08:20<00:03, 17.37it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8697/8754 [08:20<00:03, 17.38it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8698/8754 [08:20<00:03, 17.38it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8699/8754 [08:20<00:03, 17.38it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8700/8754 [08:20<00:03, 17.38it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8701/8754 [08:20<00:03, 17.38it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8702/8754 [08:20<00:02, 17.38it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8703/8754 [08:20<00:02, 17.38it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8704/8754 [08:20<00:02, 17.39it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8705/8754 [08:20<00:02, 17.39it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8706/8754 [08:20<00:02, 17.39it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8707/8754 [08:20<00:02, 17.39it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8708/8754 [08:20<00:02, 17.39it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8709/8754 [08:20<00:02, 17.39it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 8710/8754 [08:20<00:02, 17.39it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8711/8754 [08:20<00:02, 17.39it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8712/8754 [08:20<00:02, 17.40it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8713/8754 [08:20<00:02, 17.40it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8714/8754 [08:20<00:02, 17.40it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8715/8754 [08:20<00:02, 17.40it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8716/8754 [08:20<00:02, 17.40it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8717/8754 [08:20<00:02, 17.40it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8718/8754 [08:20<00:02, 17.40it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8719/8754 [08:20<00:02, 17.41it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8720/8754 [08:20<00:01, 17.41it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8721/8754 [08:20<00:01, 17.41it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8722/8754 [08:20<00:01, 17.41it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8723/8754 [08:21<00:01, 17.41it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8724/8754 [08:21<00:01, 17.41it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8725/8754 [08:21<00:01, 17.41it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8726/8754 [08:21<00:01, 17.42it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8727/8754 [08:21<00:01, 17.42it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8728/8754 [08:21<00:01, 17.42it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8729/8754 [08:21<00:01, 17.42it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8730/8754 [08:21<00:01, 17.42it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8731/8754 [08:21<00:01, 17.42it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8732/8754 [08:21<00:01, 17.42it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8733/8754 [08:21<00:01, 17.42it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8734/8754 [08:21<00:01, 17.43it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8735/8754 [08:21<00:01, 17.43it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8736/8754 [08:21<00:01, 17.43it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8737/8754 [08:21<00:00, 17.43it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8738/8754 [08:21<00:00, 17.43it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8739/8754 [08:21<00:00, 17.43it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8740/8754 [08:21<00:00, 17.43it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8741/8754 [08:21<00:00, 17.44it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8742/8754 [08:21<00:00, 17.44it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8743/8754 [08:21<00:00, 17.44it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8744/8754 [08:21<00:00, 17.44it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8745/8754 [08:21<00:00, 17.44it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8746/8754 [08:21<00:00, 17.44it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8747/8754 [08:21<00:00, 17.44it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8748/8754 [08:21<00:00, 17.44it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8749/8754 [08:21<00:00, 17.45it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8750/8754 [08:21<00:00, 17.45it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8751/8754 [08:21<00:00, 17.45it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8752/8754 [08:21<00:00, 17.45it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 8753/8754 [08:21<00:00, 17.45it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 1: 100%|█| 8754/8754 [08:21<00:00, 17.45it/s, loss=0.551, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  98%|▉| 8597/8754 [08:17<00:09, 17.27it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  98%|▉| 8598/8754 [08:18<00:09, 17.24it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  98%|▉| 8599/8754 [08:18<00:08, 17.24it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  98%|▉| 8600/8754 [08:18<00:08, 17.24it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  98%|▉| 8601/8754 [08:18<00:08, 17.24it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  98%|▉| 8602/8754 [08:18<00:08, 17.24it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  98%|▉| 8603/8754 [08:18<00:08, 17.25it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  98%|▉| 8604/8754 [08:18<00:08, 17.25it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  98%|▉| 8605/8754 [08:18<00:08, 17.25it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  98%|▉| 8606/8754 [08:18<00:08, 17.25it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  98%|▉| 8607/8754 [08:18<00:08, 17.25it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  98%|▉| 8608/8754 [08:18<00:08, 17.25it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  98%|▉| 8609/8754 [08:18<00:08, 17.25it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  98%|▉| 8610/8754 [08:19<00:08, 17.25it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  98%|▉| 8611/8754 [08:19<00:08, 17.26it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  98%|▉| 8612/8754 [08:19<00:08, 17.26it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  98%|▉| 8613/8754 [08:19<00:08, 17.26it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  98%|▉| 8614/8754 [08:19<00:08, 17.26it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  98%|▉| 8615/8754 [08:19<00:08, 17.26it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  98%|▉| 8616/8754 [08:19<00:07, 17.26it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  98%|▉| 8617/8754 [08:19<00:07, 17.26it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  98%|▉| 8618/8754 [08:19<00:07, 17.27it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  98%|▉| 8619/8754 [08:19<00:07, 17.27it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  98%|▉| 8620/8754 [08:19<00:07, 17.27it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  98%|▉| 8621/8754 [08:19<00:07, 17.27it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  98%|▉| 8622/8754 [08:19<00:07, 17.27it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8623/8754 [08:19<00:07, 17.27it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8624/8754 [08:19<00:07, 17.27it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8625/8754 [08:19<00:07, 17.28it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8626/8754 [08:19<00:07, 17.28it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8627/8754 [08:19<00:07, 17.28it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8628/8754 [08:19<00:07, 17.28it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8629/8754 [08:19<00:07, 17.28it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8630/8754 [08:19<00:07, 17.28it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8631/8754 [08:19<00:07, 17.28it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8632/8754 [08:19<00:07, 17.29it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8633/8754 [08:19<00:06, 17.29it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8634/8754 [08:19<00:06, 17.29it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8635/8754 [08:19<00:06, 17.29it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8636/8754 [08:19<00:06, 17.29it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8637/8754 [08:19<00:06, 17.29it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8638/8754 [08:19<00:06, 17.29it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8639/8754 [08:19<00:06, 17.29it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8640/8754 [08:19<00:06, 17.30it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8641/8754 [08:19<00:06, 17.30it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8642/8754 [08:19<00:06, 17.30it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8643/8754 [08:19<00:06, 17.30it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8644/8754 [08:19<00:06, 17.30it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8645/8754 [08:19<00:06, 17.30it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8646/8754 [08:19<00:06, 17.30it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8647/8754 [08:19<00:06, 17.31it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8648/8754 [08:19<00:06, 17.31it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8649/8754 [08:19<00:06, 17.31it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8650/8754 [08:19<00:06, 17.31it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8651/8754 [08:19<00:05, 17.31it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8652/8754 [08:19<00:05, 17.31it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8653/8754 [08:19<00:05, 17.31it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8654/8754 [08:19<00:05, 17.32it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8655/8754 [08:19<00:05, 17.32it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8656/8754 [08:19<00:05, 17.32it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8657/8754 [08:19<00:05, 17.32it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8658/8754 [08:19<00:05, 17.32it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8659/8754 [08:19<00:05, 17.32it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8660/8754 [08:19<00:05, 17.32it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8661/8754 [08:19<00:05, 17.33it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8662/8754 [08:19<00:05, 17.33it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8663/8754 [08:19<00:05, 17.33it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8664/8754 [08:19<00:05, 17.33it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8665/8754 [08:19<00:05, 17.33it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8666/8754 [08:19<00:05, 17.33it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8667/8754 [08:20<00:05, 17.33it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8668/8754 [08:20<00:04, 17.33it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8669/8754 [08:20<00:04, 17.34it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8670/8754 [08:20<00:04, 17.34it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8671/8754 [08:20<00:04, 17.34it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8672/8754 [08:20<00:04, 17.34it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8673/8754 [08:20<00:04, 17.34it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8674/8754 [08:20<00:04, 17.34it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8675/8754 [08:20<00:04, 17.34it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8676/8754 [08:20<00:04, 17.35it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8677/8754 [08:20<00:04, 17.35it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8678/8754 [08:20<00:04, 17.35it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8679/8754 [08:20<00:04, 17.35it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8680/8754 [08:20<00:04, 17.35it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8681/8754 [08:20<00:04, 17.35it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8682/8754 [08:20<00:04, 17.35it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8683/8754 [08:20<00:04, 17.35it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8684/8754 [08:20<00:04, 17.36it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8685/8754 [08:20<00:03, 17.36it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8686/8754 [08:20<00:03, 17.36it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8687/8754 [08:20<00:03, 17.36it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8688/8754 [08:20<00:03, 17.36it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8689/8754 [08:20<00:03, 17.36it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8690/8754 [08:20<00:03, 17.36it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8691/8754 [08:20<00:03, 17.37it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8692/8754 [08:20<00:03, 17.37it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8693/8754 [08:20<00:03, 17.37it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8694/8754 [08:20<00:03, 17.37it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8695/8754 [08:20<00:03, 17.37it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8696/8754 [08:20<00:03, 17.37it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8697/8754 [08:20<00:03, 17.37it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8698/8754 [08:20<00:03, 17.37it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8699/8754 [08:20<00:03, 17.38it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8700/8754 [08:20<00:03, 17.38it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8701/8754 [08:20<00:03, 17.38it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8702/8754 [08:20<00:02, 17.38it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8703/8754 [08:20<00:02, 17.38it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8704/8754 [08:20<00:02, 17.38it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8705/8754 [08:20<00:02, 17.38it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8706/8754 [08:20<00:02, 17.39it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8707/8754 [08:20<00:02, 17.39it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8708/8754 [08:20<00:02, 17.39it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8709/8754 [08:20<00:02, 17.39it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 8710/8754 [08:20<00:02, 17.39it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8711/8754 [08:20<00:02, 17.39it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8712/8754 [08:20<00:02, 17.39it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8713/8754 [08:20<00:02, 17.39it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8714/8754 [08:20<00:02, 17.40it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8715/8754 [08:20<00:02, 17.40it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8716/8754 [08:20<00:02, 17.40it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8717/8754 [08:20<00:02, 17.40it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8718/8754 [08:20<00:02, 17.40it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8719/8754 [08:21<00:02, 17.40it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8720/8754 [08:21<00:01, 17.40it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8721/8754 [08:21<00:01, 17.41it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8722/8754 [08:21<00:01, 17.41it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8723/8754 [08:21<00:01, 17.41it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8724/8754 [08:21<00:01, 17.41it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8725/8754 [08:21<00:01, 17.41it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8726/8754 [08:21<00:01, 17.41it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8727/8754 [08:21<00:01, 17.41it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8728/8754 [08:21<00:01, 17.42it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8729/8754 [08:21<00:01, 17.42it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8730/8754 [08:21<00:01, 17.42it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8731/8754 [08:21<00:01, 17.42it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8732/8754 [08:21<00:01, 17.42it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8733/8754 [08:21<00:01, 17.42it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8734/8754 [08:21<00:01, 17.42it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8735/8754 [08:21<00:01, 17.42it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8736/8754 [08:21<00:01, 17.43it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8737/8754 [08:21<00:00, 17.43it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8738/8754 [08:21<00:00, 17.43it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8739/8754 [08:21<00:00, 17.43it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8740/8754 [08:21<00:00, 17.43it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8741/8754 [08:21<00:00, 17.43it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8742/8754 [08:21<00:00, 17.43it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8743/8754 [08:21<00:00, 17.44it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8744/8754 [08:21<00:00, 17.44it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8745/8754 [08:21<00:00, 17.44it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8746/8754 [08:21<00:00, 17.44it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8747/8754 [08:21<00:00, 17.44it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8748/8754 [08:21<00:00, 17.44it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8749/8754 [08:21<00:00, 17.44it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8750/8754 [08:21<00:00, 17.45it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8751/8754 [08:21<00:00, 17.45it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8752/8754 [08:21<00:00, 17.45it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 8753/8754 [08:21<00:00, 17.45it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 2: 100%|█| 8754/8754 [08:21<00:00, 17.45it/s, loss=0.503, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  98%|▉| 8597/8754 [08:17<00:09, 17.28it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  98%|▉| 8598/8754 [08:18<00:09, 17.24it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  98%|▉| 8599/8754 [08:18<00:08, 17.24it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  98%|▉| 8600/8754 [08:18<00:08, 17.25it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  98%|▉| 8601/8754 [08:18<00:08, 17.25it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  98%|▉| 8602/8754 [08:18<00:08, 17.25it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  98%|▉| 8603/8754 [08:18<00:08, 17.25it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  98%|▉| 8604/8754 [08:18<00:08, 17.25it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  98%|▉| 8605/8754 [08:18<00:08, 17.25it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  98%|▉| 8606/8754 [08:18<00:08, 17.25it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  98%|▉| 8607/8754 [08:18<00:08, 17.26it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  98%|▉| 8608/8754 [08:18<00:08, 17.26it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  98%|▉| 8609/8754 [08:18<00:08, 17.26it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  98%|▉| 8610/8754 [08:18<00:08, 17.26it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  98%|▉| 8611/8754 [08:18<00:08, 17.26it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  98%|▉| 8612/8754 [08:18<00:08, 17.26it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  98%|▉| 8613/8754 [08:18<00:08, 17.26it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  98%|▉| 8614/8754 [08:18<00:08, 17.26it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  98%|▉| 8615/8754 [08:18<00:08, 17.27it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  98%|▉| 8616/8754 [08:18<00:07, 17.27it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  98%|▉| 8617/8754 [08:18<00:07, 17.27it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  98%|▉| 8618/8754 [08:19<00:07, 17.27it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  98%|▉| 8619/8754 [08:19<00:07, 17.27it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  98%|▉| 8620/8754 [08:19<00:07, 17.27it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  98%|▉| 8621/8754 [08:19<00:07, 17.27it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  98%|▉| 8622/8754 [08:19<00:07, 17.28it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8623/8754 [08:19<00:07, 17.28it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8624/8754 [08:19<00:07, 17.28it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8625/8754 [08:19<00:07, 17.28it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8626/8754 [08:19<00:07, 17.28it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8627/8754 [08:19<00:07, 17.28it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8628/8754 [08:19<00:07, 17.28it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8629/8754 [08:19<00:07, 17.29it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8630/8754 [08:19<00:07, 17.29it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8631/8754 [08:19<00:07, 17.29it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8632/8754 [08:19<00:07, 17.29it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8633/8754 [08:19<00:06, 17.29it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8634/8754 [08:19<00:06, 17.29it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8635/8754 [08:19<00:06, 17.29it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8636/8754 [08:19<00:06, 17.30it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8637/8754 [08:19<00:06, 17.30it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8638/8754 [08:19<00:06, 17.30it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8639/8754 [08:19<00:06, 17.30it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8640/8754 [08:19<00:06, 17.30it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8641/8754 [08:19<00:06, 17.30it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8642/8754 [08:19<00:06, 17.30it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8643/8754 [08:19<00:06, 17.31it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8644/8754 [08:19<00:06, 17.31it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8645/8754 [08:19<00:06, 17.31it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8646/8754 [08:19<00:06, 17.31it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8647/8754 [08:19<00:06, 17.31it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8648/8754 [08:19<00:06, 17.31it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8649/8754 [08:19<00:06, 17.31it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8650/8754 [08:19<00:06, 17.31it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8651/8754 [08:19<00:05, 17.32it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8652/8754 [08:19<00:05, 17.32it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8653/8754 [08:19<00:05, 17.32it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8654/8754 [08:19<00:05, 17.32it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8655/8754 [08:19<00:05, 17.32it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8656/8754 [08:19<00:05, 17.32it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8657/8754 [08:19<00:05, 17.32it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8658/8754 [08:19<00:05, 17.33it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8659/8754 [08:19<00:05, 17.33it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8660/8754 [08:19<00:05, 17.33it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8661/8754 [08:19<00:05, 17.33it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8662/8754 [08:19<00:05, 17.33it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8663/8754 [08:19<00:05, 17.33it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8664/8754 [08:19<00:05, 17.33it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8665/8754 [08:19<00:05, 17.34it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8666/8754 [08:19<00:05, 17.34it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8667/8754 [08:19<00:05, 17.34it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8668/8754 [08:19<00:04, 17.34it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8669/8754 [08:19<00:04, 17.34it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8670/8754 [08:19<00:04, 17.34it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8671/8754 [08:19<00:04, 17.34it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8672/8754 [08:19<00:04, 17.35it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8673/8754 [08:19<00:04, 17.35it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8674/8754 [08:20<00:04, 17.35it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8675/8754 [08:20<00:04, 17.35it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8676/8754 [08:20<00:04, 17.35it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8677/8754 [08:20<00:04, 17.35it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8678/8754 [08:20<00:04, 17.35it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8679/8754 [08:20<00:04, 17.35it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8680/8754 [08:20<00:04, 17.36it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8681/8754 [08:20<00:04, 17.36it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8682/8754 [08:20<00:04, 17.36it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8683/8754 [08:20<00:04, 17.36it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8684/8754 [08:20<00:04, 17.36it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8685/8754 [08:20<00:03, 17.36it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8686/8754 [08:20<00:03, 17.36it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8687/8754 [08:20<00:03, 17.36it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8688/8754 [08:20<00:03, 17.37it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8689/8754 [08:20<00:03, 17.37it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8690/8754 [08:20<00:03, 17.37it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8691/8754 [08:20<00:03, 17.37it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8692/8754 [08:20<00:03, 17.37it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8693/8754 [08:20<00:03, 17.37it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8694/8754 [08:20<00:03, 17.37it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8695/8754 [08:20<00:03, 17.38it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8696/8754 [08:20<00:03, 17.38it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8697/8754 [08:20<00:03, 17.38it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8698/8754 [08:20<00:03, 17.38it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8699/8754 [08:20<00:03, 17.38it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8700/8754 [08:20<00:03, 17.38it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8701/8754 [08:20<00:03, 17.38it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8702/8754 [08:20<00:02, 17.38it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8703/8754 [08:20<00:02, 17.39it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8704/8754 [08:20<00:02, 17.39it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8705/8754 [08:20<00:02, 17.39it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8706/8754 [08:20<00:02, 17.39it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8707/8754 [08:20<00:02, 17.39it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8708/8754 [08:20<00:02, 17.39it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8709/8754 [08:20<00:02, 17.39it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 8710/8754 [08:20<00:02, 17.40it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8711/8754 [08:20<00:02, 17.40it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8712/8754 [08:20<00:02, 17.40it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8713/8754 [08:20<00:02, 17.40it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8714/8754 [08:20<00:02, 17.40it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8715/8754 [08:20<00:02, 17.40it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8716/8754 [08:20<00:02, 17.40it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8717/8754 [08:20<00:02, 17.41it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8718/8754 [08:20<00:02, 17.41it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8719/8754 [08:20<00:02, 17.41it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8720/8754 [08:20<00:01, 17.41it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8721/8754 [08:20<00:01, 17.41it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8722/8754 [08:20<00:01, 17.41it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8723/8754 [08:20<00:01, 17.41it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8724/8754 [08:20<00:01, 17.41it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8725/8754 [08:20<00:01, 17.42it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8726/8754 [08:20<00:01, 17.42it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8727/8754 [08:21<00:01, 17.42it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8728/8754 [08:21<00:01, 17.42it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8729/8754 [08:21<00:01, 17.42it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8730/8754 [08:21<00:01, 17.42it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8731/8754 [08:21<00:01, 17.42it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8732/8754 [08:21<00:01, 17.43it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8733/8754 [08:21<00:01, 17.43it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8734/8754 [08:21<00:01, 17.43it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8735/8754 [08:21<00:01, 17.43it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8736/8754 [08:21<00:01, 17.43it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8737/8754 [08:21<00:00, 17.43it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8738/8754 [08:21<00:00, 17.43it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8739/8754 [08:21<00:00, 17.44it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8740/8754 [08:21<00:00, 17.44it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8741/8754 [08:21<00:00, 17.44it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8742/8754 [08:21<00:00, 17.44it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8743/8754 [08:21<00:00, 17.44it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8744/8754 [08:21<00:00, 17.44it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8745/8754 [08:21<00:00, 17.44it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8746/8754 [08:21<00:00, 17.44it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8747/8754 [08:21<00:00, 17.45it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8748/8754 [08:21<00:00, 17.45it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8749/8754 [08:21<00:00, 17.45it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8750/8754 [08:21<00:00, 17.45it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8751/8754 [08:21<00:00, 17.45it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8752/8754 [08:21<00:00, 17.45it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 8753/8754 [08:21<00:00, 17.45it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 3: 100%|█| 8754/8754 [08:21<00:00, 17.46it/s, loss=0.466, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  98%|▉| 8597/8754 [08:17<00:09, 17.28it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  98%|▉| 8598/8754 [08:18<00:09, 17.24it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  98%|▉| 8599/8754 [08:18<00:08, 17.24it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  98%|▉| 8600/8754 [08:18<00:08, 17.24it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  98%|▉| 8601/8754 [08:18<00:08, 17.25it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  98%|▉| 8602/8754 [08:18<00:08, 17.25it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  98%|▉| 8603/8754 [08:18<00:08, 17.25it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  98%|▉| 8604/8754 [08:18<00:08, 17.25it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  98%|▉| 8605/8754 [08:18<00:08, 17.25it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  98%|▉| 8606/8754 [08:18<00:08, 17.25it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  98%|▉| 8607/8754 [08:18<00:08, 17.25it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  98%|▉| 8608/8754 [08:18<00:08, 17.26it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  98%|▉| 8609/8754 [08:18<00:08, 17.26it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  98%|▉| 8610/8754 [08:18<00:08, 17.26it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  98%|▉| 8611/8754 [08:18<00:08, 17.26it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  98%|▉| 8612/8754 [08:18<00:08, 17.26it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  98%|▉| 8613/8754 [08:18<00:08, 17.26it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  98%|▉| 8614/8754 [08:18<00:08, 17.26it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  98%|▉| 8615/8754 [08:18<00:08, 17.27it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  98%|▉| 8616/8754 [08:19<00:07, 17.27it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  98%|▉| 8617/8754 [08:19<00:07, 17.27it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  98%|▉| 8618/8754 [08:19<00:07, 17.27it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  98%|▉| 8619/8754 [08:19<00:07, 17.27it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  98%|▉| 8620/8754 [08:19<00:07, 17.27it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  98%|▉| 8621/8754 [08:19<00:07, 17.27it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  98%|▉| 8622/8754 [08:19<00:07, 17.27it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8623/8754 [08:19<00:07, 17.28it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8624/8754 [08:19<00:07, 17.28it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8625/8754 [08:19<00:07, 17.28it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8626/8754 [08:19<00:07, 17.28it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8627/8754 [08:19<00:07, 17.28it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8628/8754 [08:19<00:07, 17.28it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8629/8754 [08:19<00:07, 17.28it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8630/8754 [08:19<00:07, 17.29it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8631/8754 [08:19<00:07, 17.29it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8632/8754 [08:19<00:07, 17.29it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8633/8754 [08:19<00:06, 17.29it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8634/8754 [08:19<00:06, 17.29it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8635/8754 [08:19<00:06, 17.29it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8636/8754 [08:19<00:06, 17.29it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8637/8754 [08:19<00:06, 17.30it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8638/8754 [08:19<00:06, 17.30it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8639/8754 [08:19<00:06, 17.30it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8640/8754 [08:19<00:06, 17.30it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8641/8754 [08:19<00:06, 17.30it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8642/8754 [08:19<00:06, 17.30it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8643/8754 [08:19<00:06, 17.30it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8644/8754 [08:19<00:06, 17.31it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8645/8754 [08:19<00:06, 17.31it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8646/8754 [08:19<00:06, 17.31it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8647/8754 [08:19<00:06, 17.31it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8648/8754 [08:19<00:06, 17.31it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8649/8754 [08:19<00:06, 17.31it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8650/8754 [08:19<00:06, 17.31it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8651/8754 [08:19<00:05, 17.31it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8652/8754 [08:19<00:05, 17.32it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8653/8754 [08:19<00:05, 17.32it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8654/8754 [08:19<00:05, 17.32it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8655/8754 [08:19<00:05, 17.32it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8656/8754 [08:19<00:05, 17.32it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8657/8754 [08:19<00:05, 17.32it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8658/8754 [08:19<00:05, 17.32it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8659/8754 [08:19<00:05, 17.33it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8660/8754 [08:19<00:05, 17.33it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8661/8754 [08:19<00:05, 17.33it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8662/8754 [08:19<00:05, 17.33it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8663/8754 [08:19<00:05, 17.33it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8664/8754 [08:19<00:05, 17.33it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8665/8754 [08:19<00:05, 17.33it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8666/8754 [08:19<00:05, 17.34it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8667/8754 [08:19<00:05, 17.34it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8668/8754 [08:19<00:04, 17.34it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8669/8754 [08:19<00:04, 17.34it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8670/8754 [08:19<00:04, 17.34it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8671/8754 [08:19<00:04, 17.34it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8672/8754 [08:20<00:04, 17.34it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8673/8754 [08:20<00:04, 17.35it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8674/8754 [08:20<00:04, 17.35it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8675/8754 [08:20<00:04, 17.35it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8676/8754 [08:20<00:04, 17.35it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8677/8754 [08:20<00:04, 17.35it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8678/8754 [08:20<00:04, 17.35it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8679/8754 [08:20<00:04, 17.35it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8680/8754 [08:20<00:04, 17.35it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8681/8754 [08:20<00:04, 17.36it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8682/8754 [08:20<00:04, 17.36it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8683/8754 [08:20<00:04, 17.36it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8684/8754 [08:20<00:04, 17.36it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8685/8754 [08:20<00:03, 17.36it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8686/8754 [08:20<00:03, 17.36it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8687/8754 [08:20<00:03, 17.36it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8688/8754 [08:20<00:03, 17.36it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8689/8754 [08:20<00:03, 17.37it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8690/8754 [08:20<00:03, 17.37it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8691/8754 [08:20<00:03, 17.37it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8692/8754 [08:20<00:03, 17.37it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8693/8754 [08:20<00:03, 17.37it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8694/8754 [08:20<00:03, 17.37it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8695/8754 [08:20<00:03, 17.37it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8696/8754 [08:20<00:03, 17.38it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8697/8754 [08:20<00:03, 17.38it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8698/8754 [08:20<00:03, 17.38it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8699/8754 [08:20<00:03, 17.38it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8700/8754 [08:20<00:03, 17.38it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8701/8754 [08:20<00:03, 17.38it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8702/8754 [08:20<00:02, 17.38it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8703/8754 [08:20<00:02, 17.39it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8704/8754 [08:20<00:02, 17.39it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8705/8754 [08:20<00:02, 17.39it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8706/8754 [08:20<00:02, 17.39it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8707/8754 [08:20<00:02, 17.39it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8708/8754 [08:20<00:02, 17.39it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8709/8754 [08:20<00:02, 17.39it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4:  99%|▉| 8710/8754 [08:20<00:02, 17.39it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8711/8754 [08:20<00:02, 17.40it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8712/8754 [08:20<00:02, 17.40it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8713/8754 [08:20<00:02, 17.40it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8714/8754 [08:20<00:02, 17.40it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8715/8754 [08:20<00:02, 17.40it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8716/8754 [08:20<00:02, 17.40it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8717/8754 [08:20<00:02, 17.40it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8718/8754 [08:20<00:02, 17.41it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8719/8754 [08:20<00:02, 17.41it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8720/8754 [08:20<00:01, 17.41it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8721/8754 [08:20<00:01, 17.41it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8722/8754 [08:20<00:01, 17.41it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8723/8754 [08:20<00:01, 17.41it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8724/8754 [08:20<00:01, 17.41it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8725/8754 [08:21<00:01, 17.41it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8726/8754 [08:21<00:01, 17.42it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8727/8754 [08:21<00:01, 17.42it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8728/8754 [08:21<00:01, 17.42it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8729/8754 [08:21<00:01, 17.42it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8730/8754 [08:21<00:01, 17.42it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8731/8754 [08:21<00:01, 17.42it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8732/8754 [08:21<00:01, 17.42it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8733/8754 [08:21<00:01, 17.43it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8734/8754 [08:21<00:01, 17.43it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8735/8754 [08:21<00:01, 17.43it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8736/8754 [08:21<00:01, 17.43it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8737/8754 [08:21<00:00, 17.43it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8738/8754 [08:21<00:00, 17.43it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8739/8754 [08:21<00:00, 17.43it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8740/8754 [08:21<00:00, 17.44it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8741/8754 [08:21<00:00, 17.44it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8742/8754 [08:21<00:00, 17.44it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8743/8754 [08:21<00:00, 17.44it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8744/8754 [08:21<00:00, 17.44it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8745/8754 [08:21<00:00, 17.44it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8746/8754 [08:21<00:00, 17.44it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8747/8754 [08:21<00:00, 17.44it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8748/8754 [08:21<00:00, 17.45it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8749/8754 [08:21<00:00, 17.45it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8750/8754 [08:21<00:00, 17.45it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8751/8754 [08:21<00:00, 17.45it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8752/8754 [08:21<00:00, 17.45it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|▉| 8753/8754 [08:21<00:00, 17.45it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 4: 100%|█| 8754/8754 [08:21<00:00, 17.45it/s, loss=0.426, v_num=2, val_los\u001b[A\n",
      "Epoch 5:  98%|▉| 8597/8754 [08:17<00:09, 17.26it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  98%|▉| 8598/8754 [08:18<00:09, 17.23it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  98%|▉| 8599/8754 [08:19<00:08, 17.23it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  98%|▉| 8600/8754 [08:19<00:08, 17.23it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  98%|▉| 8601/8754 [08:19<00:08, 17.23it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  98%|▉| 8602/8754 [08:19<00:08, 17.24it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  98%|▉| 8603/8754 [08:19<00:08, 17.24it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  98%|▉| 8604/8754 [08:19<00:08, 17.24it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  98%|▉| 8605/8754 [08:19<00:08, 17.24it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  98%|▉| 8606/8754 [08:19<00:08, 17.24it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  98%|▉| 8607/8754 [08:19<00:08, 17.24it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  98%|▉| 8608/8754 [08:19<00:08, 17.24it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  98%|▉| 8609/8754 [08:19<00:08, 17.25it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  98%|▉| 8610/8754 [08:19<00:08, 17.25it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  98%|▉| 8611/8754 [08:19<00:08, 17.25it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  98%|▉| 8612/8754 [08:19<00:08, 17.25it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  98%|▉| 8613/8754 [08:19<00:08, 17.25it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  98%|▉| 8614/8754 [08:19<00:08, 17.25it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  98%|▉| 8615/8754 [08:19<00:08, 17.25it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  98%|▉| 8616/8754 [08:19<00:07, 17.26it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  98%|▉| 8617/8754 [08:19<00:07, 17.26it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  98%|▉| 8618/8754 [08:19<00:07, 17.26it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  98%|▉| 8619/8754 [08:19<00:07, 17.26it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  98%|▉| 8620/8754 [08:19<00:07, 17.26it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  98%|▉| 8621/8754 [08:19<00:07, 17.26it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  98%|▉| 8622/8754 [08:19<00:07, 17.26it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8623/8754 [08:19<00:07, 17.26it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8624/8754 [08:19<00:07, 17.27it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8625/8754 [08:19<00:07, 17.27it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8626/8754 [08:19<00:07, 17.27it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8627/8754 [08:19<00:07, 17.27it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8628/8754 [08:19<00:07, 17.27it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8629/8754 [08:19<00:07, 17.27it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8630/8754 [08:19<00:07, 17.27it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8631/8754 [08:19<00:07, 17.28it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8632/8754 [08:19<00:07, 17.28it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8633/8754 [08:19<00:07, 17.28it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8634/8754 [08:19<00:06, 17.28it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8635/8754 [08:19<00:06, 17.28it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8636/8754 [08:19<00:06, 17.28it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8637/8754 [08:19<00:06, 17.28it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8638/8754 [08:19<00:06, 17.29it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8639/8754 [08:19<00:06, 17.29it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8640/8754 [08:19<00:06, 17.29it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8641/8754 [08:19<00:06, 17.29it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8642/8754 [08:19<00:06, 17.29it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8643/8754 [08:19<00:06, 17.29it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8644/8754 [08:19<00:06, 17.29it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8645/8754 [08:19<00:06, 17.30it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8646/8754 [08:19<00:06, 17.30it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8647/8754 [08:19<00:06, 17.30it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8648/8754 [08:19<00:06, 17.30it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8649/8754 [08:19<00:06, 17.30it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8650/8754 [08:19<00:06, 17.30it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8651/8754 [08:19<00:05, 17.30it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8652/8754 [08:19<00:05, 17.31it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8653/8754 [08:19<00:05, 17.31it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8654/8754 [08:20<00:05, 17.31it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8655/8754 [08:20<00:05, 17.31it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8656/8754 [08:20<00:05, 17.31it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8657/8754 [08:20<00:05, 17.31it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8658/8754 [08:20<00:05, 17.31it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8659/8754 [08:20<00:05, 17.31it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8660/8754 [08:20<00:05, 17.32it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8661/8754 [08:20<00:05, 17.32it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8662/8754 [08:20<00:05, 17.32it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8663/8754 [08:20<00:05, 17.32it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8664/8754 [08:20<00:05, 17.32it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8665/8754 [08:20<00:05, 17.32it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8666/8754 [08:20<00:05, 17.32it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8667/8754 [08:20<00:05, 17.33it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8668/8754 [08:20<00:04, 17.33it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8669/8754 [08:20<00:04, 17.33it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8670/8754 [08:20<00:04, 17.33it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8671/8754 [08:20<00:04, 17.33it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8672/8754 [08:20<00:04, 17.33it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8673/8754 [08:20<00:04, 17.33it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8674/8754 [08:20<00:04, 17.34it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8675/8754 [08:20<00:04, 17.34it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8676/8754 [08:20<00:04, 17.34it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8677/8754 [08:20<00:04, 17.34it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8678/8754 [08:20<00:04, 17.34it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8679/8754 [08:20<00:04, 17.34it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8680/8754 [08:20<00:04, 17.34it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8681/8754 [08:20<00:04, 17.34it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8682/8754 [08:20<00:04, 17.35it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8683/8754 [08:20<00:04, 17.35it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8684/8754 [08:20<00:04, 17.35it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8685/8754 [08:20<00:03, 17.35it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8686/8754 [08:20<00:03, 17.35it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8687/8754 [08:20<00:03, 17.35it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8688/8754 [08:20<00:03, 17.35it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8689/8754 [08:20<00:03, 17.35it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8690/8754 [08:20<00:03, 17.36it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8691/8754 [08:20<00:03, 17.36it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8692/8754 [08:20<00:03, 17.36it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8693/8754 [08:20<00:03, 17.36it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8694/8754 [08:20<00:03, 17.36it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8695/8754 [08:20<00:03, 17.36it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8696/8754 [08:20<00:03, 17.36it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8697/8754 [08:20<00:03, 17.37it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8698/8754 [08:20<00:03, 17.37it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8699/8754 [08:20<00:03, 17.37it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8700/8754 [08:20<00:03, 17.37it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8701/8754 [08:20<00:03, 17.37it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8702/8754 [08:20<00:02, 17.37it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8703/8754 [08:20<00:02, 17.37it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8704/8754 [08:20<00:02, 17.38it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8705/8754 [08:20<00:02, 17.38it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8706/8754 [08:20<00:02, 17.38it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8707/8754 [08:21<00:02, 17.38it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8708/8754 [08:21<00:02, 17.38it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8709/8754 [08:21<00:02, 17.38it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5:  99%|▉| 8710/8754 [08:21<00:02, 17.38it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8711/8754 [08:21<00:02, 17.38it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8712/8754 [08:21<00:02, 17.39it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8713/8754 [08:21<00:02, 17.39it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8714/8754 [08:21<00:02, 17.39it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8715/8754 [08:21<00:02, 17.39it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8716/8754 [08:21<00:02, 17.39it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8717/8754 [08:21<00:02, 17.39it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8718/8754 [08:21<00:02, 17.39it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8719/8754 [08:21<00:02, 17.40it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8720/8754 [08:21<00:01, 17.40it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8721/8754 [08:21<00:01, 17.40it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8722/8754 [08:21<00:01, 17.40it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8723/8754 [08:21<00:01, 17.40it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8724/8754 [08:21<00:01, 17.40it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8725/8754 [08:21<00:01, 17.40it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8726/8754 [08:21<00:01, 17.40it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8727/8754 [08:21<00:01, 17.41it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8728/8754 [08:21<00:01, 17.41it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8729/8754 [08:21<00:01, 17.41it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8730/8754 [08:21<00:01, 17.41it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8731/8754 [08:21<00:01, 17.41it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8732/8754 [08:21<00:01, 17.41it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8733/8754 [08:21<00:01, 17.41it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8734/8754 [08:21<00:01, 17.42it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8735/8754 [08:21<00:01, 17.42it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8736/8754 [08:21<00:01, 17.42it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8737/8754 [08:21<00:00, 17.42it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8738/8754 [08:21<00:00, 17.42it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8739/8754 [08:21<00:00, 17.42it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8740/8754 [08:21<00:00, 17.42it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8741/8754 [08:21<00:00, 17.43it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8742/8754 [08:21<00:00, 17.43it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8743/8754 [08:21<00:00, 17.43it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8744/8754 [08:21<00:00, 17.43it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8745/8754 [08:21<00:00, 17.43it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8746/8754 [08:21<00:00, 17.43it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8747/8754 [08:21<00:00, 17.43it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8748/8754 [08:21<00:00, 17.43it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8749/8754 [08:21<00:00, 17.44it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8750/8754 [08:21<00:00, 17.44it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8751/8754 [08:21<00:00, 17.44it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8752/8754 [08:21<00:00, 17.44it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|▉| 8753/8754 [08:21<00:00, 17.44it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 5: 100%|█| 8754/8754 [08:21<00:00, 17.44it/s, loss=0.4, v_num=2, val_loss=\u001b[A\n",
      "Epoch 6:  98%|▉| 8597/8754 [08:18<00:09, 17.26it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  98%|▉| 8598/8754 [08:19<00:09, 17.23it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  98%|▉| 8599/8754 [08:19<00:08, 17.23it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  98%|▉| 8600/8754 [08:19<00:08, 17.23it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  98%|▉| 8601/8754 [08:19<00:08, 17.23it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  98%|▉| 8602/8754 [08:19<00:08, 17.23it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  98%|▉| 8603/8754 [08:19<00:08, 17.23it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  98%|▉| 8604/8754 [08:19<00:08, 17.23it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  98%|▉| 8605/8754 [08:19<00:08, 17.24it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  98%|▉| 8606/8754 [08:19<00:08, 17.24it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  98%|▉| 8607/8754 [08:19<00:08, 17.24it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  98%|▉| 8608/8754 [08:19<00:08, 17.24it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  98%|▉| 8609/8754 [08:19<00:08, 17.24it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  98%|▉| 8610/8754 [08:19<00:08, 17.24it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  98%|▉| 8611/8754 [08:19<00:08, 17.24it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  98%|▉| 8612/8754 [08:19<00:08, 17.25it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  98%|▉| 8613/8754 [08:19<00:08, 17.25it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  98%|▉| 8614/8754 [08:19<00:08, 17.25it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  98%|▉| 8615/8754 [08:19<00:08, 17.25it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  98%|▉| 8616/8754 [08:19<00:07, 17.25it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  98%|▉| 8617/8754 [08:19<00:07, 17.25it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  98%|▉| 8618/8754 [08:19<00:07, 17.25it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  98%|▉| 8619/8754 [08:19<00:07, 17.26it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  98%|▉| 8620/8754 [08:19<00:07, 17.26it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  98%|▉| 8621/8754 [08:19<00:07, 17.26it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  98%|▉| 8622/8754 [08:19<00:07, 17.26it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8623/8754 [08:19<00:07, 17.26it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8624/8754 [08:19<00:07, 17.26it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8625/8754 [08:19<00:07, 17.26it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8626/8754 [08:19<00:07, 17.26it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8627/8754 [08:19<00:07, 17.27it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8628/8754 [08:19<00:07, 17.27it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8629/8754 [08:19<00:07, 17.27it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8630/8754 [08:19<00:07, 17.27it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8631/8754 [08:19<00:07, 17.27it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8632/8754 [08:19<00:07, 17.27it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8633/8754 [08:19<00:07, 17.27it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8634/8754 [08:19<00:06, 17.28it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8635/8754 [08:19<00:06, 17.28it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8636/8754 [08:19<00:06, 17.28it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8637/8754 [08:19<00:06, 17.28it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8638/8754 [08:19<00:06, 17.28it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8639/8754 [08:19<00:06, 17.28it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8640/8754 [08:19<00:06, 17.28it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8641/8754 [08:19<00:06, 17.29it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8642/8754 [08:19<00:06, 17.29it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8643/8754 [08:19<00:06, 17.29it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8644/8754 [08:19<00:06, 17.29it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8645/8754 [08:19<00:06, 17.29it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8646/8754 [08:19<00:06, 17.29it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8647/8754 [08:19<00:06, 17.29it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8648/8754 [08:20<00:06, 17.30it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8649/8754 [08:20<00:06, 17.30it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8650/8754 [08:20<00:06, 17.30it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8651/8754 [08:20<00:05, 17.30it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8652/8754 [08:20<00:05, 17.30it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8653/8754 [08:20<00:05, 17.30it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8654/8754 [08:20<00:05, 17.30it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8655/8754 [08:20<00:05, 17.31it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8656/8754 [08:20<00:05, 17.31it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8657/8754 [08:20<00:05, 17.31it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8658/8754 [08:20<00:05, 17.31it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8659/8754 [08:20<00:05, 17.31it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8660/8754 [08:20<00:05, 17.31it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8661/8754 [08:20<00:05, 17.31it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8662/8754 [08:20<00:05, 17.31it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8663/8754 [08:20<00:05, 17.32it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8664/8754 [08:20<00:05, 17.32it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8665/8754 [08:20<00:05, 17.32it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8666/8754 [08:20<00:05, 17.32it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8667/8754 [08:20<00:05, 17.32it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8668/8754 [08:20<00:04, 17.32it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8669/8754 [08:20<00:04, 17.32it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8670/8754 [08:20<00:04, 17.33it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8671/8754 [08:20<00:04, 17.33it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8672/8754 [08:20<00:04, 17.33it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8673/8754 [08:20<00:04, 17.33it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8674/8754 [08:20<00:04, 17.33it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8675/8754 [08:20<00:04, 17.33it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8676/8754 [08:20<00:04, 17.33it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8677/8754 [08:20<00:04, 17.34it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8678/8754 [08:20<00:04, 17.34it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8679/8754 [08:20<00:04, 17.34it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8680/8754 [08:20<00:04, 17.34it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8681/8754 [08:20<00:04, 17.34it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8682/8754 [08:20<00:04, 17.34it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8683/8754 [08:20<00:04, 17.34it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8684/8754 [08:20<00:04, 17.34it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8685/8754 [08:20<00:03, 17.35it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8686/8754 [08:20<00:03, 17.35it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8687/8754 [08:20<00:03, 17.35it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8688/8754 [08:20<00:03, 17.35it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8689/8754 [08:20<00:03, 17.35it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8690/8754 [08:20<00:03, 17.35it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8691/8754 [08:20<00:03, 17.35it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8692/8754 [08:20<00:03, 17.36it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8693/8754 [08:20<00:03, 17.36it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8694/8754 [08:20<00:03, 17.36it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8695/8754 [08:20<00:03, 17.36it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8696/8754 [08:20<00:03, 17.36it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8697/8754 [08:20<00:03, 17.36it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8698/8754 [08:20<00:03, 17.36it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8699/8754 [08:20<00:03, 17.36it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8700/8754 [08:20<00:03, 17.37it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8701/8754 [08:21<00:03, 17.37it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8702/8754 [08:21<00:02, 17.37it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8703/8754 [08:21<00:02, 17.37it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8704/8754 [08:21<00:02, 17.37it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8705/8754 [08:21<00:02, 17.37it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8706/8754 [08:21<00:02, 17.37it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8707/8754 [08:21<00:02, 17.37it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8708/8754 [08:21<00:02, 17.38it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8709/8754 [08:21<00:02, 17.38it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6:  99%|▉| 8710/8754 [08:21<00:02, 17.38it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8711/8754 [08:21<00:02, 17.38it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8712/8754 [08:21<00:02, 17.38it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8713/8754 [08:21<00:02, 17.38it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8714/8754 [08:21<00:02, 17.38it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8715/8754 [08:21<00:02, 17.39it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8716/8754 [08:21<00:02, 17.39it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8717/8754 [08:21<00:02, 17.39it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8718/8754 [08:21<00:02, 17.39it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8719/8754 [08:21<00:02, 17.39it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8720/8754 [08:21<00:01, 17.39it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8721/8754 [08:21<00:01, 17.39it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8722/8754 [08:21<00:01, 17.40it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8723/8754 [08:21<00:01, 17.40it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8724/8754 [08:21<00:01, 17.40it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8725/8754 [08:21<00:01, 17.40it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8726/8754 [08:21<00:01, 17.40it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8727/8754 [08:21<00:01, 17.40it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8728/8754 [08:21<00:01, 17.40it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8729/8754 [08:21<00:01, 17.40it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8730/8754 [08:21<00:01, 17.41it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8731/8754 [08:21<00:01, 17.41it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8732/8754 [08:21<00:01, 17.41it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8733/8754 [08:21<00:01, 17.41it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8734/8754 [08:21<00:01, 17.41it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8735/8754 [08:21<00:01, 17.41it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8736/8754 [08:21<00:01, 17.41it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8737/8754 [08:21<00:00, 17.42it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8738/8754 [08:21<00:00, 17.42it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8739/8754 [08:21<00:00, 17.42it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8740/8754 [08:21<00:00, 17.42it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8741/8754 [08:21<00:00, 17.42it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8742/8754 [08:21<00:00, 17.42it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8743/8754 [08:21<00:00, 17.42it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8744/8754 [08:21<00:00, 17.43it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8745/8754 [08:21<00:00, 17.43it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8746/8754 [08:21<00:00, 17.43it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8747/8754 [08:21<00:00, 17.43it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8748/8754 [08:21<00:00, 17.43it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8749/8754 [08:21<00:00, 17.43it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8750/8754 [08:21<00:00, 17.43it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8751/8754 [08:21<00:00, 17.43it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8752/8754 [08:21<00:00, 17.44it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|▉| 8753/8754 [08:21<00:00, 17.44it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 6: 100%|█| 8754/8754 [08:21<00:00, 17.44it/s, loss=0.372, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  98%|▉| 8597/8754 [08:17<00:09, 17.27it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  98%|▉| 8598/8754 [08:18<00:09, 17.23it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  98%|▉| 8599/8754 [08:18<00:08, 17.24it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  98%|▉| 8600/8754 [08:18<00:08, 17.24it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  98%|▉| 8601/8754 [08:18<00:08, 17.24it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  98%|▉| 8602/8754 [08:18<00:08, 17.24it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  98%|▉| 8603/8754 [08:18<00:08, 17.24it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  98%|▉| 8604/8754 [08:19<00:08, 17.24it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  98%|▉| 8605/8754 [08:19<00:08, 17.24it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  98%|▉| 8606/8754 [08:19<00:08, 17.24it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  98%|▉| 8607/8754 [08:19<00:08, 17.25it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  98%|▉| 8608/8754 [08:19<00:08, 17.25it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  98%|▉| 8609/8754 [08:19<00:08, 17.25it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  98%|▉| 8610/8754 [08:19<00:08, 17.25it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  98%|▉| 8611/8754 [08:19<00:08, 17.25it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  98%|▉| 8612/8754 [08:19<00:08, 17.25it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  98%|▉| 8613/8754 [08:19<00:08, 17.25it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  98%|▉| 8614/8754 [08:19<00:08, 17.26it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  98%|▉| 8615/8754 [08:19<00:08, 17.26it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  98%|▉| 8616/8754 [08:19<00:07, 17.26it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  98%|▉| 8617/8754 [08:19<00:07, 17.26it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  98%|▉| 8618/8754 [08:19<00:07, 17.26it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  98%|▉| 8619/8754 [08:19<00:07, 17.26it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  98%|▉| 8620/8754 [08:19<00:07, 17.26it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  98%|▉| 8621/8754 [08:19<00:07, 17.27it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  98%|▉| 8622/8754 [08:19<00:07, 17.27it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8623/8754 [08:19<00:07, 17.27it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8624/8754 [08:19<00:07, 17.27it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8625/8754 [08:19<00:07, 17.27it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8626/8754 [08:19<00:07, 17.27it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8627/8754 [08:19<00:07, 17.27it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8628/8754 [08:19<00:07, 17.28it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8629/8754 [08:19<00:07, 17.28it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8630/8754 [08:19<00:07, 17.28it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8631/8754 [08:19<00:07, 17.28it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8632/8754 [08:19<00:07, 17.28it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8633/8754 [08:19<00:07, 17.28it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8634/8754 [08:19<00:06, 17.28it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8635/8754 [08:19<00:06, 17.28it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8636/8754 [08:19<00:06, 17.29it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8637/8754 [08:19<00:06, 17.29it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8638/8754 [08:19<00:06, 17.29it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8639/8754 [08:19<00:06, 17.29it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8640/8754 [08:19<00:06, 17.29it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8641/8754 [08:19<00:06, 17.29it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8642/8754 [08:19<00:06, 17.29it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8643/8754 [08:19<00:06, 17.30it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8644/8754 [08:19<00:06, 17.30it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8645/8754 [08:19<00:06, 17.30it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8646/8754 [08:19<00:06, 17.30it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8647/8754 [08:19<00:06, 17.30it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8648/8754 [08:19<00:06, 17.30it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8649/8754 [08:19<00:06, 17.30it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8650/8754 [08:19<00:06, 17.31it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8651/8754 [08:19<00:05, 17.31it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8652/8754 [08:19<00:05, 17.31it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8653/8754 [08:19<00:05, 17.31it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8654/8754 [08:19<00:05, 17.31it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8655/8754 [08:19<00:05, 17.31it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8656/8754 [08:19<00:05, 17.31it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8657/8754 [08:19<00:05, 17.32it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8658/8754 [08:19<00:05, 17.32it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8659/8754 [08:20<00:05, 17.32it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8660/8754 [08:20<00:05, 17.32it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8661/8754 [08:20<00:05, 17.32it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8662/8754 [08:20<00:05, 17.32it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8663/8754 [08:20<00:05, 17.32it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8664/8754 [08:20<00:05, 17.32it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8665/8754 [08:20<00:05, 17.33it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8666/8754 [08:20<00:05, 17.33it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8667/8754 [08:20<00:05, 17.33it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8668/8754 [08:20<00:04, 17.33it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8669/8754 [08:20<00:04, 17.33it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8670/8754 [08:20<00:04, 17.33it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8671/8754 [08:20<00:04, 17.33it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8672/8754 [08:20<00:04, 17.34it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8673/8754 [08:20<00:04, 17.34it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8674/8754 [08:20<00:04, 17.34it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8675/8754 [08:20<00:04, 17.34it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8676/8754 [08:20<00:04, 17.34it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8677/8754 [08:20<00:04, 17.34it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8678/8754 [08:20<00:04, 17.34it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8679/8754 [08:20<00:04, 17.34it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8680/8754 [08:20<00:04, 17.35it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8681/8754 [08:20<00:04, 17.35it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8682/8754 [08:20<00:04, 17.35it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8683/8754 [08:20<00:04, 17.35it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8684/8754 [08:20<00:04, 17.35it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8685/8754 [08:20<00:03, 17.35it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8686/8754 [08:20<00:03, 17.35it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8687/8754 [08:20<00:03, 17.36it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8688/8754 [08:20<00:03, 17.36it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8689/8754 [08:20<00:03, 17.36it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8690/8754 [08:20<00:03, 17.36it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8691/8754 [08:20<00:03, 17.36it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8692/8754 [08:20<00:03, 17.36it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8693/8754 [08:20<00:03, 17.36it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8694/8754 [08:20<00:03, 17.37it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8695/8754 [08:20<00:03, 17.37it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8696/8754 [08:20<00:03, 17.37it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8697/8754 [08:20<00:03, 17.37it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8698/8754 [08:20<00:03, 17.37it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8699/8754 [08:20<00:03, 17.37it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8700/8754 [08:20<00:03, 17.37it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8701/8754 [08:20<00:03, 17.37it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8702/8754 [08:20<00:02, 17.38it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8703/8754 [08:20<00:02, 17.38it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8704/8754 [08:20<00:02, 17.38it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8705/8754 [08:20<00:02, 17.38it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8706/8754 [08:20<00:02, 17.38it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8707/8754 [08:20<00:02, 17.38it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8708/8754 [08:20<00:02, 17.38it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8709/8754 [08:20<00:02, 17.38it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 8710/8754 [08:20<00:02, 17.39it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8711/8754 [08:20<00:02, 17.39it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8712/8754 [08:21<00:02, 17.39it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8713/8754 [08:21<00:02, 17.39it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8714/8754 [08:21<00:02, 17.39it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8715/8754 [08:21<00:02, 17.39it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8716/8754 [08:21<00:02, 17.39it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8717/8754 [08:21<00:02, 17.40it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8718/8754 [08:21<00:02, 17.40it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8719/8754 [08:21<00:02, 17.40it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8720/8754 [08:21<00:01, 17.40it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8721/8754 [08:21<00:01, 17.40it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8722/8754 [08:21<00:01, 17.40it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8723/8754 [08:21<00:01, 17.40it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8724/8754 [08:21<00:01, 17.41it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8725/8754 [08:21<00:01, 17.41it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8726/8754 [08:21<00:01, 17.41it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8727/8754 [08:21<00:01, 17.41it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8728/8754 [08:21<00:01, 17.41it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8729/8754 [08:21<00:01, 17.41it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8730/8754 [08:21<00:01, 17.41it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8731/8754 [08:21<00:01, 17.42it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8732/8754 [08:21<00:01, 17.42it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8733/8754 [08:21<00:01, 17.42it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8734/8754 [08:21<00:01, 17.42it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8735/8754 [08:21<00:01, 17.42it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8736/8754 [08:21<00:01, 17.42it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8737/8754 [08:21<00:00, 17.42it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8738/8754 [08:21<00:00, 17.42it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8739/8754 [08:21<00:00, 17.43it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8740/8754 [08:21<00:00, 17.43it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8741/8754 [08:21<00:00, 17.43it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8742/8754 [08:21<00:00, 17.43it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8743/8754 [08:21<00:00, 17.43it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8744/8754 [08:21<00:00, 17.43it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8745/8754 [08:21<00:00, 17.43it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8746/8754 [08:21<00:00, 17.44it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8747/8754 [08:21<00:00, 17.44it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8748/8754 [08:21<00:00, 17.44it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8749/8754 [08:21<00:00, 17.44it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8750/8754 [08:21<00:00, 17.44it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8751/8754 [08:21<00:00, 17.44it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8752/8754 [08:21<00:00, 17.44it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 8753/8754 [08:21<00:00, 17.44it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|█| 8754/8754 [08:21<00:00, 17.45it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "Epoch 7: 100%|█| 8754/8754 [08:21<00:00, 17.45it/s, loss=0.336, v_num=2, val_los\u001b[A\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing DataLoader 0: 100%|███████████████████| 157/157 [00:02<00:00, 52.73it/s]\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m  Runningstage.testing   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m                           \u001b[0m┃\n",
      "┃\u001b[1m \u001b[0m\u001b[1m         metric          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7982491850852966    \u001b[0m\u001b[35m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5566862225532532    \u001b[0m\u001b[35m \u001b[0m│\n",
      "└───────────────────────────┴───────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Train bi-lstm model\n",
    "! python train.py --encoder=bi-lstm --save_dir='./models'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 1234\n",
      "Using the latest cached version of the dataset since stanfordnlp/snli couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at /home/fanmin/.cache/huggingface/datasets/stanfordnlp___snli/plain_text/0.0.0/cdb5c3d5eed6ead6e5a341c8e56e669bb666725b (last modified on Wed Apr 17 17:24:15 2024).\n",
      "unique token len 37211\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Missing logger folder: ./models/bi-lstm-max-pool\n",
      "/home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory ./models exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type               | Params\n",
      "------------------------------------------------\n",
      "0 | encoder  | LSTMEncoder        | 49.7 M\n",
      "1 | mlp      | Sequential         | 8.7 M \n",
      "2 | accuracy | MulticlassAccuracy | 0     \n",
      "------------------------------------------------\n",
      "47.2 M    Trainable params\n",
      "11.2 M    Non-trainable params\n",
      "58.3 M    Total params\n",
      "233.277   Total estimated model params size (MB)\n",
      "Epoch 0:  98%|████████▊| 1075/1095 [07:17<00:08,  2.46it/s, loss=0.783, v_num=0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1076/1095 [07:18<00:07,  2.46it/s, loss=0.783, v_num=0]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1077/1095 [07:18<00:07,  2.46it/s, loss=0.783, v_num=0]\u001b[A\n",
      "Epoch 0:  98%|████████▊| 1078/1095 [07:18<00:06,  2.46it/s, loss=0.783, v_num=0]\u001b[A\n",
      "Epoch 0:  99%|████████▊| 1079/1095 [07:18<00:06,  2.46it/s, loss=0.783, v_num=0]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1080/1095 [07:18<00:06,  2.46it/s, loss=0.783, v_num=0]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1081/1095 [07:18<00:05,  2.47it/s, loss=0.783, v_num=0]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1082/1095 [07:18<00:05,  2.47it/s, loss=0.783, v_num=0]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1083/1095 [07:18<00:04,  2.47it/s, loss=0.783, v_num=0]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1084/1095 [07:18<00:04,  2.47it/s, loss=0.783, v_num=0]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1085/1095 [07:18<00:04,  2.47it/s, loss=0.783, v_num=0]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1086/1095 [07:18<00:03,  2.48it/s, loss=0.783, v_num=0]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1087/1095 [07:18<00:03,  2.48it/s, loss=0.783, v_num=0]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1088/1095 [07:18<00:02,  2.48it/s, loss=0.783, v_num=0]\u001b[A\n",
      "Epoch 0:  99%|████████▉| 1089/1095 [07:18<00:02,  2.48it/s, loss=0.783, v_num=0]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 1090/1095 [07:18<00:02,  2.48it/s, loss=0.783, v_num=0]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 1091/1095 [07:18<00:01,  2.49it/s, loss=0.783, v_num=0]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 1092/1095 [07:18<00:01,  2.49it/s, loss=0.783, v_num=0]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 1093/1095 [07:19<00:00,  2.49it/s, loss=0.783, v_num=0]\u001b[A\n",
      "Epoch 0: 100%|████████▉| 1094/1095 [07:19<00:00,  2.49it/s, loss=0.783, v_num=0]\u001b[A\n",
      "Epoch 0: 100%|█| 1095/1095 [07:19<00:00,  2.49it/s, loss=0.783, v_num=0, val_los\u001b[A\n",
      "Epoch 1:  98%|▉| 1075/1095 [05:10<00:05,  3.46it/s, loss=0.581, v_num=0, val_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  98%|▉| 1076/1095 [05:11<00:05,  3.46it/s, loss=0.581, v_num=0, val_los\u001b[A\n",
      "Epoch 1:  98%|▉| 1077/1095 [05:11<00:05,  3.46it/s, loss=0.581, v_num=0, val_los\u001b[A\n",
      "Epoch 1:  98%|▉| 1078/1095 [05:11<00:04,  3.46it/s, loss=0.581, v_num=0, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 1079/1095 [05:11<00:04,  3.46it/s, loss=0.581, v_num=0, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 1080/1095 [05:11<00:04,  3.47it/s, loss=0.581, v_num=0, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 1081/1095 [05:11<00:04,  3.47it/s, loss=0.581, v_num=0, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 1082/1095 [05:11<00:03,  3.47it/s, loss=0.581, v_num=0, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 1083/1095 [05:11<00:03,  3.47it/s, loss=0.581, v_num=0, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 1084/1095 [05:11<00:03,  3.48it/s, loss=0.581, v_num=0, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 1085/1095 [05:11<00:02,  3.48it/s, loss=0.581, v_num=0, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 1086/1095 [05:11<00:02,  3.48it/s, loss=0.581, v_num=0, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 1087/1095 [05:11<00:02,  3.49it/s, loss=0.581, v_num=0, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 1088/1095 [05:11<00:02,  3.49it/s, loss=0.581, v_num=0, val_los\u001b[A\n",
      "Epoch 1:  99%|▉| 1089/1095 [05:12<00:01,  3.49it/s, loss=0.581, v_num=0, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 1090/1095 [05:12<00:01,  3.49it/s, loss=0.581, v_num=0, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 1091/1095 [05:12<00:01,  3.50it/s, loss=0.581, v_num=0, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 1092/1095 [05:12<00:00,  3.50it/s, loss=0.581, v_num=0, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 1093/1095 [05:12<00:00,  3.50it/s, loss=0.581, v_num=0, val_los\u001b[A\n",
      "Epoch 1: 100%|▉| 1094/1095 [05:12<00:00,  3.50it/s, loss=0.581, v_num=0, val_los\u001b[A\n",
      "Epoch 1: 100%|█| 1095/1095 [05:12<00:00,  3.51it/s, loss=0.581, v_num=0, val_los\u001b[A\n",
      "Epoch 2:  98%|▉| 1075/1095 [05:10<00:05,  3.47it/s, loss=0.508, v_num=0, val_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  98%|▉| 1076/1095 [05:11<00:05,  3.46it/s, loss=0.508, v_num=0, val_los\u001b[A\n",
      "Epoch 2:  98%|▉| 1077/1095 [05:11<00:05,  3.46it/s, loss=0.508, v_num=0, val_los\u001b[A\n",
      "Epoch 2:  98%|▉| 1078/1095 [05:11<00:04,  3.46it/s, loss=0.508, v_num=0, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 1079/1095 [05:11<00:04,  3.47it/s, loss=0.508, v_num=0, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 1080/1095 [05:11<00:04,  3.47it/s, loss=0.508, v_num=0, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 1081/1095 [05:11<00:04,  3.47it/s, loss=0.508, v_num=0, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 1082/1095 [05:11<00:03,  3.47it/s, loss=0.508, v_num=0, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 1083/1095 [05:11<00:03,  3.48it/s, loss=0.508, v_num=0, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 1084/1095 [05:11<00:03,  3.48it/s, loss=0.508, v_num=0, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 1085/1095 [05:11<00:02,  3.48it/s, loss=0.508, v_num=0, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 1086/1095 [05:11<00:02,  3.48it/s, loss=0.508, v_num=0, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 1087/1095 [05:11<00:02,  3.49it/s, loss=0.508, v_num=0, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 1088/1095 [05:11<00:02,  3.49it/s, loss=0.508, v_num=0, val_los\u001b[A\n",
      "Epoch 2:  99%|▉| 1089/1095 [05:11<00:01,  3.49it/s, loss=0.508, v_num=0, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 1090/1095 [05:11<00:01,  3.50it/s, loss=0.508, v_num=0, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 1091/1095 [05:11<00:01,  3.50it/s, loss=0.508, v_num=0, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 1092/1095 [05:11<00:00,  3.50it/s, loss=0.508, v_num=0, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 1093/1095 [05:12<00:00,  3.50it/s, loss=0.508, v_num=0, val_los\u001b[A\n",
      "Epoch 2: 100%|▉| 1094/1095 [05:12<00:00,  3.51it/s, loss=0.508, v_num=0, val_los\u001b[A\n",
      "Epoch 2: 100%|█| 1095/1095 [05:12<00:00,  3.51it/s, loss=0.508, v_num=0, val_los\u001b[A\n",
      "Epoch 3:  98%|▉| 1075/1095 [05:10<00:05,  3.46it/s, loss=0.476, v_num=0, val_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  98%|▉| 1076/1095 [05:11<00:05,  3.46it/s, loss=0.476, v_num=0, val_los\u001b[A\n",
      "Epoch 3:  98%|▉| 1077/1095 [05:11<00:05,  3.46it/s, loss=0.476, v_num=0, val_los\u001b[A\n",
      "Epoch 3:  98%|▉| 1078/1095 [05:11<00:04,  3.46it/s, loss=0.476, v_num=0, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 1079/1095 [05:11<00:04,  3.46it/s, loss=0.476, v_num=0, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 1080/1095 [05:11<00:04,  3.47it/s, loss=0.476, v_num=0, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 1081/1095 [05:11<00:04,  3.47it/s, loss=0.476, v_num=0, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 1082/1095 [05:11<00:03,  3.47it/s, loss=0.476, v_num=0, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 1083/1095 [05:11<00:03,  3.47it/s, loss=0.476, v_num=0, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 1084/1095 [05:11<00:03,  3.48it/s, loss=0.476, v_num=0, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 1085/1095 [05:11<00:02,  3.48it/s, loss=0.476, v_num=0, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 1086/1095 [05:11<00:02,  3.48it/s, loss=0.476, v_num=0, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 1087/1095 [05:11<00:02,  3.48it/s, loss=0.476, v_num=0, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 1088/1095 [05:11<00:02,  3.49it/s, loss=0.476, v_num=0, val_los\u001b[A\n",
      "Epoch 3:  99%|▉| 1089/1095 [05:12<00:01,  3.49it/s, loss=0.476, v_num=0, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 1090/1095 [05:12<00:01,  3.49it/s, loss=0.476, v_num=0, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 1091/1095 [05:12<00:01,  3.49it/s, loss=0.476, v_num=0, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 1092/1095 [05:12<00:00,  3.50it/s, loss=0.476, v_num=0, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 1093/1095 [05:12<00:00,  3.50it/s, loss=0.476, v_num=0, val_los\u001b[A\n",
      "Epoch 3: 100%|▉| 1094/1095 [05:12<00:00,  3.50it/s, loss=0.476, v_num=0, val_los\u001b[A\n",
      "Epoch 3: 100%|█| 1095/1095 [05:12<00:00,  3.51it/s, loss=0.476, v_num=0, val_los\u001b[A\n",
      "Epoch 4:  98%|▉| 1075/1095 [05:12<00:05,  3.44it/s, loss=0.43, v_num=0, val_loss\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  98%|▉| 1076/1095 [05:13<00:05,  3.43it/s, loss=0.43, v_num=0, val_loss\u001b[A\n",
      "Epoch 4:  98%|▉| 1077/1095 [05:13<00:05,  3.43it/s, loss=0.43, v_num=0, val_loss\u001b[A\n",
      "Epoch 4:  98%|▉| 1078/1095 [05:13<00:04,  3.43it/s, loss=0.43, v_num=0, val_loss\u001b[A\n",
      "Epoch 4:  99%|▉| 1079/1095 [05:13<00:04,  3.44it/s, loss=0.43, v_num=0, val_loss\u001b[A\n",
      "Epoch 4:  99%|▉| 1080/1095 [05:14<00:04,  3.44it/s, loss=0.43, v_num=0, val_loss\u001b[A\n",
      "Epoch 4:  99%|▉| 1081/1095 [05:14<00:04,  3.44it/s, loss=0.43, v_num=0, val_loss\u001b[A\n",
      "Epoch 4:  99%|▉| 1082/1095 [05:14<00:03,  3.44it/s, loss=0.43, v_num=0, val_loss\u001b[A\n",
      "Epoch 4:  99%|▉| 1083/1095 [05:14<00:03,  3.45it/s, loss=0.43, v_num=0, val_loss\u001b[A\n",
      "Epoch 4:  99%|▉| 1084/1095 [05:14<00:03,  3.45it/s, loss=0.43, v_num=0, val_loss\u001b[A\n",
      "Epoch 4:  99%|▉| 1085/1095 [05:14<00:02,  3.45it/s, loss=0.43, v_num=0, val_loss\u001b[A\n",
      "Epoch 4:  99%|▉| 1086/1095 [05:14<00:02,  3.45it/s, loss=0.43, v_num=0, val_loss\u001b[A\n",
      "Epoch 4:  99%|▉| 1087/1095 [05:14<00:02,  3.46it/s, loss=0.43, v_num=0, val_loss\u001b[A\n",
      "Epoch 4:  99%|▉| 1088/1095 [05:14<00:02,  3.46it/s, loss=0.43, v_num=0, val_loss\u001b[A\n",
      "Epoch 4:  99%|▉| 1089/1095 [05:14<00:01,  3.46it/s, loss=0.43, v_num=0, val_loss\u001b[A\n",
      "Epoch 4: 100%|▉| 1090/1095 [05:14<00:01,  3.46it/s, loss=0.43, v_num=0, val_loss\u001b[A\n",
      "Epoch 4: 100%|▉| 1091/1095 [05:14<00:01,  3.47it/s, loss=0.43, v_num=0, val_loss\u001b[A\n",
      "Epoch 4: 100%|▉| 1092/1095 [05:14<00:00,  3.47it/s, loss=0.43, v_num=0, val_loss\u001b[A\n",
      "Epoch 4: 100%|▉| 1093/1095 [05:14<00:00,  3.47it/s, loss=0.43, v_num=0, val_loss\u001b[A\n",
      "Epoch 4: 100%|▉| 1094/1095 [05:14<00:00,  3.48it/s, loss=0.43, v_num=0, val_loss\u001b[A\n",
      "Epoch 4: 100%|█| 1095/1095 [05:14<00:00,  3.48it/s, loss=0.43, v_num=0, val_loss\u001b[A\n",
      "Epoch 5:  98%|▉| 1075/1095 [05:10<00:05,  3.47it/s, loss=0.396, v_num=0, val_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  98%|▉| 1076/1095 [05:10<00:05,  3.46it/s, loss=0.396, v_num=0, val_los\u001b[A\n",
      "Epoch 5:  98%|▉| 1077/1095 [05:10<00:05,  3.46it/s, loss=0.396, v_num=0, val_los\u001b[A\n",
      "Epoch 5:  98%|▉| 1078/1095 [05:11<00:04,  3.47it/s, loss=0.396, v_num=0, val_los\u001b[A\n",
      "Epoch 5:  99%|▉| 1079/1095 [05:11<00:04,  3.47it/s, loss=0.396, v_num=0, val_los\u001b[A\n",
      "Epoch 5:  99%|▉| 1080/1095 [05:11<00:04,  3.47it/s, loss=0.396, v_num=0, val_los\u001b[A\n",
      "Epoch 5:  99%|▉| 1081/1095 [05:11<00:04,  3.47it/s, loss=0.396, v_num=0, val_los\u001b[A\n",
      "Epoch 5:  99%|▉| 1082/1095 [05:11<00:03,  3.48it/s, loss=0.396, v_num=0, val_los\u001b[A\n",
      "Epoch 5:  99%|▉| 1083/1095 [05:11<00:03,  3.48it/s, loss=0.396, v_num=0, val_los\u001b[A\n",
      "Epoch 5:  99%|▉| 1084/1095 [05:11<00:03,  3.48it/s, loss=0.396, v_num=0, val_los\u001b[A\n",
      "Epoch 5:  99%|▉| 1085/1095 [05:11<00:02,  3.48it/s, loss=0.396, v_num=0, val_los\u001b[A\n",
      "Epoch 5:  99%|▉| 1086/1095 [05:11<00:02,  3.49it/s, loss=0.396, v_num=0, val_los\u001b[A\n",
      "Epoch 5:  99%|▉| 1087/1095 [05:11<00:02,  3.49it/s, loss=0.396, v_num=0, val_los\u001b[A\n",
      "Epoch 5:  99%|▉| 1088/1095 [05:11<00:02,  3.49it/s, loss=0.396, v_num=0, val_los\u001b[A\n",
      "Epoch 5:  99%|▉| 1089/1095 [05:11<00:01,  3.49it/s, loss=0.396, v_num=0, val_los\u001b[A\n",
      "Epoch 5: 100%|▉| 1090/1095 [05:11<00:01,  3.50it/s, loss=0.396, v_num=0, val_los\u001b[A\n",
      "Epoch 5: 100%|▉| 1091/1095 [05:11<00:01,  3.50it/s, loss=0.396, v_num=0, val_los\u001b[A\n",
      "Epoch 5: 100%|▉| 1092/1095 [05:11<00:00,  3.50it/s, loss=0.396, v_num=0, val_los\u001b[A\n",
      "Epoch 5: 100%|▉| 1093/1095 [05:11<00:00,  3.50it/s, loss=0.396, v_num=0, val_los\u001b[A\n",
      "Epoch 5: 100%|▉| 1094/1095 [05:11<00:00,  3.51it/s, loss=0.396, v_num=0, val_los\u001b[A\n",
      "Epoch 5: 100%|█| 1095/1095 [05:11<00:00,  3.51it/s, loss=0.396, v_num=0, val_los\u001b[A\n",
      "Epoch 6:  98%|▉| 1075/1095 [05:10<00:05,  3.46it/s, loss=0.38, v_num=0, val_loss\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  98%|▉| 1076/1095 [05:11<00:05,  3.46it/s, loss=0.38, v_num=0, val_loss\u001b[A\n",
      "Epoch 6:  98%|▉| 1077/1095 [05:11<00:05,  3.46it/s, loss=0.38, v_num=0, val_loss\u001b[A\n",
      "Epoch 6:  98%|▉| 1078/1095 [05:11<00:04,  3.46it/s, loss=0.38, v_num=0, val_loss\u001b[A\n",
      "Epoch 6:  99%|▉| 1079/1095 [05:11<00:04,  3.46it/s, loss=0.38, v_num=0, val_loss\u001b[A\n",
      "Epoch 6:  99%|▉| 1080/1095 [05:11<00:04,  3.47it/s, loss=0.38, v_num=0, val_loss\u001b[A\n",
      "Epoch 6:  99%|▉| 1081/1095 [05:11<00:04,  3.47it/s, loss=0.38, v_num=0, val_loss\u001b[A\n",
      "Epoch 6:  99%|▉| 1082/1095 [05:11<00:03,  3.47it/s, loss=0.38, v_num=0, val_loss\u001b[A\n",
      "Epoch 6:  99%|▉| 1083/1095 [05:11<00:03,  3.47it/s, loss=0.38, v_num=0, val_loss\u001b[A\n",
      "Epoch 6:  99%|▉| 1084/1095 [05:11<00:03,  3.48it/s, loss=0.38, v_num=0, val_loss\u001b[A\n",
      "Epoch 6:  99%|▉| 1085/1095 [05:11<00:02,  3.48it/s, loss=0.38, v_num=0, val_loss\u001b[A\n",
      "Epoch 6:  99%|▉| 1086/1095 [05:11<00:02,  3.48it/s, loss=0.38, v_num=0, val_loss\u001b[A\n",
      "Epoch 6:  99%|▉| 1087/1095 [05:11<00:02,  3.48it/s, loss=0.38, v_num=0, val_loss\u001b[A\n",
      "Epoch 6:  99%|▉| 1088/1095 [05:12<00:02,  3.49it/s, loss=0.38, v_num=0, val_loss\u001b[A\n",
      "Epoch 6:  99%|▉| 1089/1095 [05:12<00:01,  3.49it/s, loss=0.38, v_num=0, val_loss\u001b[A\n",
      "Epoch 6: 100%|▉| 1090/1095 [05:12<00:01,  3.49it/s, loss=0.38, v_num=0, val_loss\u001b[A\n",
      "Epoch 6: 100%|▉| 1091/1095 [05:12<00:01,  3.49it/s, loss=0.38, v_num=0, val_loss\u001b[A\n",
      "Epoch 6: 100%|▉| 1092/1095 [05:12<00:00,  3.50it/s, loss=0.38, v_num=0, val_loss\u001b[A\n",
      "Epoch 6: 100%|▉| 1093/1095 [05:12<00:00,  3.50it/s, loss=0.38, v_num=0, val_loss\u001b[A\n",
      "Epoch 6: 100%|▉| 1094/1095 [05:12<00:00,  3.50it/s, loss=0.38, v_num=0, val_loss\u001b[A\n",
      "Epoch 6: 100%|█| 1095/1095 [05:12<00:00,  3.51it/s, loss=0.38, v_num=0, val_loss\u001b[A\n",
      "Epoch 7:  98%|▉| 1075/1095 [05:10<00:05,  3.46it/s, loss=0.353, v_num=0, val_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  98%|▉| 1076/1095 [05:11<00:05,  3.46it/s, loss=0.353, v_num=0, val_los\u001b[A\n",
      "Epoch 7:  98%|▉| 1077/1095 [05:11<00:05,  3.46it/s, loss=0.353, v_num=0, val_los\u001b[A\n",
      "Epoch 7:  98%|▉| 1078/1095 [05:11<00:04,  3.46it/s, loss=0.353, v_num=0, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 1079/1095 [05:11<00:04,  3.46it/s, loss=0.353, v_num=0, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 1080/1095 [05:11<00:04,  3.47it/s, loss=0.353, v_num=0, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 1081/1095 [05:11<00:04,  3.47it/s, loss=0.353, v_num=0, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 1082/1095 [05:11<00:03,  3.47it/s, loss=0.353, v_num=0, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 1083/1095 [05:11<00:03,  3.47it/s, loss=0.353, v_num=0, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 1084/1095 [05:11<00:03,  3.48it/s, loss=0.353, v_num=0, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 1085/1095 [05:11<00:02,  3.48it/s, loss=0.353, v_num=0, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 1086/1095 [05:11<00:02,  3.48it/s, loss=0.353, v_num=0, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 1087/1095 [05:11<00:02,  3.48it/s, loss=0.353, v_num=0, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 1088/1095 [05:12<00:02,  3.49it/s, loss=0.353, v_num=0, val_los\u001b[A\n",
      "Epoch 7:  99%|▉| 1089/1095 [05:12<00:01,  3.49it/s, loss=0.353, v_num=0, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 1090/1095 [05:12<00:01,  3.49it/s, loss=0.353, v_num=0, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 1091/1095 [05:12<00:01,  3.49it/s, loss=0.353, v_num=0, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 1092/1095 [05:12<00:00,  3.50it/s, loss=0.353, v_num=0, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 1093/1095 [05:12<00:00,  3.50it/s, loss=0.353, v_num=0, val_los\u001b[A\n",
      "Epoch 7: 100%|▉| 1094/1095 [05:12<00:00,  3.50it/s, loss=0.353, v_num=0, val_los\u001b[A\n",
      "Epoch 7: 100%|█| 1095/1095 [05:12<00:00,  3.50it/s, loss=0.353, v_num=0, val_los\u001b[A\n",
      "Epoch 8:  98%|▉| 1075/1095 [08:29<00:09,  2.11it/s, loss=0.345, v_num=0, val_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8:  98%|▉| 1076/1095 [08:30<00:09,  2.11it/s, loss=0.345, v_num=0, val_los\u001b[A\n",
      "Epoch 8:  98%|▉| 1077/1095 [08:30<00:08,  2.11it/s, loss=0.345, v_num=0, val_los\u001b[A\n",
      "Epoch 8:  98%|▉| 1078/1095 [08:30<00:08,  2.11it/s, loss=0.345, v_num=0, val_los\u001b[A\n",
      "Epoch 8:  99%|▉| 1079/1095 [08:30<00:07,  2.11it/s, loss=0.345, v_num=0, val_los\u001b[A\n",
      "Epoch 8:  99%|▉| 1080/1095 [08:31<00:07,  2.11it/s, loss=0.345, v_num=0, val_los\u001b[A\n",
      "Epoch 8:  99%|▉| 1081/1095 [08:31<00:06,  2.12it/s, loss=0.345, v_num=0, val_los\u001b[A\n",
      "Epoch 8:  99%|▉| 1082/1095 [08:31<00:06,  2.12it/s, loss=0.345, v_num=0, val_los\u001b[A\n",
      "Epoch 8:  99%|▉| 1083/1095 [08:31<00:05,  2.12it/s, loss=0.345, v_num=0, val_los\u001b[A\n",
      "Epoch 8:  99%|▉| 1084/1095 [08:31<00:05,  2.12it/s, loss=0.345, v_num=0, val_los\u001b[A\n",
      "Epoch 8:  99%|▉| 1085/1095 [08:31<00:04,  2.12it/s, loss=0.345, v_num=0, val_los\u001b[A\n",
      "Epoch 8:  99%|▉| 1086/1095 [08:31<00:04,  2.12it/s, loss=0.345, v_num=0, val_los\u001b[A\n",
      "Epoch 8:  99%|▉| 1087/1095 [08:31<00:03,  2.12it/s, loss=0.345, v_num=0, val_los\u001b[A\n",
      "Epoch 8:  99%|▉| 1088/1095 [08:31<00:03,  2.13it/s, loss=0.345, v_num=0, val_los\u001b[A\n",
      "Epoch 8:  99%|▉| 1089/1095 [08:32<00:02,  2.13it/s, loss=0.345, v_num=0, val_los\u001b[A\n",
      "Epoch 8: 100%|▉| 1090/1095 [08:32<00:02,  2.13it/s, loss=0.345, v_num=0, val_los\u001b[A\n",
      "Epoch 8: 100%|▉| 1091/1095 [08:32<00:01,  2.13it/s, loss=0.345, v_num=0, val_los\u001b[A\n",
      "Epoch 8: 100%|▉| 1092/1095 [08:32<00:01,  2.13it/s, loss=0.345, v_num=0, val_los\u001b[A\n",
      "Epoch 8: 100%|▉| 1093/1095 [08:32<00:00,  2.13it/s, loss=0.345, v_num=0, val_los\u001b[A\n",
      "Epoch 8: 100%|▉| 1094/1095 [08:32<00:00,  2.13it/s, loss=0.345, v_num=0, val_los\u001b[A\n",
      "Epoch 8: 100%|█| 1095/1095 [08:32<00:00,  2.14it/s, loss=0.345, v_num=0, val_los\u001b[A\n",
      "Epoch 9:  98%|▉| 1075/1095 [07:40<00:08,  2.34it/s, loss=0.343, v_num=0, val_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9:  98%|▉| 1076/1095 [07:41<00:08,  2.33it/s, loss=0.343, v_num=0, val_los\u001b[A\n",
      "Epoch 9:  98%|▉| 1077/1095 [07:41<00:07,  2.34it/s, loss=0.343, v_num=0, val_los\u001b[A\n",
      "Epoch 9:  98%|▉| 1078/1095 [07:41<00:07,  2.34it/s, loss=0.343, v_num=0, val_los\u001b[A\n",
      "Epoch 9:  99%|▉| 1079/1095 [07:41<00:06,  2.34it/s, loss=0.343, v_num=0, val_los\u001b[A\n",
      "Epoch 9:  99%|▉| 1080/1095 [07:41<00:06,  2.34it/s, loss=0.343, v_num=0, val_los\u001b[A\n",
      "Epoch 9:  99%|▉| 1081/1095 [07:41<00:05,  2.34it/s, loss=0.343, v_num=0, val_los\u001b[A\n",
      "Epoch 9:  99%|▉| 1082/1095 [07:41<00:05,  2.34it/s, loss=0.343, v_num=0, val_los\u001b[A\n",
      "Epoch 9:  99%|▉| 1083/1095 [07:41<00:05,  2.35it/s, loss=0.343, v_num=0, val_los\u001b[A\n",
      "Epoch 9:  99%|▉| 1084/1095 [07:41<00:04,  2.35it/s, loss=0.343, v_num=0, val_los\u001b[A\n",
      "Epoch 9:  99%|▉| 1085/1095 [07:41<00:04,  2.35it/s, loss=0.343, v_num=0, val_los\u001b[A\n",
      "Epoch 9:  99%|▉| 1086/1095 [07:41<00:03,  2.35it/s, loss=0.343, v_num=0, val_los\u001b[A\n",
      "Epoch 9:  99%|▉| 1087/1095 [07:41<00:03,  2.35it/s, loss=0.343, v_num=0, val_los\u001b[A\n",
      "Epoch 9:  99%|▉| 1088/1095 [07:41<00:02,  2.36it/s, loss=0.343, v_num=0, val_los\u001b[A\n",
      "Epoch 9:  99%|▉| 1089/1095 [07:41<00:02,  2.36it/s, loss=0.343, v_num=0, val_los\u001b[A\n",
      "Epoch 9: 100%|▉| 1090/1095 [07:41<00:02,  2.36it/s, loss=0.343, v_num=0, val_los\u001b[A\n",
      "Epoch 9: 100%|▉| 1091/1095 [07:41<00:01,  2.36it/s, loss=0.343, v_num=0, val_los\u001b[A\n",
      "Epoch 9: 100%|▉| 1092/1095 [07:42<00:01,  2.36it/s, loss=0.343, v_num=0, val_los\u001b[A\n",
      "Epoch 9: 100%|▉| 1093/1095 [07:42<00:00,  2.37it/s, loss=0.343, v_num=0, val_los\u001b[A\n",
      "Epoch 9: 100%|▉| 1094/1095 [07:42<00:00,  2.37it/s, loss=0.343, v_num=0, val_los\u001b[A\n",
      "Epoch 9: 100%|█| 1095/1095 [07:42<00:00,  2.37it/s, loss=0.343, v_num=0, val_los\u001b[A\n",
      "Epoch 10:  98%|▉| 1075/1095 [04:44<00:05,  3.78it/s, loss=0.3, v_num=0, val_loss\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10:  98%|▉| 1076/1095 [04:45<00:05,  3.77it/s, loss=0.3, v_num=0, val_loss\u001b[A\n",
      "Epoch 10:  98%|▉| 1077/1095 [04:45<00:04,  3.78it/s, loss=0.3, v_num=0, val_loss\u001b[A\n",
      "Epoch 10:  98%|▉| 1078/1095 [04:45<00:04,  3.78it/s, loss=0.3, v_num=0, val_loss\u001b[A\n",
      "Epoch 10:  99%|▉| 1079/1095 [04:45<00:04,  3.78it/s, loss=0.3, v_num=0, val_loss\u001b[A\n",
      "Epoch 10:  99%|▉| 1080/1095 [04:45<00:03,  3.79it/s, loss=0.3, v_num=0, val_loss\u001b[A\n",
      "Epoch 10:  99%|▉| 1081/1095 [04:45<00:03,  3.79it/s, loss=0.3, v_num=0, val_loss\u001b[A\n",
      "Epoch 10:  99%|▉| 1082/1095 [04:45<00:03,  3.79it/s, loss=0.3, v_num=0, val_loss\u001b[A\n",
      "Epoch 10:  99%|▉| 1083/1095 [04:45<00:03,  3.79it/s, loss=0.3, v_num=0, val_loss\u001b[A\n",
      "Epoch 10:  99%|▉| 1084/1095 [04:45<00:02,  3.80it/s, loss=0.3, v_num=0, val_loss\u001b[A\n",
      "Epoch 10:  99%|▉| 1085/1095 [04:45<00:02,  3.80it/s, loss=0.3, v_num=0, val_loss\u001b[A\n",
      "Epoch 10:  99%|▉| 1086/1095 [04:45<00:02,  3.80it/s, loss=0.3, v_num=0, val_loss\u001b[A\n",
      "Epoch 10:  99%|▉| 1087/1095 [04:45<00:02,  3.80it/s, loss=0.3, v_num=0, val_loss\u001b[A\n",
      "Epoch 10:  99%|▉| 1088/1095 [04:45<00:01,  3.81it/s, loss=0.3, v_num=0, val_loss\u001b[A\n",
      "Epoch 10:  99%|▉| 1089/1095 [04:45<00:01,  3.81it/s, loss=0.3, v_num=0, val_loss\u001b[A\n",
      "Epoch 10: 100%|▉| 1090/1095 [04:45<00:01,  3.81it/s, loss=0.3, v_num=0, val_loss\u001b[A\n",
      "Epoch 10: 100%|▉| 1091/1095 [04:45<00:01,  3.82it/s, loss=0.3, v_num=0, val_loss\u001b[A\n",
      "Epoch 10: 100%|▉| 1092/1095 [04:45<00:00,  3.82it/s, loss=0.3, v_num=0, val_loss\u001b[A\n",
      "Epoch 10: 100%|▉| 1093/1095 [04:46<00:00,  3.82it/s, loss=0.3, v_num=0, val_loss\u001b[A\n",
      "Epoch 10: 100%|▉| 1094/1095 [04:46<00:00,  3.82it/s, loss=0.3, v_num=0, val_loss\u001b[A\n",
      "Epoch 10: 100%|█| 1095/1095 [04:46<00:00,  3.83it/s, loss=0.3, v_num=0, val_loss\u001b[A\n",
      "Epoch 11:  98%|▉| 1075/1095 [04:44<00:05,  3.78it/s, loss=0.256, v_num=0, val_lo\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11:  98%|▉| 1076/1095 [04:45<00:05,  3.77it/s, loss=0.256, v_num=0, val_lo\u001b[A\n",
      "Epoch 11:  98%|▉| 1077/1095 [04:45<00:04,  3.77it/s, loss=0.256, v_num=0, val_lo\u001b[A\n",
      "Epoch 11:  98%|▉| 1078/1095 [04:45<00:04,  3.77it/s, loss=0.256, v_num=0, val_lo\u001b[A\n",
      "Epoch 11:  99%|▉| 1079/1095 [04:45<00:04,  3.78it/s, loss=0.256, v_num=0, val_lo\u001b[A\n",
      "Epoch 11:  99%|▉| 1080/1095 [04:45<00:03,  3.78it/s, loss=0.256, v_num=0, val_lo\u001b[A\n",
      "Epoch 11:  99%|▉| 1081/1095 [04:45<00:03,  3.78it/s, loss=0.256, v_num=0, val_lo\u001b[A\n",
      "Epoch 11:  99%|▉| 1082/1095 [04:45<00:03,  3.78it/s, loss=0.256, v_num=0, val_lo\u001b[A\n",
      "Epoch 11:  99%|▉| 1083/1095 [04:45<00:03,  3.79it/s, loss=0.256, v_num=0, val_lo\u001b[A\n",
      "Epoch 11:  99%|▉| 1084/1095 [04:46<00:02,  3.79it/s, loss=0.256, v_num=0, val_lo\u001b[A\n",
      "Epoch 11:  99%|▉| 1085/1095 [04:46<00:02,  3.79it/s, loss=0.256, v_num=0, val_lo\u001b[A\n",
      "Epoch 11:  99%|▉| 1086/1095 [04:46<00:02,  3.80it/s, loss=0.256, v_num=0, val_lo\u001b[A\n",
      "Epoch 11:  99%|▉| 1087/1095 [04:46<00:02,  3.80it/s, loss=0.256, v_num=0, val_lo\u001b[A\n",
      "Epoch 11:  99%|▉| 1088/1095 [04:46<00:01,  3.80it/s, loss=0.256, v_num=0, val_lo\u001b[A\n",
      "Epoch 11:  99%|▉| 1089/1095 [04:46<00:01,  3.80it/s, loss=0.256, v_num=0, val_lo\u001b[A\n",
      "Epoch 11: 100%|▉| 1090/1095 [04:46<00:01,  3.81it/s, loss=0.256, v_num=0, val_lo\u001b[A\n",
      "Epoch 11: 100%|▉| 1091/1095 [04:46<00:01,  3.81it/s, loss=0.256, v_num=0, val_lo\u001b[A\n",
      "Epoch 11: 100%|▉| 1092/1095 [04:46<00:00,  3.81it/s, loss=0.256, v_num=0, val_lo\u001b[A\n",
      "Epoch 11: 100%|▉| 1093/1095 [04:46<00:00,  3.81it/s, loss=0.256, v_num=0, val_lo\u001b[A\n",
      "Epoch 11: 100%|▉| 1094/1095 [04:46<00:00,  3.82it/s, loss=0.256, v_num=0, val_lo\u001b[A\n",
      "Epoch 11: 100%|█| 1095/1095 [04:46<00:00,  3.82it/s, loss=0.256, v_num=0, val_lo\u001b[A\n",
      "Epoch 11: 100%|█| 1095/1095 [04:46<00:00,  3.82it/s, loss=0.256, v_num=0, val_lo\u001b[A\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/fanmin/mambaforge/envs/gpu_env/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Testing DataLoader 0: 100%|█████████████████████| 20/20 [00:01<00:00, 16.36it/s]\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m  Runningstage.testing   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m                           \u001b[0m┃\n",
      "┃\u001b[1m \u001b[0m\u001b[1m         metric          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8423249125480652    \u001b[0m\u001b[35m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.44701725244522095   \u001b[0m\u001b[35m \u001b[0m│\n",
      "└───────────────────────────┴───────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Train bi-lstm with max-pooling model\n",
    "! python train.py --encoder=bi-lstm-max-pool --save_dir='./models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentEval\n",
    "Make sure to run `atcs-lstm/SentEval/data/downstream/get_transfer_data.bash` in `atcs-lstm/SentEval/data/downstream/` first before running the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --senteval --checkpt='/home/fanmin/atcs-lstm/models/avg-epoch=19-val_loss=0.66-val_accuracy=0.72.ckpt'\n",
    "# {'MR': {'devacc': 76.84, 'acc': 76.89, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 46.54, 'acc': 36.24, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 78.06, 'acc': 68.77, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 90.73, 'acc': 91.31, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 79.24, 'acc': 80.29, 'ndev': 872, 'ntest': 1821}, 'TREC': {'devacc': 73.24, 'acc': 83.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 73.58, 'acc': 72.87, 'f1': 81.38, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 81.0, 'acc': 78.49, 'ndev': 500, 'ntest': 4927}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python projecttrain.py --senteval --checkpt='/home/fanmin/atcs-lstm/models/lstm-epoch\\=4-val_loss\\=0.50-val_accuracy\\=0.80.ckpt' --encoder=lstm\n",
    "# {'MR': {'devacc': 72.9, 'acc': 73.04, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 75.87, 'acc': 76.08, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 86.98, 'acc': 87.18, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 85.76, 'acc': 86.06, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 77.64, 'acc': 76.39, 'ndev': 872, 'ntest': 1821}, 'TREC': {'devacc': 62.88, 'acc': 73.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 72.82, 'acc': 72.87, 'f1': 81.2, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 82.4, 'acc': 84.96, 'ndev': 500, 'ntest': 4927}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 1234\n",
      "unique token len 37211\n",
      "2024-04-22 20:09:24,035 : open file: /home/fanmin/atcs-lstm/models/bi-lstm-epoch=4-val_loss=0.50-val_accuracy=0.80.ckpt\n",
      "2024-04-22 20:09:24,330 : ***** Transfer task : MR *****\n",
      "\n",
      "\n",
      "prepare uniq tokens 20325\n",
      "2024-04-22 20:09:24,555 : Generating sentence embeddings\n",
      "2024-04-22 20:09:25,856 : Generated sentence embeddings\n",
      "2024-04-22 20:09:25,856 : Training pytorch-MLP-nhid0-adam-bs64 with (inner) 2-fold cross-validation\n",
      "2024-04-22 20:09:30,219 : Best param found at split 1: l2reg = 0.001                 with score 75.5\n",
      "2024-04-22 20:09:35,584 : Best param found at split 2: l2reg = 0.001                 with score 74.86\n",
      "2024-04-22 20:09:36,333 : Dev acc : 75.18 Test acc : 74.2\n",
      "\n",
      "2024-04-22 20:09:36,334 : ***** Transfer task : CR *****\n",
      "\n",
      "\n",
      "prepare uniq tokens 5674\n",
      "2024-04-22 20:09:36,378 : Generating sentence embeddings\n",
      "2024-04-22 20:09:36,727 : Generated sentence embeddings\n",
      "2024-04-22 20:09:36,728 : Training pytorch-MLP-nhid0-adam-bs64 with (inner) 2-fold cross-validation\n",
      "2024-04-22 20:09:38,376 : Best param found at split 1: l2reg = 0.01                 with score 78.27\n",
      "2024-04-22 20:09:40,389 : Best param found at split 2: l2reg = 0.001                 with score 77.44\n",
      "2024-04-22 20:09:40,644 : Dev acc : 77.85 Test acc : 77.11\n",
      "\n",
      "2024-04-22 20:09:40,645 : ***** Transfer task : MPQA *****\n",
      "\n",
      "\n",
      "prepare uniq tokens 6238\n",
      "2024-04-22 20:09:40,690 : Generating sentence embeddings\n",
      "2024-04-22 20:09:40,964 : Generated sentence embeddings\n",
      "2024-04-22 20:09:40,964 : Training pytorch-MLP-nhid0-adam-bs64 with (inner) 2-fold cross-validation\n",
      "2024-04-22 20:09:45,139 : Best param found at split 1: l2reg = 0.001                 with score 87.21\n",
      "2024-04-22 20:09:50,328 : Best param found at split 2: l2reg = 0.0001                 with score 87.01\n",
      "2024-04-22 20:09:51,383 : Dev acc : 87.11 Test acc : 87.2\n",
      "\n",
      "2024-04-22 20:09:51,384 : ***** Transfer task : SUBJ *****\n",
      "\n",
      "\n",
      "prepare uniq tokens 22636\n",
      "2024-04-22 20:09:51,459 : Generating sentence embeddings\n",
      "2024-04-22 20:09:52,499 : Generated sentence embeddings\n",
      "2024-04-22 20:09:52,499 : Training pytorch-MLP-nhid0-adam-bs64 with (inner) 2-fold cross-validation\n",
      "2024-04-22 20:09:57,245 : Best param found at split 1: l2reg = 1e-05                 with score 89.32\n",
      "2024-04-22 20:10:03,262 : Best param found at split 2: l2reg = 1e-05                 with score 88.6\n",
      "2024-04-22 20:10:04,190 : Dev acc : 88.96 Test acc : 88.95\n",
      "\n",
      "2024-04-22 20:10:04,191 : ***** Transfer task : SST Binary classification *****\n",
      "\n",
      "\n",
      "prepare uniq tokens 17558\n",
      "2024-04-22 20:10:04,466 : Computing embedding for train\n",
      "2024-04-22 20:10:07,917 : Computed train embeddings\n",
      "2024-04-22 20:10:07,917 : Computing embedding for dev\n",
      "2024-04-22 20:10:08,003 : Computed dev embeddings\n",
      "2024-04-22 20:10:08,003 : Computing embedding for test\n",
      "2024-04-22 20:10:08,166 : Computed test embeddings\n",
      "2024-04-22 20:10:08,166 : Training pytorch-MLP-nhid0-adam-bs64 with standard validation..\n",
      "2024-04-22 20:10:46,705 : [('reg:1e-05', 79.36), ('reg:0.0001', 79.13), ('reg:0.001', 78.78), ('reg:0.01', 77.87)]\n",
      "2024-04-22 20:10:46,705 : Validation : best param found is reg = 1e-05 with score             79.36\n",
      "2024-04-22 20:10:46,705 : Evaluating...\n",
      "2024-04-22 20:10:56,276 : \n",
      "Dev acc : 79.36 Test acc : 80.07 for             SST Binary classification\n",
      "\n",
      "2024-04-22 20:10:56,277 : ***** Transfer task : TREC *****\n",
      "\n",
      "\n",
      "prepare uniq tokens 9764\n",
      "2024-04-22 20:10:56,604 : Computed train embeddings\n",
      "2024-04-22 20:10:56,626 : Computed test embeddings\n",
      "2024-04-22 20:10:56,627 : Training pytorch-MLP-nhid0-adam-bs64 with 2-fold cross-validation\n",
      "2024-04-22 20:11:03,602 : [('reg:1e-05', 81.55), ('reg:0.0001', 80.98), ('reg:0.001', 79.48), ('reg:0.01', 73.17)]\n",
      "2024-04-22 20:11:03,602 : Cross-validation : best param found is reg = 1e-05             with score 81.55\n",
      "2024-04-22 20:11:03,602 : Evaluating...\n",
      "2024-04-22 20:11:04,634 : \n",
      "Dev acc : 81.55 Test acc : 86.8             for TREC\n",
      "\n",
      "2024-04-22 20:11:04,634 : ***** Transfer task : MRPC *****\n",
      "\n",
      "\n",
      "prepare uniq tokens 19553\n",
      "2024-04-22 20:11:04,698 : Computing embedding for train\n",
      "2024-04-22 20:11:05,498 : Computed train embeddings\n",
      "2024-04-22 20:11:05,498 : Computing embedding for test\n",
      "2024-04-22 20:11:05,847 : Computed test embeddings\n",
      "2024-04-22 20:11:05,884 : Training pytorch-MLP-nhid0-adam-bs64 with 2-fold cross-validation\n",
      "2024-04-22 20:11:08,910 : [('reg:1e-05', 72.4), ('reg:0.0001', 72.37), ('reg:0.001', 72.4), ('reg:0.01', 72.06)]\n",
      "2024-04-22 20:11:08,910 : Cross-validation : best param found is reg = 1e-05             with score 72.4\n",
      "2024-04-22 20:11:08,910 : Evaluating...\n",
      "2024-04-22 20:11:09,706 : Dev acc : 72.4 Test acc 72.52; Test F1 79.81 for MRPC.\n",
      "\n",
      "2024-04-22 20:11:09,707 : ***** Transfer task : SICK-Entailment*****\n",
      "\n",
      "\n",
      "prepare uniq tokens 2411\n",
      "2024-04-22 20:11:09,736 : Computing embedding for train\n",
      "2024-04-22 20:11:10,237 : Computed train embeddings\n",
      "2024-04-22 20:11:10,237 : Computing embedding for dev\n",
      "2024-04-22 20:11:10,307 : Computed dev embeddings\n",
      "2024-04-22 20:11:10,307 : Computing embedding for test\n",
      "2024-04-22 20:11:10,838 : Computed test embeddings\n",
      "2024-04-22 20:11:10,907 : Training pytorch-MLP-nhid0-adam-bs64 with standard validation..\n",
      "2024-04-22 20:11:14,198 : [('reg:1e-05', 84.6), ('reg:0.0001', 84.2), ('reg:0.001', 84.0), ('reg:0.01', 83.0)]\n",
      "2024-04-22 20:11:14,199 : Validation : best param found is reg = 1e-05 with score             84.6\n",
      "2024-04-22 20:11:14,199 : Evaluating...\n",
      "2024-04-22 20:11:14,949 : \n",
      "Dev acc : 84.6 Test acc : 84.23 for                        SICK entailment\n",
      "\n",
      "{'MR': {'devacc': 75.18, 'acc': 74.2, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 77.85, 'acc': 77.11, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.11, 'acc': 87.2, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 88.96, 'acc': 88.95, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 79.36, 'acc': 80.07, 'ndev': 872, 'ntest': 1821}, 'TREC': {'devacc': 81.55, 'acc': 86.8, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 72.4, 'acc': 72.52, 'f1': 79.81, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 84.6, 'acc': 84.23, 'ndev': 500, 'ntest': 4927}}\n"
     ]
    }
   ],
   "source": [
    "! python train.py --senteval --checkpt='/home/fanmin/atcs-lstm/models/bi-lstm-epoch=4-val_loss=0.50-val_accuracy=0.80.ckpt' --encoder=bi-lstm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 1234\n",
      "unique token len 37211\n",
      "2024-04-22 20:11:34,863 : open file: /home/fanmin/atcs-lstm/models/bi-lstm-max-pool-epoch=8-val_loss=0.42-val_accuracy=0.84.ckpt\n",
      "2024-04-22 20:11:35,167 : ***** Transfer task : MR *****\n",
      "\n",
      "\n",
      "prepare uniq tokens 20325\n",
      "2024-04-22 20:11:35,392 : Generating sentence embeddings\n",
      "2024-04-22 20:11:36,852 : Generated sentence embeddings\n",
      "2024-04-22 20:11:36,852 : Training pytorch-MLP-nhid0-adam-bs64 with (inner) 2-fold cross-validation\n",
      "2024-04-22 20:11:41,595 : Best param found at split 1: l2reg = 1e-05                 with score 77.6\n",
      "2024-04-22 20:11:47,722 : Best param found at split 2: l2reg = 0.0001                 with score 76.42\n",
      "2024-04-22 20:11:48,887 : Dev acc : 77.01 Test acc : 77.03\n",
      "\n",
      "2024-04-22 20:11:48,888 : ***** Transfer task : CR *****\n",
      "\n",
      "\n",
      "prepare uniq tokens 5674\n",
      "2024-04-22 20:11:48,933 : Generating sentence embeddings\n",
      "2024-04-22 20:11:49,335 : Generated sentence embeddings\n",
      "2024-04-22 20:11:49,335 : Training pytorch-MLP-nhid0-adam-bs64 with (inner) 2-fold cross-validation\n",
      "2024-04-22 20:11:51,649 : Best param found at split 1: l2reg = 0.0001                 with score 79.44\n",
      "2024-04-22 20:11:53,781 : Best param found at split 2: l2reg = 0.001                 with score 78.71\n",
      "2024-04-22 20:11:54,090 : Dev acc : 79.07 Test acc : 79.31\n",
      "\n",
      "2024-04-22 20:11:54,090 : ***** Transfer task : MPQA *****\n",
      "\n",
      "\n",
      "prepare uniq tokens 6238\n",
      "2024-04-22 20:11:54,134 : Generating sentence embeddings\n",
      "2024-04-22 20:11:54,509 : Generated sentence embeddings\n",
      "2024-04-22 20:11:54,510 : Training pytorch-MLP-nhid0-adam-bs64 with (inner) 2-fold cross-validation\n",
      "2024-04-22 20:11:59,022 : Best param found at split 1: l2reg = 1e-05                 with score 87.99\n",
      "2024-04-22 20:12:05,038 : Best param found at split 2: l2reg = 0.0001                 with score 87.59\n",
      "2024-04-22 20:12:05,944 : Dev acc : 87.79 Test acc : 88.2\n",
      "\n",
      "2024-04-22 20:12:05,945 : ***** Transfer task : SUBJ *****\n",
      "\n",
      "\n",
      "prepare uniq tokens 22636\n",
      "2024-04-22 20:12:06,022 : Generating sentence embeddings\n",
      "2024-04-22 20:12:07,207 : Generated sentence embeddings\n",
      "2024-04-22 20:12:07,208 : Training pytorch-MLP-nhid0-adam-bs64 with (inner) 2-fold cross-validation\n",
      "2024-04-22 20:12:13,450 : Best param found at split 1: l2reg = 1e-05                 with score 91.36\n",
      "2024-04-22 20:12:20,739 : Best param found at split 2: l2reg = 1e-05                 with score 91.28\n",
      "2024-04-22 20:12:21,793 : Dev acc : 91.32 Test acc : 91.87\n",
      "\n",
      "2024-04-22 20:12:21,794 : ***** Transfer task : SST Binary classification *****\n",
      "\n",
      "\n",
      "prepare uniq tokens 17558\n",
      "2024-04-22 20:12:22,076 : Computing embedding for train\n",
      "2024-04-22 20:12:26,250 : Computed train embeddings\n",
      "2024-04-22 20:12:26,250 : Computing embedding for dev\n",
      "2024-04-22 20:12:26,346 : Computed dev embeddings\n",
      "2024-04-22 20:12:26,346 : Computing embedding for test\n",
      "2024-04-22 20:12:26,540 : Computed test embeddings\n",
      "2024-04-22 20:12:26,540 : Training pytorch-MLP-nhid0-adam-bs64 with standard validation..\n",
      "2024-04-22 20:13:12,470 : [('reg:1e-05', 81.19), ('reg:0.0001', 81.19), ('reg:0.001', 80.16), ('reg:0.01', 77.64)]\n",
      "2024-04-22 20:13:12,470 : Validation : best param found is reg = 1e-05 with score             81.19\n",
      "2024-04-22 20:13:12,470 : Evaluating...\n",
      "2024-04-22 20:13:26,731 : \n",
      "Dev acc : 81.19 Test acc : 83.03 for             SST Binary classification\n",
      "\n",
      "2024-04-22 20:13:26,732 : ***** Transfer task : TREC *****\n",
      "\n",
      "\n",
      "prepare uniq tokens 9764\n",
      "2024-04-22 20:13:27,118 : Computed train embeddings\n",
      "2024-04-22 20:13:27,146 : Computed test embeddings\n",
      "2024-04-22 20:13:27,146 : Training pytorch-MLP-nhid0-adam-bs64 with 2-fold cross-validation\n",
      "2024-04-22 20:13:36,015 : [('reg:1e-05', 80.91), ('reg:0.0001', 80.61), ('reg:0.001', 75.53), ('reg:0.01', 63.44)]\n",
      "2024-04-22 20:13:36,015 : Cross-validation : best param found is reg = 1e-05             with score 80.91\n",
      "2024-04-22 20:13:36,015 : Evaluating...\n",
      "2024-04-22 20:13:37,728 : \n",
      "Dev acc : 80.91 Test acc : 89.2             for TREC\n",
      "\n",
      "2024-04-22 20:13:37,728 : ***** Transfer task : MRPC *****\n",
      "\n",
      "\n",
      "prepare uniq tokens 19553\n",
      "2024-04-22 20:13:37,794 : Computing embedding for train\n",
      "2024-04-22 20:13:38,714 : Computed train embeddings\n",
      "2024-04-22 20:13:38,714 : Computing embedding for test\n",
      "2024-04-22 20:13:39,113 : Computed test embeddings\n",
      "2024-04-22 20:13:39,151 : Training pytorch-MLP-nhid0-adam-bs64 with 2-fold cross-validation\n",
      "2024-04-22 20:13:43,525 : [('reg:1e-05', 73.9), ('reg:0.0001', 73.72), ('reg:0.001', 73.04), ('reg:0.01', 69.46)]\n",
      "2024-04-22 20:13:43,525 : Cross-validation : best param found is reg = 1e-05             with score 73.9\n",
      "2024-04-22 20:13:43,525 : Evaluating...\n",
      "2024-04-22 20:13:44,408 : Dev acc : 73.9 Test acc 73.86; Test F1 81.48 for MRPC.\n",
      "\n",
      "2024-04-22 20:13:44,408 : ***** Transfer task : SICK-Entailment*****\n",
      "\n",
      "\n",
      "prepare uniq tokens 2411\n",
      "2024-04-22 20:13:44,438 : Computing embedding for train\n",
      "2024-04-22 20:13:45,034 : Computed train embeddings\n",
      "2024-04-22 20:13:45,034 : Computing embedding for dev\n",
      "2024-04-22 20:13:45,116 : Computed dev embeddings\n",
      "2024-04-22 20:13:45,116 : Computing embedding for test\n",
      "2024-04-22 20:13:45,751 : Computed test embeddings\n",
      "2024-04-22 20:13:45,820 : Training pytorch-MLP-nhid0-adam-bs64 with standard validation..\n",
      "2024-04-22 20:13:49,481 : [('reg:1e-05', 83.8), ('reg:0.0001', 83.0), ('reg:0.001', 83.4), ('reg:0.01', 78.0)]\n",
      "2024-04-22 20:13:49,481 : Validation : best param found is reg = 1e-05 with score             83.8\n",
      "2024-04-22 20:13:49,481 : Evaluating...\n",
      "2024-04-22 20:13:50,424 : \n",
      "Dev acc : 83.8 Test acc : 84.33 for                        SICK entailment\n",
      "\n",
      "{'MR': {'devacc': 77.01, 'acc': 77.03, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 79.07, 'acc': 79.31, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.79, 'acc': 88.2, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 91.32, 'acc': 91.87, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 81.19, 'acc': 83.03, 'ndev': 872, 'ntest': 1821}, 'TREC': {'devacc': 80.91, 'acc': 89.2, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 73.9, 'acc': 73.86, 'f1': 81.48, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 83.8, 'acc': 84.33, 'ndev': 500, 'ntest': 4927}}\n"
     ]
    }
   ],
   "source": [
    "! python train.py --senteval --checkpt='/home/fanmin/atcs-lstm/models/bi-lstm-max-pool-epoch=8-val_loss=0.42-val_accuracy=0.84.ckpt' --encoder=bi-lstm-max-pool"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
